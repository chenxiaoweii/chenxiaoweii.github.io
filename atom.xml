<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一期一会</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.dujiong.net/"/>
  <updated>2017-05-06T08:50:12.741Z</updated>
  <id>http://blog.dujiong.net/</id>
  
  <author>
    <name>dujiong</name>
    <email>dujiong.uestc@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NVMe over RDMA浅析</title>
    <link href="http://blog.dujiong.net/2017/04/05/NVMe-over-RDMA/"/>
    <id>http://blog.dujiong.net/2017/04/05/NVMe-over-RDMA/</id>
    <published>2017-04-05T12:49:59.000Z</published>
    <updated>2017-05-06T08:50:12.741Z</updated>
    
    <content type="html">&lt;p&gt;NVMe是一种Host与SSD之间的通信协议，为了把NVMe扩展到端到端的跨网络传输，NVMe的开发者提出了NVMe over Fabrics，用于解决将NVMe置于各种传输环境下所遇到的问题。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h3 id=&quot;NVMe-over-Fabrics整体架构&quot;&gt;&lt;a href=&quot;#NVMe-over-Fabrics整体架构&quot; class=&quot;headerlink&quot; title=&quot;NVMe over Fabrics整体架构&quot;&gt;&lt;/a&gt;NVMe over Fabrics整体架构&lt;/h3&gt;&lt;p&gt;NVMe over Fabrics的整体架构如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/vvD1rC7.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;可以看出，NVMe Host(Controller)-side Transport Abstraction这两层便是NVMe over Fabrics协议的实现层。该协议只是一个用于NVMe Transport的抽象层而已，它并不实现真正的命令和数据传输功能，它只是为命令和数据传输定义了统一的规范，因此该协议只是“指导方针”。它是构建在”Fabrics”之上的，即它并不关心实际的Fabrics到底是什么，它只是提供了Fabrics通用的对接NVMe的接口，完成了对NVMe接口和命令在各种Fabrics而非只是PCIe上（NVMe Base协议只涉及PCIe这一种Fabric）的拓展。因此，为了使NVMe可以架构于不同的Fabric之上，各Fabric还需开发专用的功能实现层，真正实现基于此Fabric的数据传输功能，并完成和Transport Abstraction抽象层（即NVMe over Fabrics协议的实现层）的对接以使得传输抽象层可以调用到这些函数。         &lt;/p&gt;
&lt;h3 id=&quot;NVMe-over-RDMA&quot;&gt;&lt;a href=&quot;#NVMe-over-RDMA&quot; class=&quot;headerlink&quot; title=&quot;NVMe over RDMA&quot;&gt;&lt;/a&gt;NVMe over RDMA&lt;/h3&gt;&lt;h4 id=&quot;整体架构&quot;&gt;&lt;a href=&quot;#整体架构&quot; class=&quot;headerlink&quot; title=&quot;整体架构&quot;&gt;&lt;/a&gt;整体架构&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;http://blog.dujiong.net/2017/02/27/RDMA/&quot;&gt;RDMA技术浅析&lt;/a&gt;一文中介绍了RDMA这种”Fabric”以及它的几种实现方式。鉴于后续研究需要，这里选择RoCE（如无特别说明，均指的是v2版本）这种”Fabric”来展开说明。&lt;br&gt;NVMe over RoCE的架构如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/Ukm4No4.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;NVMe Host和NVMe Subsystem Controller是NVMe Base协议扩展到NVMe over Fabrics的部分；NVMe Host(Controller)-side Transport Abstraction则是NVMe over Fabrics传输抽象层的实现。RoCE层则是支持RoCE技术的网卡及相关驱动和RDMA协议栈，而不论InfiniBand、RoCE或者iWarp何种具体的RDMA实现形式，都约定提供统一的操作接口，RDMA Verbs便是RDMA技术向上层提供的接口。NVMe RDMA则是实现将RDMA的接口Verbs和NVMe对接的关键粘合层，简言之，其作用是将NVMe Transport Abstraction传输抽象层提供的传输接口可以调用到下层RDMA提供的传输接口（即verbs）。  &lt;/p&gt;
&lt;h4 id=&quot;具体实现&quot;&gt;&lt;a href=&quot;#具体实现&quot; class=&quot;headerlink&quot; title=&quot;具体实现&quot;&gt;&lt;/a&gt;具体实现&lt;/h4&gt;&lt;p&gt;此架构的具体操作原理将结合Linux 4.8版本内核的实现来解读。在4.8版本的Linux内核中加入了符合上图逻辑的NVMe over RDMA的代码。因此对于我们来说，只要编译最新内核并添加相关模块完成配置后，内核的代码完全不需要添加或修改。对于用户空间的应用程序来说，使用NVMe over RDMA的操作与传统的文件读写操作并无差异。这得益于Linux内核的软件分层架构，虚拟文件系统层、文件系统层和通用块层的架构可以屏蔽底层的硬件细节，且对上层提供统一的接口，这样上层用户空间的应用程序自然不会（也不应该）关注到底层硬件及使用协议的差别。Host端的Linux kernel的架构如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/a9Nzq2g.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;如果用户使用NVMe特有的命令，即当用户下发NVMe commands时，这需要通过Linux内核的IOCTL接口，该接口的目的本就是实现用户和底层驱动打交道。可以看到，NVMe commands通过IOCTL机制便到达NVMe Common代码层，该层代码的任务就是解析并执行各种NVMe command。在NVMe Base标准的定义中，命令是被封装在capsule这一数据结构中传输，应用将capsule压入内存中的NVMe Submission Queue，接着controller取出capsule并解析应用的command，处理之后将回复capsule压入NVMe Completion Queue，应用取出回复消息完成一次通信。具体过程参考前文&lt;a href=&quot;http://blog.dujiong.net/2017/02/17/NVMe/&quot;&gt;NVMe技术浅析&lt;/a&gt;&lt;br&gt;而在NVMe over RDMA的环境下，传输过程如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/zqCUGcT.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;首先，host的command被封装进capsule后压入Host NVMe Submission Queue；接着该capsule会被放入Host RDMA Send Queue变成RDMA_SEND消息的消息负载；接着消息被host的RDMA网卡发包，当被target网卡接收后，capsule被放到target网卡的RDMA ReceIve Queue；接下来该capsule被放置到target端的内存中并由target处理该command；待处理完后，target生成回复信息（Respond Command）并封装进capsule然后将该回复capsule压入target端的NVMe Controller的Submission Queue，并经由target的网卡发送给host。这样，一次NVMe over RDMA的通信就完成了。&lt;/p&gt;
&lt;h4 id=&quot;NVMe-over-RDMA读写文件原理&quot;&gt;&lt;a href=&quot;#NVMe-over-RDMA读写文件原理&quot; class=&quot;headerlink&quot; title=&quot;NVMe over RDMA读写文件原理&quot;&gt;&lt;/a&gt;NVMe over RDMA读写文件原理&lt;/h4&gt;&lt;p&gt;下面以host读文件为例讨论数据流的传输。&lt;br&gt;事实上，在NVMe over Fabrics协议中，提出了两种数据传输方式，一种是将data附到capsule中，只要在传输command时负载需要传输的data即可。另一种则是直接的内存传输方式。这种方式原理上是将要传输的数据的地址和长度等元信息负载到capsule中。如用户发出read请求后，最终的capsule消息负载中会包含数据的存储地址、要读取的数据的长度以及要读到的内存地址。利用RDMA技术，这种传输的特性将表现的更加淋漓尽致。&lt;br&gt;由Linux内核知识可知，虚拟文件系统（VFS）向上层提供统一的文件操作接口，当用户空间进程读取文件时便调用read API；接着VFS调用到真正的文件系统（例如Ext4等）真正的read函数，真正文件系统的作用是确定数据的位置，简言之就是根据用户要读取的文件、长度、偏移量等确定文件的逻辑块号（在真实的文件存储中，一个文件是被分割成若干块存储的）；接下来内核利用通用块层（Block Layer）启动IO操作来传送所请求的数据，通用块层为所有的块设备提供了一个抽象视图，隐藏了硬件块设备间的差异，而每次IO操作由一个“块I/O”结构（struct bio）的对象来描述。至此，所有的读文件操作都是这套同样的流程。由于NVMe或AHCI都是更底层的接口标准，因此差异从通用块层之下才开始。&lt;br&gt;此后，进入到NVMe Transport Abstraction（即NVMe over Fabrics协议的实现层）或者NVMe over PCIe，这时便会由此生成NVMe（base或over Fabrics）标准的Read Command，由NVMe Base标准对Read Command的定义，Read Command中包含了数据应该读到的内存区域的地址（Read Command的Data Pointer字段）、以及存储数据的逻辑块号的起始地址（Starting LBA字段）等其他在read操作中必需的字段（显然地，这些信息从上层通用块层传递的bio对象中获取），并将该Read Command封装进NVMe的通信单元capsule中。接着便进入到下一层的RDMA stack。RDMA Stack之下便是支持RDMA技术的网卡驱动，最底层便是不同技术（IB/RoCE/iWarp）实现的RDMA网卡。内核的NVMe RDMA层代码完成调用RDMA的接口verbs将该capsule封装为RDMA_SEND消息的负载。&lt;br&gt;Target的controller处理完read command后，数据流传输便开始了。数据流的传输则完全借助于RoCE技术。由于此前在target端注册了host的内存，接下来从target的SSD中读出的数据便直接封装为RDMA_WRITE消息的负载（注意host的Read Command触发的却是target的RDMA_WRITE操作），然后将这部分数据直接从target端写入到host的内存中，而写入的地址在Read Command中便已指明。该过程的实现得益于RDMA的数据零复制技术。      &lt;/p&gt;
&lt;p&gt;说了这么多，通过一幅图来直观展示上述NVMe over RDMA读文件的过程。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/Mi5Okr5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h3&gt;&lt;p&gt;本文分析了将NVMe扩展到端到端的跨网络传输—NVMe over RDMA的架构和实现原理。以读文件为例，详细分析了数据流的传输过程。后续将进入实战阶段，在服务器上搭建小型NVMe存储系统，并通过RDMA进行跨网络传输。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe是一种Host与SSD之间的通信协议，为了把NVMe扩展到端到端的跨网络传输，NVMe的开发者提出了NVMe over Fabrics，用于解决将NVMe置于各种传输环境下所遇到的问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.dujiong.net/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>LevelDB原理剖析</title>
    <link href="http://blog.dujiong.net/2017/03/27/leveldb/"/>
    <id>http://blog.dujiong.net/2017/03/27/leveldb/</id>
    <published>2017-03-27T08:49:41.000Z</published>
    <updated>2017-05-06T10:52:50.443Z</updated>
    
    <content type="html">&lt;p&gt;LevelDB是能够处理十亿级别规模Key-Value型数据持久性存储的C++程序库，是谷歌两位大牛Jeff Dean和Sanjay Ghemawat发起的开源项目。自己已经将拜读LevelDB纳入了项目完成后的学习计划。最近偶然看到一篇关于LevelDB很好的博客，特记录下来，希望对后面的学习有帮助。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;转自：&lt;a href=&quot;http://www.ezlippi.com/blog/2014/11/leveldb.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;leveldb原理剖析&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之一：介绍&quot;&gt;&lt;a href=&quot;#LevelDB剖析之一：介绍&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之一：介绍&quot;&gt;&lt;/a&gt;LevelDB剖析之一：介绍&lt;/h3&gt;&lt;p&gt;LevelDB有如下一些特点：&lt;br&gt;首先，LevelDB是一个持久化存储的KV系统，和Redis这种内存型的KV系统不同，LevelDB不会像Redis一样狂吃内存，而是将大部分数据存储到磁盘上。&lt;br&gt;其次，LevleDB在存储数据时，是根据记录的key值有序存储的，就是说相邻的key值在存储文件中是依次顺序存储的，而应用可以自定义key大小比较函数，LevleDB会按照用户定义的比较函数依序存储这些记录。&lt;br&gt;再次，像大多数KV系统一样，LevelDB的操作接口很简单，基本操作包括写记录，读记录以及删除记录。也支持针对多条操作的原子批量操作。&lt;br&gt;另外，LevelDB支持数据快照（snapshot）功能，使得读取操作不受写操作影响，可以在读操作过程中始终看到一致的数据。&lt;br&gt;除此外，LevelDB还支持数据压缩等操作，这对于减小存储空间以及增快IO效率都有直接的帮助。&lt;br&gt;LevelDB性能非常突出，官方网站报道其随机写性能达到40万条记录每秒，而随机读性能达到6万条记录每秒。总体来说，LevelDB的写操作要大大快于读操作，而顺序读写操作则大大快于随机读写操作。      &lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之二：整体架构&quot;&gt;&lt;a href=&quot;#LevelDB剖析之二：整体架构&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之二：整体架构&quot;&gt;&lt;/a&gt;LevelDB剖析之二：整体架构&lt;/h3&gt;&lt;p&gt;LevelDB本质上是一套存储系统以及在这套存储系统上提供的一些操作接口。为了便于理解整个系统及其处理流程，我们可以从两个不同的角度来看待LevleDB：静态角度和动态角度。从静态角度，可以假想整个系统正在运行过程中（不断插入删除读取数据），此时我们给LevelDB照相，从照片可以看到之前系统的数据在内存和磁盘中是如何分布的，处于什么状态等；从动态的角度，主要是了解系统是如何写入一条记录，读出一条记录，删除一条记录的，同时也包括除了这些接口操作外的内部操作比如compaction，系统运行时崩溃后如何恢复系统等等方面。&lt;br&gt;本节所讲的整体架构主要从静态角度来描述，之后接下来的几节内容会详述静态结构涉及到的文件或者内存数据结构，LevelDB剖析后半部分主要介绍动态视角下的LevelDB，就是说整个系统是怎么运转起来的。&lt;br&gt;LevelDB为存储系统，数据记录的存储介质包括内存以及磁盘文件，如果像上面说的，当LevelDB运行了一段时间，此时我们给LevelDB进行透视拍照，那么您会看到如下一番景象：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/CiivZ3q.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图1.1：LevelDB结构&lt;/p&gt;
&lt;p&gt;从图中可以看出，构成LevelDB静态结构的包括六个主要部分：内存中的MemTable和Immutable MemTable以及磁盘上的几种主要文件：Current文件，Manifest文件，log文件以及SSTable文件。当然，LevelDB除了这六个主要部分还有一些辅助的文件，但是以上六个文件和数据结构是LevelDB的主体构成元素。&lt;br&gt;LevelDB的Log文件和Memtable与Bigtable论文中介绍的是一致的，当应用写入一条Key:Value记录的时候，LevelDB会先往log文件里写入，成功后将记录插进Memtable中，这样基本就算完成了写入操作，因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，所以这是为何说LevelDB写入速度极快的主要原因。&lt;br&gt;Log文件在系统中的作用主要是用于系统崩溃恢复而不丢失数据，假如没有Log文件，因为写入的记录刚开始是保存在内存中的，此时如果系统崩溃，内存中的数据还没有来得及Dump到磁盘，所以会丢失数据（Redis就存在这个问题）。为了避免这种情况，LevelDB在写入内存前先将操作记录到Log文件中，然后再记入内存中，这样即使系统崩溃，也可以从Log文件中恢复内存中的Memtable，不会造成数据的丢失。&lt;br&gt;当Memtable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevleDB会生成新的Log文件和Memtable，原先的Memtable就成为Immutable Memtable，顾名思义，就是说这个Memtable的内容是不可更改的，只能读不能写入或者删除。新到来的数据被记入新的Log文件和Memtable，LevelDB后台调度会将Immutable Memtable的数据导出到磁盘，形成一个新的SSTable文件。SSTable就是由内存中的数据不断导出并进行Compaction操作后形成的，而且SSTable的所有文件是一种层级结构，第一层为Level 0，第二层为Level 1，依次类推，层级逐渐增高，这也是为何称之为LevelDB的原因。&lt;br&gt;SSTable中的文件是Key有序的，就是说在文件中小key记录排在大Key记录之前，各个Level的SSTable都是如此，但是这里需要注意的一点是：Level 0的SSTable文件（后缀为.sst）和其它Level的文件相比有特殊性：这个层级内的.sst文件，两个文件可能存在key重叠，比如有两个level 0的sst文件，文件A和文件B，文件A的key范围是：{bar, car}，文件B的Key范围是{blue,samecity}，那么很可能两个文件都存在key=”blood”的记录。对于其它Level的SSTable文件来说，则不会出现同一层级内.sst文件的key重叠现象，就是说Level L中任意两个.sst文件，那么可以保证它们的key值是不会重叠的。这点需要特别注意，后面您会看到很多操作的差异都是由于这个原因造成的。&lt;br&gt;SSTable中的某个文件属于特定层级，而且其存储的记录是key有序的，那么必然有文件中的最小key和最大key，这是非常重要的信息，LevelDB应该记下这些信息。Manifest就是干这个的，它记载了SSTable各个文件的管理信息，比如属于哪个Level，文件名称叫啥，最小key和最大key各自是多少。下图是Manifest所存储内容的示意：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/5JI6jhp.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图2.1：Manifest存储示意图&lt;/p&gt;
&lt;p&gt;图中只显示了两个文件（manifest会记载所有SSTable文件的这些信息），即Level 0的test.sst1和test.sst2文件，同时记载了这些文件各自对应的key范围，比如test.sstt1的key范围是“an”到 “banana”，而文件test.sst2的key范围是“baby”到“samecity”，可以看出两者的key范围是有重叠的。&lt;br&gt;Current文件是干什么的呢？这个文件的内容只有一个信息，就是记载当前的manifest文件名。因为在LevleDB的运行过程中，随着Compaction的进行，SSTable文件会发生变化，会有新的文件产生，老的文件被废弃，Manifest也会跟着反映这种变化，此时往往会新生成Manifest文件来记载这种变化，而Current则用来指出哪个Manifest文件才是我们关心的那个Manifest文件。&lt;br&gt;以上介绍的内容就构成了LevelDB的整体静态结构，在LevelDB剖析接下来的内容中，我们会首先介绍重要文件或者内存数据的具体数据布局与结构。&lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之三：log文件&quot;&gt;&lt;a href=&quot;#LevelDB剖析之三：log文件&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之三：log文件&quot;&gt;&lt;/a&gt;LevelDB剖析之三：log文件&lt;/h3&gt;&lt;p&gt;上节内容讲到log文件在LevelDB中的主要作用是系统故障恢复时，能够保证不会丢失数据。因为在将记录写入内存的Memtable之前，会先写入Log文件，这样即使系统发生故障，Memtable中的数据没有来得及Dump到磁盘的SSTable文件，LevelDB也可以根据log文件恢复内存的Memtable数据结构内容，不会造成系统丢失数据，在这点上LevelDB和Bigtable是一致的。&lt;br&gt;下面我们带大家看看log文件的具体物理和逻辑布局是怎样的，LevelDB对于一个log文件，会把它切割成以32K为单位的物理Block，每次读取的单位以一个Block作为基本读取单位，下图展示的log文件由3个Block构成，所以从物理布局来讲，一个log文件就是由连续的32K大小Block构成的。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/lYMLMrA.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图3.1 log文件布局  &lt;/p&gt;
&lt;p&gt;在应用的视野里是看不到这些Block的，应用看到的是一系列的Key:Value对，在LevelDB内部，会将一个Key:Value对看做一条记录的数据，另外在这个数据前增加一个记录头，用来记载一些管理信息，以方便内部处理。&lt;br&gt;记录头包含三个字段，ChechSum是对“类型”和“数据”字段的校验码，为了避免处理不完整或者是被破坏的数据，当LevelDB读取记录数据时候会对数据进行校验，如果发现和存储的CheckSum相同，说明数据完整无破坏，可以继续后续流程。“记录长度”记载了数据的大小，“数据”则是上面讲的Key:Value数值对，“类型”字段则指出了每条记录的逻辑结构和log文件物理分块结构之间的关系，具体而言，主要有以下四种类型：FULL/FIRST/MIDDLE/LAST。&lt;br&gt;如果记录类型是FULL，代表了当前记录内容完整地存储在一个物理Block里，没有被不同的物理Block切割开；如果记录被相邻的物理Block切割开，则类型会是其他三种类型中的一种。我们以图3.1所示的例子来具体说明。&lt;br&gt;假设目前存在三条记录，Record A，Record B和Record C，其中Record A大小为10K，Record B 大小为80K，Record C大小为12K，那么其在log文件中的逻辑布局会如图3.1所示。Record A是图中蓝色区域所示，因为大小为10K&amp;lt;32K，能够放在一个物理Block中，所以其类型为FULL；Record B 大小为80K，而Block 1因为放入了Record A，所以还剩下22K，不足以放下Record B，所以在Block 1的剩余部分放入Record B的开头一部分，类型标识为FIRST，代表了是一个记录的起始部分；Record B还有58K没有存储，这些只能依次放在后续的物理Block里面，因为Block 2大小只有32K，仍然放不下Record B的剩余部分，所以Block 2全部用来放Record B，且标识类型为MIDDLE，意思是这是Record B中间一段数据；Record B剩下的部分可以完全放在Block 3中，类型标识为LAST，代表了这是Record B的末尾数据；图中黄色的Record C因为大小为12K，Block 3剩下的空间足以全部放下它，所以其类型标识为FULL。&lt;br&gt;从这个小例子可以看出逻辑记录和物理Block之间的关系，LevelDB一次物理读取为一个Block，然后根据类型情况拼接出逻辑记录，供后续流程处理。&lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之四：SSTable文件&quot;&gt;&lt;a href=&quot;#LevelDB剖析之四：SSTable文件&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之四：SSTable文件&quot;&gt;&lt;/a&gt;LevelDB剖析之四：SSTable文件&lt;/h3&gt;&lt;p&gt;SSTable是Bigtable中至关重要的一块，对于LevelDB来说也是如此，对LevelDB的SSTable实现细节的了解也有助于了解Bigtable中一些实现细节。&lt;br&gt;本节内容主要讲述SSTable的静态布局结构，我们曾在“LevelDB剖析之二：整体架构”中说过，SSTable文件形成了不同Level的层级结构，至于这个层级结构是如何形成的我们放在后面Compaction一节细说。本节主要介绍SSTable某个文件的物理布局和逻辑布局结构，这对了解LevelDB的运行过程很有帮助。&lt;br&gt;LevelDB同层级有很多SSTable文件（以后缀.sst为特征），所有.sst文件内部布局都是一样的。上节介绍Log文件是物理分块的，SSTable也一样会将文件划分为固定大小的物理存储块，但是两者逻辑布局大不相同，根本原因是：Log文件中的记录是Key无序的，即先后记录的key大小没有明确大小关系，而.sst文件内部则是根据记录的Key由小到大排列的，从下面介绍的SSTable布局可以体会到Key有序是为何如此设计.sst文件结构的关键。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/zM7RpqK.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图4.1 .sst文件的分块结构  &lt;/p&gt;
&lt;p&gt;图4.1展示了一个.sst文件的物理划分结构，同Log文件一样，也是划分为固定大小的存储块，每个Block分为三个部分，红色部分是数据存储区， 蓝色的Type区用于标识数据存储区是否采用了数据压缩算法（Snappy压缩或者无压缩两种），CRC部分则是数据校验码，用于判别数据是否在生成和传输中出错。&lt;br&gt;以上是.sst的物理布局，下面介绍.sst文件的逻辑布局，所谓逻辑布局，就是说尽管大家都是物理块，但是每一块存储什么内容，内部又有什么结构等。图4.2展示了.sst文件的内部逻辑解释。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/IOpRF3E.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图4.2 逻辑布局   &lt;/p&gt;
&lt;p&gt;从图4.2可以看出，从大的方面，可以将.sst文件划分为数据存储区和数据管理区，数据存储区存放实际的Key:Value数据，数据管理区则提供一些索引指针等管理数据，目的是更快速便捷的查找相应的记录。两个区域都是在上述的分块基础上的，就是说文件的前面若干块实际存储KV数据，后面数据管理区存储管理数据。管理数据又分为四种不同类型：紫色的Meta Block，红色的MetaBlock 索引和蓝色的数据索引块以及一个文件尾部块。&lt;br&gt;LevelDB 1.2版对于Meta Block尚无实际使用，只是保留了一个接口，估计会在后续版本中加入内容，下面我们看看数据索引区和文件尾部Footer的内部结构。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/I141rvW.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图4.3 数据索引   &lt;/p&gt;
&lt;p&gt;图4.3是数据索引的内部结构示意图。再次强调一下，Data Block内的KV记录是按照Key由小到大排列的，数据索引区的每条记录是对某个Data Block建立的索引信息，每条索引信息包含三个内容，以图4.3所示的数据块i的索引Index i来说：红色部分的第一个字段记载大于等于数据块i中最大的Key值的那个Key，第二个字段指出数据块i在.sst文件中的起始位置，第三个字段指出Data Block i的大小（有时候是有数据压缩的）。后面两个字段好理解，是用于定位数据块在文件中的位置的，第一个字段需要详细解释一下，在索引里保存的这个Key值未必一定是某条记录的Key,以图4.3的例子来说，假设数据块i 的最小Key=“samecity”，最大Key=“the best”;数据块i+1的最小Key=“the fox”,最大Key=“zoo”,那么对于数据块i的索引Index i来说，其第一个字段记载大于等于数据块i的最大Key(“the best”)同时要小于数据块i+1的最小Key(“the fox”)，所以例子中Index i的第一个字段是：“the c”，这个是满足要求的；而Index i+1的第一个字段则是“zoo”，即数据块i+1的最大Key。&lt;br&gt;metaindex_handle指出了metaindex block的起始位置和大小；index、_handle指出了index Block的起始地址和大小；这两个字段可以理解为索引的索引，是为了正确读出索引值而设立的，后面跟着一个填充区和魔数。&lt;br&gt;上面主要介绍的是数据管理区的内部结构，下面我们看看数据区的一个Block的数据部分内部是如何布局的（图4.1中的红色部分），图4.4是其内部布局示意图。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/UB90CaD.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图4.4 数据Block内部结构    &lt;/p&gt;
&lt;p&gt;从图中可以看出，其内部也分为两个部分，前面是一个个KV记录，其顺序是根据Key值由小到大排列的，在Block尾部则是一些“重启点”（Restart Point）,其实是一些指针，指出Block内容中的一些记录位置。&lt;br&gt;“重启点”是干什么的呢？我们一再强调，Block内容里的KV记录是按照Key大小有序的，这样的话，相邻的两条记录很可能Key部分存在重叠，比如key i=“the Car”，Key i+1=“the color”,那么两者存在重叠部分“the c”，为了减少Key的存储量，Key i+1可以只存储和上一条Key不同的部分“olor”，两者的共同部分从Key i中可以获得。记录的Key在Block内容部分就是这么存储的，主要目的是减少存储开销。“重启点”的意思是：在这条记录开始，不再采取只记载不同的Key部分，而是重新记录所有的Key值，假设Key i+1是一个重启点，那么Key里面会完整存储“the color”，而不是采用简略的“olor”方式。Block尾部就是指出哪些记录是这些重启点的。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/fApkG8i.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图4.5 记录格式   &lt;/p&gt;
&lt;p&gt;在Block内容区，每个KV记录的内部结构是怎样的？图4.5给出了其详细结构，每个记录包含5个字段：key共享长度，比如上面的“olor”记录， 其key和上一条记录共享的Key部分长度是“the c”的长度，即5；key非共享长度，对于“olor”来说，是4；value长度指出Key:Value中Value的长度，在后面的Value内容字段中存储实际的Value值；而key非共享内容则实际存储“olor”这个Key字符串。&lt;br&gt;上面讲的这些就是.sst文件的全部内部奥秘。&lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之五：MemTable详解&quot;&gt;&lt;a href=&quot;#LevelDB剖析之五：MemTable详解&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之五：MemTable详解&quot;&gt;&lt;/a&gt;LevelDB剖析之五：MemTable详解&lt;/h3&gt;&lt;p&gt;LevelDB剖析前述小节大致讲述了磁盘文件相关的重要静态结构，本小节讲述内存中的数据结构Memtable，Memtable在整个体系中的重要地位也不言而喻。总体而言，所有KV数据都是存储在Memtable，Immutable Memtable和SSTable中的，Immutable Memtable从结构上讲和Memtable是完全一样的，区别仅仅在于其是只读的，不允许写入操作，而Memtable则是允许写入和读取的。当Memtable写入的数据占用内存到达指定数量，则自动转换为Immutable Memtable，等待Dump到磁盘中，系统会自动生成新的Memtable供写操作写入新数据，理解了Memtable，那么Immutable Memtable自然不在话下。&lt;br&gt;LevelDB的MemTable提供了将KV数据写入，删除以及读取KV记录的操作接口，但是事实上Memtable并不存在真正的删除操作,删除某个Key的Value在Memtable内是作为插入一条记录实施的，但是会打上一个Key的删除标记，真正的删除操作是Lazy的，会在以后的Compaction过程中去掉这个KV。&lt;br&gt;需要注意的是，LevelDB的Memtable中KV对是根据Key大小有序存储的，在系统插入新的KV时，LevelDB要把这个KV插到合适的位置上以保持这种Key有序性。其实，LevelDB的Memtable类只是一个接口类，真正的操作是通过背后的SkipList来做的，包括插入操作和读取操作等，所以Memtable的核心数据结构是一个SkipList。&lt;br&gt;SkipList是平衡树的一种替代数据结构，但是和红黑树不相同的是，SkipList对于树的平衡的实现是基于一种随机化的算法的，这样也就是说SkipList的插入和删除的工作是比较简单的。LevelDB的SkipList基本上是一个具体实现，并无特殊之处。&lt;br&gt;SkipList不仅是维护有序数据的一个简单实现，而且相比较平衡树来说，在插入数据的时候可以避免频繁的树节点调整操作，所以写入效率是很高的，LevelDB整体而言是个高写入系统，SkipList在其中应该也起到了很重要的作用。Redis为了加快插入操作，也使用了SkipList来作为内部实现数据结构。 &lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之六：写入与删除记录&quot;&gt;&lt;a href=&quot;#LevelDB剖析之六：写入与删除记录&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之六：写入与删除记录&quot;&gt;&lt;/a&gt;LevelDB剖析之六：写入与删除记录&lt;/h3&gt;&lt;p&gt;在之前的五节LevelDB剖析中，我们介绍了LevelDB的一些静态文件及其详细布局，从本节开始，我们看看LevelDB的一些动态操作，比如读写记录，Compaction，错误恢复等操作。&lt;br&gt;本节介绍levelDB的记录更新操作，即插入一条KV记录或者删除一条KV记录。levelDB的更新操作速度是非常快的，源于其内部机制决定了这种更新操作的简单性。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/buYgI4P.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图6.1 LevelDB写入记录 &lt;/p&gt;
&lt;p&gt;图6.1是levelDB如何更新KV数据的示意图，从图中可以看出，对于一个插入操作Put(Key,Value)来说，完成插入操作包含两个具体步骤：首先是将这条KV记录以顺序写的方式追加到之前介绍过的log文件末尾，因为尽管这是一个磁盘读写操作，但是文件的顺序追加写入效率是很高的，所以并不会导致写入速度的降低；第二个步骤是:如果写入log文件成功，那么将这条KV记录插入内存中的Memtable中，前面介绍过，Memtable只是一层封装，其内部其实是一个Key有序的SkipList列表，插入一条新记录的过程也很简单，即先查找合适的插入位置，然后修改相应的链接指针将新记录插入即可。完成这一步，写入记录就算完成了，所以一个插入记录操作涉及一次磁盘文件追加写和内存SkipList插入操作，这是为何levelDB写入速度如此高效的根本原因。&lt;br&gt;从上面的介绍过程中也可以看出：log文件内是key无序的，而Memtable中是key有序的。那么如果是删除一条KV记录呢？对于levelDB来说，并不存在立即删除的操作，而是与插入操作相同的，区别是，插入操作插入的是Key:Value 值，而删除操作插入的是“Key:删除标记”，并不真正去删除记录，而是后台Compaction的时候才去做真正的删除操作。&lt;br&gt;levelDB的写入操作就是如此简单。真正的麻烦在后面将要介绍的读取操作中。 &lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之七：读取记录&quot;&gt;&lt;a href=&quot;#LevelDB剖析之七：读取记录&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之七：读取记录&quot;&gt;&lt;/a&gt;LevelDB剖析之七：读取记录&lt;/h3&gt;&lt;p&gt;LevelDB是针对大规模Key/Value数据的单机存储库，从应用的角度来看，LevelDB就是一个存储工具。而作为称职的存储工具，常见的调用接口无非是新增KV，删除KV，读取KV，更新Key对应的Value值这么几种操作。LevelDB的接口没有直接支持更新操作的接口，如果需要更新某个Key的Value,你可以选择直接生猛地插入新的KV，保持Key相同，这样系统内的key对应的value就会被更新；或者你可以先删除旧的KV， 之后再插入新的KV，这样比较委婉地完成KV的更新操作。&lt;br&gt;假设应用提交一个Key值，下面我们看看LevelDB是如何从存储的数据中读出其对应的Value值的。图7-1是LevelDB读取过程的整体示意图。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/FF4f9KB.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图7-1  LevelDB读取记录流程  &lt;/p&gt;
&lt;p&gt;LevelDB首先会去查看内存中的Memtable，如果Memtable中包含key及其对应的value，则返回value值即可；如果在Memtable没有读到key，则接下来到同样处于内存中的Immutable Memtable中去读取，类似地，如果读到就返回，若是没有读到,那么只能万般无奈下从磁盘中的大量SSTable文件中查找。因为SSTable数量较多，而且分成多个Level，所以在SSTable中读数据是相当蜿蜒曲折的一段旅程。总的读取原则是这样的：首先从属于level 0的文件中查找，如果找到则返回对应的value值，如果没有找到那么到level 1中的文件中去找，如此循环往复，直到在某层SSTable文件中找到这个key对应的value为止（或者查到最高level，查找失败，说明整个系统中不存在这个Key)。&lt;br&gt;那么为什么是从Memtable到Immutable Memtable，再从Immutable Memtable到文件，而文件中为何是从低level到高level这么一个查询路径呢？道理何在？之所以选择这么个查询路径，是因为从信息的更新时间来说，很明显Memtable存储的是最新鲜的KV对；Immutable Memtable中存储的KV数据对的新鲜程度次之；而所有SSTable文件中的KV数据新鲜程度一定不如内存中的Memtable和Immutable Memtable的。对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。也就是说，上面列出的查找路径就是按照数据新鲜程度排列出来的，越新鲜的越先查找。&lt;br&gt;为啥要优先查找新鲜的数据呢？这个道理不言而喻，举个例子。比如我们先往levelDB里面插入一条数据 {key=”www.samecity.com”  value=”我们”},过了几天，samecity网站改名为：69同城，此时我们插入数据{key=”www.samecity.com”  value=”69同城”}，同样的key,不同的value；逻辑上理解好像levelDB中只有一个存储记录，即第二个记录，但是在levelDB中很可能存在两条记录，即上面的两个记录都在levelDB中存储了，此时如果用户查询key=”www.samecity.com”,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。&lt;br&gt;前文有讲：对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。这是一个结论，理论上需要一个证明过程，否则会招致如下的问题：为神马呢？从道理上讲呢，很明白：因为Level L+1的数据不是从石头缝里蹦出来的，也不是做梦梦到的，那它是从哪里来的？Level L+1的数据是从Level L 经过Compaction后得到的（如果您不知道什么是Compaction，那么……..也许以后会知道的），也就是说，您看到的现在的Level L+1层的SSTable数据是从原来的Level L中来的，现在的Level L比原来的Level L数据要新鲜，所以可证，现在的Level L比现在的Level L+1的数据要新鲜。&lt;br&gt;SSTable文件很多，如何快速地找到key对应的value值？在LevelDB中，level 0一直都爱搞特殊化，在level 0和其它level中查找某个key的过程是不一样的。因为level 0下的不同文件可能key的范围有重叠，某个要查询的key有可能多个文件都包含，这样的话LevelDB的策略是先找出level 0中哪些文件包含这个key（manifest文件中记载了level和对应的文件及文件里key的范围信息，LevelDB在内存中保留这种映射表）， 之后按照文件的新鲜程度排序，新的文件排在前面，之后依次查找，读出key对应的value。而如果是非level 0的话，因为这个level的文件之间key是不重叠的，所以只从一个文件就可以找到key对应的value。&lt;br&gt;最后一个问题,如果给定一个要查询的key和某个key range包含这个key的SSTable文件，那么levelDB是如何进行具体查找过程的呢？levelDB一般会先在内存中的Cache中查找是否包含这个文件的缓存记录，如果包含，则从缓存中读取；如果不包含，则打开SSTable文件，同时将这个文件的索引部分加载到内存中并放入Cache中。 这样Cache里面就有了这个SSTable的缓存项，但是只有索引部分在内存中，之后levelDB根据索引可以定位到哪个内容Block会包含这条key，从文件中读出这个Block的内容，在根据记录一一比较，如果找到则返回结果，如果没有找到，那么说明这个level的SSTable文件并不包含这个key，所以到下一级别的SSTable中去查找。&lt;br&gt;从之前介绍的LevelDB的写操作和这里介绍的读操作可以看出，相对写操作，读操作处理起来要复杂很多，所以写的速度必然要远远高于读数据的速度，也就是说，LevelDB比较适合写操作多于读操作的应用场合。而如果应用是很多读操作类型的，那么顺序读取效率会比较高，因为这样大部分内容都会在缓存中找到，尽可能避免大量的随机读取操作。     &lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之八：Compaction操作&quot;&gt;&lt;a href=&quot;#LevelDB剖析之八：Compaction操作&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之八：Compaction操作&quot;&gt;&lt;/a&gt;LevelDB剖析之八：Compaction操作&lt;/h3&gt;&lt;p&gt;前文有述，对于LevelDB来说，写入记录操作很简单，删除记录仅仅写入一个删除标记就算完事，但是读取记录比较复杂，需要在内存以及各个层级文件中依照新鲜程度依次查找，代价很高。为了加快读取速度，levelDB采取了compaction的方式来对已有的记录进行整理压缩，通过这种方式，来删除掉一些不再有效的KV数据，减小数据规模，减少文件数量等。&lt;br&gt;levelDB的compaction机制和过程与Bigtable所讲述的是基本一致的，Bigtable中讲到三种类型的compaction: minor ，major和full。所谓minor Compaction，就是把memtable中的数据导出到SSTable文件中；major compaction就是合并不同层级的SSTable文件，而full compaction就是将所有SSTable进行合并。&lt;br&gt;LevelDB包含其中两种，minor和major。先来看看minor Compaction的过程。minor compaction 的目的是当内存中的memtable大小到了一定值时，将内容保存到磁盘文件中，图8.1是其机理示意图。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/9nVKQV2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图8.1 minor compaction    &lt;/p&gt;
&lt;p&gt;从8.1可以看出，当memtable数量到了一定程度会转换为immutable memtable，此时不能往其中写入记录，只能从中读取KV内容。之前介绍过，immutable memtable其实是一个多层级队列SkipList，其中的记录是根据key有序排列的。所以这个minor compaction实现起来也很简单，就是按照immutable memtable中记录由小到大遍历，并依次写入一个level 0 的新建SSTable文件中，写完后建立文件的index 数据，这样就完成了一次minor compaction。从图中也可以看出，对于被删除的记录，在minor compaction过程中并不真正删除这个记录，原因也很简单，这里只知道要删掉key记录，但是这个KV数据在哪里?那需要复杂的查找，所以在minor compaction的时候并不做删除，只是将这个key作为一个记录写入文件中，至于真正的删除操作，在以后更高层级的compaction中会去做。&lt;br&gt;当某个level下的SSTable文件数目超过一定设置值后，levelDB会从这个level的SSTable中选择一个文件（level&amp;gt;0），将其和高一层级的level+1的SSTable文件合并，这就是major compaction。&lt;br&gt;我们知道在大于0的层级中，每个SSTable文件内的Key都是由小到大有序存储的，而且不同文件之间的key范围（文件内最小key和最大key之间）不会有任何重叠。Level 0的SSTable文件有些特殊，尽管每个文件也是根据Key由小到大排列，但是因为level 0的文件是通过minor compaction直接生成的，所以任意两个level 0下的两个sstable文件可能再key范围上有重叠。所以在做major compaction的时候，对于大于level 0的层级，选择其中一个文件就行，但是对于level 0来说，指定某个文件后，本level中很可能有其他SSTable文件的key范围和这个文件有重叠，这种情况下，要找出所有有重叠的文件和level 1的文件进行合并，即level 0在进行文件选择的时候，可能会有多个文件参与major compaction。&lt;br&gt;LevelDB在选定某个level进行compaction后，还要选择是具体哪个文件要进行compaction，LevelDB在这里有个小技巧， 就是说轮流来，比如这次是文件A进行compaction，那么下次就是在key range上紧挨着文件A的文件B进行compaction，这样每个文件都会有机会轮流和高层的level 文件进行合并。&lt;br&gt;如果选好了level L的文件A和level L+1层的文件进行合并，那么问题又来了，应该选择level L+1哪些文件进行合并？LevelDB选择L+1层中和文件A在key range上有重叠的所有文件来和文件A进行合并。&lt;br&gt;也就是说，选定了level L的文件A,之后在level L+1中找到了所有需要合并的文件B,C,D…等等。剩下的问题就是具体是如何进行major合并的？就是说给定了一系列文件，每个文件内部是key有序的，如何对这些文件进行合并，使得新生成的文件仍然Key有序，同时抛掉哪些不再有价值的KV数据。&lt;br&gt;图8.2说明了这一过程。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/abldSaG.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图8.2 SSTable Compaction &lt;/p&gt;
&lt;p&gt;Major compaction的过程如下：对多个文件采用多路归并排序的方式，依次找出其中最小的Key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个Key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入level L+1层中新生成的一个SSTable文件中。就这样对KV数据一一处理，形成了一系列新的L+1层数据文件，之前的L层文件和L+1层参与compaction 的文件数据此时已经没有意义了，所以全部删除。这样就完成了L层和L+1层文件记录的合并过程。&lt;br&gt;那么在major compaction过程中，判断一个KV记录是否抛弃的标准是什么呢？其中一个标准是:对于某个key来说，如果在小于L层中存在这个Key，那么这个KV在major compaction过程中可以抛掉。因为我们前面分析过，对于层级低于L的文件中如果存在同一Key的记录，那么说明对于Key来说，有更新鲜的Value存在，那么过去的Value就等于没有意义了，所以可以删除。&lt;/p&gt;
&lt;p&gt;##LevelDB剖析之九：LevelDB中的Cache&lt;br&gt;书接前文，前面讲过对于LevelDB来说，读取操作如果没有在内存的memtable中找到记录，要多次进行磁盘访问操作。假设最优情况，即第一次就在level 0中最新的文件中找到了这个key，那么也需要读取2次磁盘，一次是将SSTable的文件中的index部分读入内存，这样根据这个index可以确定key是在哪个block中存储；第二次是读入这个block的内容，然后在内存中查找key对应的value。&lt;br&gt;LevelDB中引入了两个不同的Cache:Table Cache和Block Cache。其中Block Cache是配置可选的，即在配置文件中指定是否打开这个功能。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/8ZUqBxO.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;图9.1 table cache &lt;/p&gt;
&lt;p&gt;图9.1是table cache的结构。在Cache中，key值是SSTable的文件名称，Value部分包含两部分，一个是指向磁盘打开的SSTable文件的文件指针，这是为了方便读取内容；另外一个是指向内存中这个SSTable文件对应的Table结构指针，table结构在内存中，保存了SSTable的index内容以及用来指示block cache用的cache_id ,当然除此外还有其它一些内容。&lt;br&gt;比如在get(key)读取操作中，如果LevelDB确定了key在某个level下某个文件A的key range范围内，那么需要判断是不是文件A真的包含这个KV。此时，LevelDB会首先查找Table Cache，看这个文件是否在缓存里，如果找到了，那么根据index部分就可以查找是哪个block包含这个key。如果没有在缓存中找到文件，那么打开SSTable文件，将其index部分读入内存，然后插入Cache里面，去index里面定位哪个block包含这个Key 。如果确定了文件哪个block包含这个key，那么需要读入block内容，这是第二次读取。&lt;br&gt;Block Cache是为了加快这个过程的。其中的key是文件的cache_id加上这个block在文件中的起始位置block_offset。而value则是这个Block的内容。&lt;br&gt;如果LevelDB发现这个block在block cache中，那么可以避免读取数据，直接在cache里的block内容里面查找key的value就行，如果没找到呢？那么读入block内容并把它插入block cache中。LevelDB就是这样通过两个cache来加快读取速度的。从这里可以看出，如果读取的数据局部性比较好，也就是说要读的数据大部分在cache里面都能读到，那么读取效率应该还是很高的，而如果是对key进行顺序读取效率也应该不错，因为一次读入后可以多次被复用。但是如果是随机读取，您可以推断下其效率如何。     &lt;/p&gt;
&lt;h3 id=&quot;LevelDB剖析之十：Version、VersionEdit、VersionSet&quot;&gt;&lt;a href=&quot;#LevelDB剖析之十：Version、VersionEdit、VersionSet&quot; class=&quot;headerlink&quot; title=&quot;LevelDB剖析之十：Version、VersionEdit、VersionSet&quot;&gt;&lt;/a&gt;LevelDB剖析之十：Version、VersionEdit、VersionSet&lt;/h3&gt;&lt;p&gt;Version保存了当前磁盘以及内存中所有的文件信息，一般只有一个Version叫做”current” version（当前版本）。LevelDB还保存了一系列的历史版本，这些历史版本有什么作用呢？&lt;br&gt;当一个Iterator创建后，Iterator就引用到了current version(当前版本)，只要这个Iterator不被delete那么被Iterator引用的版本就会一直存活。这就意味着当你用完一个Iterator后，需要及时删除它。&lt;br&gt;当一次Compaction结束后（会生成新的文件，合并前的文件需要删除），LevelDB会创建一个新的版本作为当前版本，原先的当前版本就会变为历史版本。&lt;br&gt;VersionSet是所有Version的集合，管理着所有存活的Version。&lt;br&gt;VersionEdit 表示Version之间的变化，相当于delta 增量，表示有增加了多少文件，删除了文件。他们之间的关系为：Version0 +VersionEdit–&amp;gt;Version1。VersionEdit会保存到MANIFEST文件中，当做数据恢复时就会从MANIFEST文件中读出来重建数据。&lt;br&gt;LevelDB的这种版本的控制，让我想到了双buffer切换，双buffer切换来自于图形学中，用于解决屏幕绘制时的闪屏问题，在服务器编程中也有用处。比如我们的服务器上有一个字典库，每天我们需要更新这个字典库，我们可以新开一个buffer，将新的字典库加载到这个新buffer中，等到加载完毕，将字典的指针指向新的字典库。&lt;br&gt;LevelDB的version管理和双buffer切换类似，但是如果原version被某个iterator引用，那么这个version会一直保持，直到没有被任何一个iterator引用，此时就可以删除这个version。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;LevelDB是能够处理十亿级别规模Key-Value型数据持久性存储的C++程序库，是谷歌两位大牛Jeff Dean和Sanjay Ghemawat发起的开源项目。自己已经将拜读LevelDB纳入了项目完成后的学习计划。最近偶然看到一篇关于LevelDB很好的博客，特记录下来，希望对后面的学习有帮助。&lt;br&gt;
    
    </summary>
    
    
      <category term="LevelDB" scheme="http://blog.dujiong.net/tags/LevelDB/"/>
    
  </entry>
  
  <entry>
    <title>CentOS配置双网卡（Mellanox和传统以太网卡）</title>
    <link href="http://blog.dujiong.net/2017/03/20/config-two-network-interfaces/"/>
    <id>http://blog.dujiong.net/2017/03/20/config-two-network-interfaces/</id>
    <published>2017-03-20T09:57:25.000Z</published>
    <updated>2017-05-09T04:52:29.314Z</updated>
    
    <content type="html">&lt;p&gt;刚给服务器装上了操作系统，期待已久的Mellanox网卡终于到了，关于该网卡这里就不做过多的介绍了，可以参考&lt;a href=&quot;http://www.mellanox.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Mellanox官网&lt;/a&gt;，下面记录在服务器上Mellanox网卡和传统以太网的配置。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安插Mellanox网卡并安装相应驱动&quot;&gt;&lt;a href=&quot;#安插Mellanox网卡并安装相应驱动&quot; class=&quot;headerlink&quot; title=&quot;安插Mellanox网卡并安装相应驱动&quot;&gt;&lt;/a&gt;安插Mellanox网卡并安装相应驱动&lt;/h3&gt;&lt;p&gt;首先，在服务器背板安插Mellanox网卡，然后执行&lt;code&gt;ifconfig&lt;/code&gt;查看是否多了一张新的网卡。初始状态下&lt;code&gt;ifconfig&lt;/code&gt;看到的Mellanox网卡的名字是ib0，这说明该网卡的默认链路层协议时InfiniBand协议，而不是Ethernet协议。由于我们后续需要使用NVMe over RoCEv2，所以需要将Mellanox的链路层协议改为Ethernet。因此，需要先下载并安装相应的驱动。&lt;br&gt;在官网上确定好操作系统对应的驱动版本号（比如CentOS7.2对应的版本是MLNX_OFED-3.4-2.0.0.0-rhel7.2-x86_64）后，下载并安装驱动。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-3.4-2.0.0.0/MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64.tgz
tar -xvf MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64.tgz
cd MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64/
./mlnxofedinstall  (--add-kernel-support)       ###kernel 4.8.15可能需要添加特定的选项
/etc/init.d/openibd restart
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;修改Mellanox网卡配置&quot;&gt;&lt;a href=&quot;#修改Mellanox网卡配置&quot; class=&quot;headerlink&quot; title=&quot;修改Mellanox网卡配置&quot;&gt;&lt;/a&gt;修改Mellanox网卡配置&lt;/h3&gt;&lt;p&gt;如前文所述，Mellanox网卡的默认链路层协议是Infiniband协议，不是Ethernet协议。所以，接下来修改网卡的链路层协议。&lt;br&gt;在修改之前，可以先通过&lt;code&gt;lspci | grep Mellanox&lt;/code&gt;查看网卡信息，确定是否为InfiniBand协议。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mst start
ibv_devinfo | grep vendor_part_id    ##得到vendor_part_id(我这里是4115)
mlxconfig -d /dev/mst/mt4115_pciconf0 q  ##再次查询网卡信息
mlxconfig -d /dev/mst/mt4115_pciconf0 set LINK_TYPE_P1=2  ##修改为Ethernet协议
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改并重新启动后，可以看到网卡的名称发生了变化（这里是enp133s0）。最后，使用&lt;code&gt;lspci | grep Mellanox&lt;/code&gt;确认修改。&lt;/p&gt;
&lt;h3 id=&quot;CentOS双网卡配置&quot;&gt;&lt;a href=&quot;#CentOS双网卡配置&quot; class=&quot;headerlink&quot; title=&quot;CentOS双网卡配置&quot;&gt;&lt;/a&gt;CentOS双网卡配置&lt;/h3&gt;&lt;p&gt;现在服务器的网卡连接情况是一张Mellanox网卡通过专用网线连接到EdgeCore 100G白牌交换机上，另外一张传统以太网卡连接到以太网交换机（192.168.1.1）。接下来双网卡配置双IP，一个192.168.1.x，可通过以太网交换机连接外网，另一个192.168.5.x，连接到100G白牌交换机。   &lt;/p&gt;
&lt;p&gt;以太网卡在服务器上对应的名称为eno1。修改其配置文件。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/sysconfig/network-scripts/ifcfg-eth0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改和添加以下内容：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;BOOTPROTO=static
HWADDR=6c:92:bf:42:27:2e    #mac address
IPADDR=192.168.1.152
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
ONBOOT=yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改后的配置文件如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/WqcbomA.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;通过&lt;code&gt;service network restart&lt;/code&gt;重启网络后，就成功的将该以太网卡设置IP设置成了静态IP（192.168.1.152）。&lt;/p&gt;
&lt;p&gt;Mellanox网卡的配置和以太网卡一样。所以，这里只列举出修改后的配置文件（/etc/sysconfig/network-scripts/ifcfg-enp133s0）&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/jcLC6h9.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;配置好后的服务器网络接口情况如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/rNo8Xe3.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;配置路由表&quot;&gt;&lt;a href=&quot;#配置路由表&quot; class=&quot;headerlink&quot; title=&quot;配置路由表&quot;&gt;&lt;/a&gt;配置路由表&lt;/h3&gt;&lt;p&gt;目前系统默认网关是192.168.1.1，所以需要增加两个路由表，实现双网关正常访问。&lt;br&gt;    vim /etc/iproute2/rt_tables&lt;br&gt;增加两行内容：&lt;br&gt;    252 net2&lt;br&gt;    251 net3&lt;br&gt;在/etc/rc.local添加静态路由规则。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip route flush table net2
ip route add default via 192.168.1.1 dev eno1 src 192.168.1.152 table net2
ip rule add from 192.168.1.152 table net2

ip route flush table net3
ip route add default via 192.168.5.1 dev enp133s0 src 192.168.5.86 table net3
ip rule add from 192.168.5.86 table net3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时，双网卡双IP应该就配置好了，&lt;code&gt;service network restart&lt;/code&gt;重启网络就可以实现不同网卡对应不同网络访问。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;刚给服务器装上了操作系统，期待已久的Mellanox网卡终于到了，关于该网卡这里就不做过多的介绍了，可以参考&lt;a href=&quot;http://www.mellanox.com/&quot;&gt;Mellanox官网&lt;/a&gt;，下面记录在服务器上Mellanox网卡和传统以太网的配置。&lt;br&gt;
    
    </summary>
    
    
      <category term="network/linux" scheme="http://blog.dujiong.net/tags/network-linux/"/>
    
      <category term="NVMe over RDMA" scheme="http://blog.dujiong.net/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>RDMA技术浅析</title>
    <link href="http://blog.dujiong.net/2017/02/27/RDMA/"/>
    <id>http://blog.dujiong.net/2017/02/27/RDMA/</id>
    <published>2017-02-27T02:22:39.000Z</published>
    <updated>2017-05-05T12:11:21.883Z</updated>
    
    <content type="html">&lt;p&gt;RDMA，即Remote DMA，最直观的解释就是将发生在本机的直接内存访问扩展到主机与主机之间。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;DMA&quot;&gt;&lt;a href=&quot;#DMA&quot; class=&quot;headerlink&quot; title=&quot;DMA&quot;&gt;&lt;/a&gt;DMA&lt;/h3&gt;&lt;p&gt;首先，对DMA技术做简单的复习和总结。&lt;br&gt;在最初的PC体系结构中，CPU是系统中唯一的总线主控器，也就是说，为了提取和存储RAM存储单元的值，CPU是唯一可以驱动地址/数据总线的硬件设备。而随着更多诸如PCI的现代总线体系结构的出现，如果提供合适的电路，每一个外围设备都可以充当总线主控器。因此，现在所有的PC都包含一个辅助的DMA电路，它可以用来控制在RAM和IO设备之间数据的传送。DMA一旦被CPU激活，就可以自行传送数据；在数据传输完成之后，DMA发出一个中断请求，再由CPU接管。当CPU和DMA同时访问同一内存单元时，所产生的的冲突由一个名为内存仲裁器的硬件电路解决。&lt;br&gt;由于DMA的设置时间比较长，所以使用DMA最多的是磁盘驱动器和其他需要一次传送大量字节的设备，而在传送数量很少的数据时直接使用CPU效率更高。   &lt;/p&gt;
&lt;h3 id=&quot;RDMA&quot;&gt;&lt;a href=&quot;#RDMA&quot; class=&quot;headerlink&quot; title=&quot;RDMA&quot;&gt;&lt;/a&gt;RDMA&lt;/h3&gt;&lt;p&gt;传统的TCP/IP技术在数据包处理过程中，要经过操作系统及其他软件层，需要占用大量的服务器资源和内存总线带宽，数据在系统内存、处理器缓存和网络控制器缓存之间来回进行复制移动，给服务器的CPU和内存造成了沉重负担。尤其是网络带宽、处理器速度与内存带宽三者的严重”不匹配性”，更加剧了网络延迟效应。&lt;br&gt;RDMA是一种新的内存访问技术，RDMA让计算机可以直接存取其他计算机的内存，而不需要经过处理器耗时的处理。RDMA将数据从一个系统快速移动到远程系统存储器中，而不对操作系统造成任何影响。&lt;br&gt;RDMA技术的原理及其与TCP/IP架构的对比如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/G5E7u7t.png&quot; alt=&quot;&quot;&gt;  &lt;/p&gt;
&lt;p&gt;因此，RDMA可以简单理解为利用相关的硬件和网络技术，服务器1的网卡可以直接读写服务器2的内存，最终达到高带宽、低延迟和低资源利用率的效果。如下图所示，应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/cFfY963.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在实现上，RDMA实际上是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过在网卡上将RDMA协议固化于硬件，以及支持零复制网络技术和内核内存旁路技术这两种途径来达到其高性能的远程直接数据存取的目标。&lt;br&gt;（1）零复制：零复制网络技术使网卡可以直接与应用内存相互传输数据，从而消除了在应用内存与内核之间复制数据的需要。因此，传输延迟会显著减小。&lt;br&gt;（2）内核旁路：内核协议栈旁路技术使应用程序无需执行内核内存调用就可向网卡发送命令。在不需要任何内核内存参与的条件下，RDMA请求从用户空间发送到本地网卡并通过网络发送给远程网卡，这就减少了在处理网络传输流时内核内存空间与用户空间之间环境切换的次数。&lt;/p&gt;
&lt;p&gt;在具体的远程内存读写中，RDMA操作用于读写操作的远程虚拟内存地址包含在RDMA消息中传送，远程应用程序要做的只是在其本地网卡中注册相应的内存缓冲区。远程节点的CPU除在连接建立、注册调用等之外，在整个RDMA数据传输过程中并不提供服务，因此没有带来任何负载。&lt;/p&gt;
&lt;h3 id=&quot;RDMA的不同实现&quot;&gt;&lt;a href=&quot;#RDMA的不同实现&quot; class=&quot;headerlink&quot; title=&quot;RDMA的不同实现&quot;&gt;&lt;/a&gt;RDMA的不同实现&lt;/h3&gt;&lt;p&gt;如下图所示，RDMA的实现方式主要分为InfiniBand和Ethernet两种传输网络。而在以太网上，又可以根据与以太网融合的协议栈的差异分为iWARP和RoCE（包括RoCEv1和RoCEv2）。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/EgvDMTD.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;其中，InfiniBand是最早实现RDMA的网络协议，被广泛应用到高性能计算中。但是InfiniBand和传统TCP/IP网络的差别非常大，需要专用的硬件设备，承担昂贵的价格。鉴于此，这里不对InfiniBand做过多的讨论。&lt;br&gt;在基于以太网的版本中，下面重点选择RoCEv2来讨论。&lt;br&gt;可以看出，RoCEv2的协议栈包括IB传输层、TCP/UDP、IP和Ethernet，其中，后面三层都使用了TCP/IP中相应层次的封包格式。RoCEv2的封包格式如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/0KMeGlN.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;其中，UDP包头中，目的端口号为4791即代表是RoCEv2帧。IB BTH即InfiniBand Base Transport Header，定义了IB传输层的相应头部字段。IB Payload即为消息负载。ICRC和FCS分别对应冗余检测和帧校验。&lt;br&gt;IB BTH格式和字段定义如下图。其中，Opcode用于表明该包的type或IB payload中更高层的协议类型。S是Solicited Event的缩写，表明回应者产生应该产生一个事件。M是MigReq的缩写，一般用于迁移状态。Pad表明有多少额外字节被填充到IB payload中。TVer即Transport Header Version，表明该包的版本号。Partition Key用来表征与本packet关联的逻辑内存分区。rsvd是reserved的缩写，该字段是保留的。Destination QP表明目的端Queue Pair序号。A是Acknowledge Request，表示该packet的应答可由响应者调度。PSN是Packet Sequence Number，用来检测丢失或重复的数据包。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/uPjVonk.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;最后，顺带说下RDMA网卡的出包。如前文所述，RDMA是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过将RDMA技术固化于网卡上实现，即，在RoCEv2协议栈中，IB BTH、UDP、IP以及Ethernet Layer全是固化在网卡上的。用户空间的Application通过OFA Stack（亦或其他组织编写的RDMA stack）提供的verbs编程接口（比如WRITE、READ、SEND等）形成IB payload，接下来便直接进入硬件，由RDMA网卡实现负载的层层封装。&lt;/p&gt;
&lt;h3 id=&quot;附&quot;&gt;&lt;a href=&quot;#附&quot; class=&quot;headerlink&quot; title=&quot;附&quot;&gt;&lt;/a&gt;附&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/vwXu2Zm.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;RDMA，即Remote DMA，最直观的解释就是将发生在本机的直接内存访问扩展到主机与主机之间。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.dujiong.net/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>NVMe技术浅析</title>
    <link href="http://blog.dujiong.net/2017/02/17/NVMe/"/>
    <id>http://blog.dujiong.net/2017/02/17/NVMe/</id>
    <published>2017-02-17T12:09:40.000Z</published>
    <updated>2017-05-06T06:39:03.366Z</updated>
    
    <content type="html">&lt;p&gt;在&lt;a href=&quot;http://blog.dujiong.net/2017/02/13/storage/&quot;&gt;存储的那些词儿&lt;/a&gt;一文中已经提到了NVMe是使用PCIe通道的一种逻辑设备接口标准（是接口标准，不是接口！），本文将进一步地分析NVMe的设计及其带来的性能提升。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;NVMe设计&quot;&gt;&lt;a href=&quot;#NVMe设计&quot; class=&quot;headerlink&quot; title=&quot;NVMe设计&quot;&gt;&lt;/a&gt;NVMe设计&lt;/h3&gt;&lt;p&gt;NVMe指定了Host与SSD之间通信的命令，以及命令执行的流程。NVMe由两种命令组成，一种是Admin Command，用于Host管理和控制SSD；另外一种是I/O Command，用于Host和SSD之间数据的传输。&lt;br&gt;同ATA中定义的命令相比，NVMe的命令个数少了很多，完全就是为SSD量身定制的。这里放一张NVMe1.2支持的IO Command列表。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/INLzJpX.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;除了指定Host与SSD之间的通信命令以外，NVMe还定义了命令的执行流程。NVMe中定义了三个关键组件用于命令和数据的处理：Submission Queue（SQ）、Completion Queue（CQ）和Doorbell Register（DB）。SQ和CQ位于Host的内存中，DB位于SSD的控制器内部。在详细说明NVMe命令的处理流程之前，先上一张图宏观感受下PCIe系统和NVMe的结合。图中的NVMe Subsystem一般就是SSD，作为一个PCIe Endpoint通过PCIe连接着Root Complex（RC），然后RC连接着CPU和内存。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/vl7O3dS.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;下面通过SQ、CQ和DB这三个关键组件来总体认识NVMe是如何处理命令的。SQ位于内存中，Host要发送命令时，先把准备好的命令放在SQ中（每个命令条目大小是64字节），然后通知SSD来取；在通知中发挥作用的就是DB寄存器，Host通过写SSD端的DB寄存器来告知SSD执行命令；此外，CQ也是位于Host内存中，一个命令执行完成，成功或失败，SSD总会往CQ中写入命令完成状态（每个条目是16字节）。&lt;br&gt;下图展示了NVMe一次完整处理指令的流程，共八步：&lt;br&gt;（1）Host将指令写到SQ；&lt;br&gt;（2）Host写DB，通知SSD取指令；&lt;br&gt;（3）SSD收到通知，从SQ中取走指令；&lt;br&gt;（4）SSD执行指令；&lt;br&gt;（5）指令执行完成，SSD往CQ写入指令执行结果；&lt;br&gt;（6）SSD通知Host指令执行完成；&lt;br&gt;（7）Host收到通知，处理CQ，查看指令完成状态；&lt;br&gt;（8）Host处理完CQ中的指令执行结果，通过DB回复SSD。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/Z25eGGp.png&quot; alt=&quot;&quot;&gt;   &lt;/p&gt;
&lt;h4 id=&quot;SQ-CQ-DB详细分析&quot;&gt;&lt;a href=&quot;#SQ-CQ-DB详细分析&quot; class=&quot;headerlink&quot; title=&quot;SQ/CQ/DB详细分析&quot;&gt;&lt;/a&gt;SQ/CQ/DB详细分析&lt;/h4&gt;&lt;p&gt;如前文所述，SQ/CQ/DB是NVMe处理命令的关键。Host往SQ中写入命令，SSD往CQ中写入命令完成结果。NVMe中有两种SQ和CQ，一种是Admin SQ，用于放Admin命令；一种是I/O SQ，放I/O命令。如下图所示，系统中只有一对Admin SQ/CQ，它们是一一对应的关系；I/O SQ/CQ却可以有很多，最大值65535（64K-1）。此外，Host端每个Core可以有一个或多个SQ，但只有一个CQ，即SQ与CQ可以是一对一或多对一的关系。这样设计的原因有二：一是性能需求，一个Core中有多线程，可以做到一个线程独享一个SQ；二是QoS需求，可以对多个SQ设置不同的优先级。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/bIIfhg0.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;此外，作为队列，每个SQ和CQ都有一定的深度，对Admin SQ/CQ来说，其深度可以是2-4096（4K），对I/O SQ/CQ，队列深度可以是2-65536（64K）。&lt;br&gt;综上，NVMe中SQ/CQ的个数可以配置，每个SQ/CQ的深度也可以配置，即NVMe的性能是可以通过配置队列个数和队列深度来灵活调节的。这一点和AHCI相比是巨大的提升，因为我们知道，AHCI只有一个命令队列，且队列深度是固定的32。&lt;br&gt;说了这么多SQ和CQ，DB呢？我们知道，SQ和CQ都是队列（且是环形队列），队列的头尾很重要，头决定谁会最先被服务，尾决定新到来命令的位置。所以，我们需要记录SQ和CQ的头尾位置，这就是DB的作用之一。DB是在SSD端的寄存器，记录SQ和CQ的头尾位置。每个SQ或CQ，都有两个对应的DB：Head DB和Tail DB。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/8IzJSna.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;从上图可以看出，这是一个生产者/消费者模型。对一个SQ来说，它的生产者是Host，因为它往SQ的Tail位置写入命令，消费者是SSD，它从SQ的Head取出指令执行。CQ则刚好相反，生产者是SSD，消费者是Host。&lt;br&gt;DB的另一个作用是通知，Host更新SQ Tail DB的同时，也是在告知SSD有新的命令需要处理；Host更新CQ Head DB的同时，也是在告知SSD返回的命令完成状态信息已经被处理。&lt;br&gt;下面以一个实例来说明SQ/CQ/DB三者配合的详细过程。&lt;br&gt;（1）开始时假设SQ1和CQ1是空的，Head=Tail=0；&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/L8WPeKE.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;（2）Host往SQ1中写入三个命令，SQ1的Tail变为3。Host在往SQ1写入三个命令后，同时去更新SSD Controller的SQ1 Tail DB寄存器，值为3。Host更新这个寄存器的同时，也是在告诉SSD Controller，有新命令了。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/sRJyKnc.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;（3）SSD Controller收到通知后，于是派人去SQ1把3个命令都取回来执行。SSD把SQ1的三个命令都消费了，SQ1的Head从而也调整为3，SSD Controller会把这个Head值写入到本地的SQ1 Head DB寄存器。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/riHdErF.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;（4）SSD执行完了两个命令，于是往CQ1中写入两个命令完成信息，同时更新CQ1对应的Tail DB寄存器，值为2。同时，SSD发消息给Host告知有命令完成。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/UmkUyR6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;（5）Host收到SSD的通知后，从CQ1中取出那两条完成信息处理。待处理完毕，Host往CQ1 Head DB寄存器中写入CQ1的head，值为2。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/vd75vaC.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;这样，就完成了一次完整的命令处理。    &lt;/p&gt;
&lt;h3 id=&quot;NVMe总结&quot;&gt;&lt;a href=&quot;#NVMe总结&quot; class=&quot;headerlink&quot; title=&quot;NVMe总结&quot;&gt;&lt;/a&gt;NVMe总结&lt;/h3&gt;&lt;p&gt;NVMe所带来的重大改进主要包含以下几方面：一是低延迟，低延时和良好的并行性可以让SSD的随机性能大幅提升；其次是支持多队列和更高的队列深度，多队列让CPU的性能得到更好的释放，而队列深度从32提升到最大64K，则大幅提升了SSD的IOPS能力；然后是NVMe的低功耗，其加入了自动功耗状态切换和动态能耗管理功能；最后是其驱动的适用性广，解决了不同PCIe SSD之间的驱动适用性问题。支持NVMe标准的PCIe SSD可适用于多个不同平台，也不需要厂商独立提供驱动支持。目前Windows、Linux、Solaris、Unix、VMware、UEFI等都加入了对NVMe SSD的支持。&lt;/p&gt;
&lt;h3 id=&quot;附&quot;&gt;&lt;a href=&quot;#附&quot; class=&quot;headerlink&quot; title=&quot;附&quot;&gt;&lt;/a&gt;附&lt;/h3&gt;&lt;p&gt;特别鸣谢：&lt;br&gt;&lt;a href=&quot;http://www.ssdfans.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ssdfans&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://blog.dujiong.net/2017/02/13/storage/&quot;&gt;存储的那些词儿&lt;/a&gt;一文中已经提到了NVMe是使用PCIe通道的一种逻辑设备接口标准（是接口标准，不是接口！），本文将进一步地分析NVMe的设计及其带来的性能提升。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.dujiong.net/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>存储的那些词儿</title>
    <link href="http://blog.dujiong.net/2017/02/13/storage/"/>
    <id>http://blog.dujiong.net/2017/02/13/storage/</id>
    <published>2017-02-13T07:06:23.000Z</published>
    <updated>2017-05-05T12:08:17.680Z</updated>
    
    <content type="html">&lt;p&gt;海量数据的迸发和传统存储技术所面临的性能瓶颈，促进了新型存储技术和系统架构的高速发展。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;存储的那些词儿&quot;&gt;&lt;a href=&quot;#存储的那些词儿&quot; class=&quot;headerlink&quot; title=&quot;存储的那些词儿&quot;&gt;&lt;/a&gt;存储的那些词儿&lt;/h3&gt;&lt;p&gt;本文简单介绍下在存储领域出现频率最高的几个词语:SATA、PCIe、AHCI和NVMe（Non-Volatile Memory Express，非易失性存储器标准）。&lt;/p&gt;
&lt;h4 id=&quot;SATA和PCIe&quot;&gt;&lt;a href=&quot;#SATA和PCIe&quot; class=&quot;headerlink&quot; title=&quot;SATA和PCIe&quot;&gt;&lt;/a&gt;SATA和PCIe&lt;/h4&gt;&lt;p&gt;SATA和PCIe大家应该比较熟悉，这两个都是总线标准。&lt;br&gt;SATA由IDE/ATA标准发展而来，主要用途是把存储设备连接到主板。SATA的发展主要经历了以下版本：&lt;br&gt;SATA revision 1.0 (1.5 Gbit/s, 150 MB/s)&lt;br&gt;SATA revision 2.0 (3 Gbit/s, 300 MB/s)&lt;br&gt;SATA revision 3.0 (6 Gbit/s, 600 MB/s)&lt;br&gt;SATA revision 3.1&lt;br&gt;SATA revision 3.2 (16 Gbit/s, 1969 MB/s)&lt;br&gt;SATA在发展过程中，考虑了向下兼容的问题，比如主板上SATA-3的接口，可以连接SATA-2的硬盘。但同时，向下兼容也造成了其发展缓慢。&lt;br&gt;出于向下兼容的考虑，SATA可以工作在两种模式：传统模式和AHCI模式。传统模式是为了兼容以前的 IDE/ATA。AHCI模式则比较新，支持SATA独有的功能，如热插拔、原生命令队列（NCQ）等。      &lt;/p&gt;
&lt;p&gt;PCIe是另一种总线标准，由AGP、PCI、PCI-x发展而来，而这些总线的发展，主要的动力是显卡的发展。AGP就是Accelerated Graphics Port（加速图像端口）的缩写。由于显卡需要很大的带宽和速度，PCI总线标准就不断升级来满足要求。当然，除了显卡外，PCI总线还用于其他的扩展卡，如网卡（包括有线网卡、无线网卡、3G/4G卡等）。  &lt;/p&gt;
&lt;h4 id=&quot;AHCI和NVMe&quot;&gt;&lt;a href=&quot;#AHCI和NVMe&quot; class=&quot;headerlink&quot; title=&quot;AHCI和NVMe&quot;&gt;&lt;/a&gt;AHCI和NVMe&lt;/h4&gt;&lt;p&gt;AHCI和NVMe是逻辑（或者说软件、驱动程序）上的标准。&lt;br&gt;从上面 SATA的不同版本可以看到，提速是一个主要任务（当然也有其他的改进）。但进入SSD时代后，SATA的改版速度（由于要考虑向下兼容），已经跟不上传输速度的要求了。这时候，业界就考虑采用PCIe来连接存贮设备。但在驱动程序层面，仍然采用AHCI。这是因为AHCI已经非常成熟，广泛被各种操作系统（如Windows、Linux）所采用。&lt;br&gt;AHCI是为了发挥SATA的潜能而设计的，当时算是“高大上”了。但当时仍然是机械硬盘统治市场，因此AHCI的设计是基于机械硬盘的特性（旋转式磁性盘片）。虽然AHCI也可以用于SSD，但却不能发挥极致。因为SSD更像内存，而不像“盘片”。譬如说，机械硬盘，如果磁头错过了一个扇区，那就得等盘片转一圈回来才能访问。SSD就不存在这个问题。因此，业界重新设计一个新的NVMe协议，希望发挥 SSD的潜能。下面是AHCI和NVMe的对比：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/4uOqsG8.png&quot; alt=&quot;&quot;&gt;   &lt;/p&gt;
&lt;h4 id=&quot;物理接口&quot;&gt;&lt;a href=&quot;#物理接口&quot; class=&quot;headerlink&quot; title=&quot;物理接口&quot;&gt;&lt;/a&gt;物理接口&lt;/h4&gt;&lt;p&gt;说完了总线和协议，下面说说物理接口。无论采用什么总线和协议，主板总得连接到存储设备上。这里所说的物理接口，指是是物理尺寸和形状，电气特征不作讨论。接口分为主机端和设备端，种类繁多，这里挑几个常见的。&lt;br&gt;1.SATA接口。采用这种接口的，只能使用SATA总线，不能使用PCIe总线。大部分2.5”SSD就是这种接口。&lt;br&gt;2.M.2接口。采用这种接口的SSD，可以使用SATA或者PCIe总线（取决于主板和SSD）。如果采用PCIe总线，又分为AHCI和NVMe两种协议。&lt;br&gt;3.SATA Express接口。SATA Express使用的是PCIe总线，向下兼容SATA总线。&lt;/p&gt;
&lt;h4 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h4&gt;&lt;p&gt;不同存储总线标准/协议之间的组合如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/J22ZZLp.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;可以看出，AHCI和NVMe是驱动程序层面的。NVMe只适用于SSD（SSD和主板也要支持NVMe才行）。AHCI则适用于机械硬盘和SSD。&lt;br&gt;在主板芯片层面，有AHCI控制器和PCIe控制器。有趣的是AHCI驱动程序“居然”可以使用PCIe控制器（中间那条橙色的线）。这个其实是个过渡方案，目的是在利用PCIe高带宽的同时，保持对上层软件的兼容性。&lt;br&gt;绿色那个框是主板上的物理接口。注意即使是同样的物理接口，也可以选择不同的总线和协议（如果主机和设备支持的话）。&lt;br&gt;右下角的PCIe SSD设备则可以有两种不同的控制器（最下面的两个框）：AHCI和NVMe。因此，同样是PCIe的SSD，也可以有不同的传输效能。&lt;br&gt;说了这么多，相信大家已经晕了。下面按传输效率做个排序。&lt;br&gt;1.PCIe NVMe&lt;br&gt;这个是最高大上的。在笔记本市场，根据效能，可以再细分为两个等级：&lt;br&gt;（1）M.2尺寸的NVMe（如三星 950 PRO）。可以有四条PCIe通道，速度最快。但由于电路板面积限制，容量和发热都是个问题。&lt;br&gt;（2）2.5″尺寸的NVMe（如东芝XG3），采用SATA Express接口，可以有两条PCIe通道，传输速率较低。此外，由于2.5″体积较大，容量和发热比 M.2 要好。&lt;br&gt;2.PCIe AHCI&lt;br&gt;效能比1稍低，是由于AHCI协议的滞后性决定的。笔记本上只有M.2外形，没有2.5″外形。&lt;br&gt;3.SATA AHCI&lt;br&gt;效能最低，但兼容性最好，根据外形可分为两类。这两类的传输效能是一样的，无分高低。&lt;br&gt;（1）M.2外形的设备，如三星850EVO的M.2盘。&lt;br&gt;（2）2.5″外形的设备，如目前广泛使用的机械硬盘，固态硬盘等。&lt;/p&gt;
&lt;h3 id=&quot;附&quot;&gt;&lt;a href=&quot;#附&quot; class=&quot;headerlink&quot; title=&quot;附&quot;&gt;&lt;/a&gt;附&lt;/h3&gt;&lt;p&gt;接下来将详细分析NVMe的设计及其带来的性能提升。   &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;海量数据的迸发和传统存储技术所面临的性能瓶颈，促进了新型存储技术和系统架构的高速发展。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.dujiong.net/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>分配排序（桶排序和基数排序）总结</title>
    <link href="http://blog.dujiong.net/2017/01/20/Distribute-Sort/"/>
    <id>http://blog.dujiong.net/2017/01/20/Distribute-Sort/</id>
    <published>2017-01-20T11:51:47.000Z</published>
    <updated>2017-04-24T13:52:37.772Z</updated>
    
    <content type="html">&lt;p&gt;分配排序，是指不需要进行两两之间的比较，而根据记录自己的关键码的分配来进行排序的一类方法，因此，在进行分配排序时，我们通常需要知道记录序列的一些具体情况，比如关键码的分布。分配排序主要包括桶排序和基数排序。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;桶排序&quot;&gt;&lt;a href=&quot;#桶排序&quot; class=&quot;headerlink&quot; title=&quot;桶排序&quot;&gt;&lt;/a&gt;桶排序&lt;/h3&gt;&lt;p&gt;首先来介绍桶排序。如果我们知道序列中的记录都位于一个比较小的区间范围之内，那我们可以把相同值的记录分配到同一个桶里面，然后依次收集这些桶就能得到一个有序的序列。&lt;br&gt;例如，下图中，我们知道待排序数组的记录值在0~9的范围之内，因此，我们可以设定10个桶，然后就把这些记录值分配到各个桶里面。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/SVkuDWp.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;此外，我们还维护了前若干桶的一个累加值，并且，在进行排序的时候，我们从数组记录值的最右边开始遍历，这样来保持排序的稳定性。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/yGcdDMb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;桶排序的实现&quot;&gt;&lt;a href=&quot;#桶排序的实现&quot; class=&quot;headerlink&quot; title=&quot;桶排序的实现&quot;&gt;&lt;/a&gt;桶排序的实现&lt;/h4&gt;&lt;p&gt;在弄清楚了原理之后，桶排序的实现就很简单了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; void BucketSort(vector&amp;lt;int&amp;gt;&amp;amp; nums, int max){
     int n=nums.size();
     vector&amp;lt;int&amp;gt; temp(begin(nums), end(nums));
     vector&amp;lt;int&amp;gt; count(max);
     for(int i=0;i&amp;lt;max;i++){
         count[i]=0;
     }
     for(int i=0;i&amp;lt;n;i++){
         count[nums[i]]++;
     }
     for(int i=0;i&amp;lt;max;i++){
         count[i]=count[i-1]+count[i];
     }
     for(int i=n-1;i&amp;gt;=0;i--){    
         nums[--count[temp[i]]]=temp[i];
     }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;桶排序性能分析&quot;&gt;&lt;a href=&quot;#桶排序性能分析&quot; class=&quot;headerlink&quot; title=&quot;桶排序性能分析&quot;&gt;&lt;/a&gt;桶排序性能分析&lt;/h4&gt;&lt;p&gt;由前面的分析可知，桶排序使用的场景是：待排序的数组长度为n，所有数组记录值位于[0,m)上，且m相对于n很小。&lt;br&gt;因为排序过程需要遍历原数组和count数组，所以总的时间复杂度为O（n+m）。空间代价为O（n+m），包括临时数组和计数器。桶排序是稳定的。   &lt;/p&gt;
&lt;h3 id=&quot;静态基数排序&quot;&gt;&lt;a href=&quot;#静态基数排序&quot; class=&quot;headerlink&quot; title=&quot;静态基数排序&quot;&gt;&lt;/a&gt;静态基数排序&lt;/h3&gt;&lt;p&gt;我们已经知道，桶排序只适合m很小的情况，那如果出现m很大，甚至大于n的情况，应该怎么办呢？不难想象，我们可以把这些记录的关键码认为地拆成几个部分，然后再运用几次桶排序，从而完成整个排序过程，这就是基数排序的思想，基数排序又可以分为静态基数排序和链式基数排序。&lt;br&gt;比如，我们需要对0到9999之间的整数进行排序，这时直接使用桶排序显然是代价较高的。我们可以将四位数看做是由四个排序码决定，个位为最低排序码。基数等于10。然后，按照千，百，十，个位数字依次进行4次桶排序。&lt;br&gt;下面以一个记录值都是两位数的数组的排序过程来说明静态基数排序两趟桶排的具体过程，这里和后面的代码都采用的是低位优先法。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/gPF11Hv.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/hr8v9TV.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;静态基数排序的实现&quot;&gt;&lt;a href=&quot;#静态基数排序的实现&quot; class=&quot;headerlink&quot; title=&quot;静态基数排序的实现&quot;&gt;&lt;/a&gt;静态基数排序的实现&lt;/h4&gt;&lt;p&gt;同样，给出静态基数排序的实现。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void RadixSort(vector&amp;lt;int&amp;gt;&amp;amp; nums, int d, int r){
    int n=nums.size();
    vector&amp;lt;int&amp;gt; temp(n,0);
    vector&amp;lt;int&amp;gt; count(r,0);        //r:radix,10
    int radix=1;
    int i,j,k;
    for(i=1;i&amp;lt;=d;i++){        //d:排序码的个数
        for(j=0;j&amp;lt;r;j++){
            count[j]=0;
        }
        for(j=0;j&amp;lt;n;j++){
            k=(nums[j]/radix)%r;
            count[k]++;
        }
        for(j=1;j&amp;lt;r;j++){
            count[j]=count[j]+count[j-1];
        }
        for(j=n-1;j&amp;gt;=0;j--){
            k=(nums[j]/radix)%r;
            count[k]--;
            temp[count[k]]=nums[j];
        }
        for(j=0;j&amp;lt;n;j++){
            nums[j]=temp[j];
        }
        radix*=r;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;静态基数排序性能分析&quot;&gt;&lt;a href=&quot;#静态基数排序性能分析&quot; class=&quot;headerlink&quot; title=&quot;静态基数排序性能分析&quot;&gt;&lt;/a&gt;静态基数排序性能分析&lt;/h4&gt;&lt;p&gt;同样的，静态基数排序也需要一个临时数组和r个计数器，所以总的空间代价为O（n+r）。时间代价则为O（d*(n+r)），因为它相当于进行了d次桶式排序。此外，静态基数排序也是稳定的。&lt;/p&gt;
&lt;h3 id=&quot;链式基数排序&quot;&gt;&lt;a href=&quot;#链式基数排序&quot; class=&quot;headerlink&quot; title=&quot;链式基数排序&quot;&gt;&lt;/a&gt;链式基数排序&lt;/h3&gt;&lt;p&gt;基于静态链的基数排序相对于静态基数排序的差别在于将分配出来的子序列存放在r个静态链组织的队列中。&lt;br&gt;下面，同样以一个例子来说明链式基数排序的过程。&lt;br&gt;假设有待排序数组[97, 53, 88, 59, 26, 41, 88, 31, 22]，首先按照个位进行第一趟分配，分配完元素后的队列如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/D5ZCnQk.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;然后进行第一趟收集，即个位有序：[41, 31, 22, 53, 26, 97, 88, 88]。&lt;br&gt;接下来，再按照十位进行第二趟分配。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/BUeHB5r.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;最后进行第二趟收集，即完成最终的排序。&lt;/p&gt;
&lt;h4 id=&quot;链式基数排序的实现&quot;&gt;&lt;a href=&quot;#链式基数排序的实现&quot; class=&quot;headerlink&quot; title=&quot;链式基数排序的实现&quot;&gt;&lt;/a&gt;链式基数排序的实现&lt;/h4&gt;&lt;p&gt;下面给出链式基数排序的实现。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;typedef struct Node{
    int key;
    int next;    //下一个节点在数组中的下标
}Node;

typedef struct StaticQueue{
    int head;
    int tail;
}StaticQueue;

void Distribute(vector&amp;lt;Node&amp;gt;&amp;amp; node, int first, int i, int r, StaticQueue* queue);
void Collect(vector&amp;lt;Node&amp;gt;&amp;amp; node, int&amp;amp; first, int r, StaticQueue* queue);

void RadixSort(vector&amp;lt;int&amp;gt;&amp;amp; nums, int d, int r){
    int i, first=0;
    int n=nums.size();
    vector&amp;lt;Node&amp;gt; node(n);
    StaticQueue* queue = new StaticQueue[r];
    for(i=0;i&amp;lt;n-1;i++){
        node[i].key=nums[i];
        node[i].next=i+1;
    }
    node[n-1].key=nums[n-1];
    node[n-1].next=-1;

    for(i=0;i&amp;lt;d;i++){            //d趟的分配和收集
        Distribute(node, first, i, r, queue);            //分配到不同队列
        Collect(node, first, r, queue);                    //聚合
    }
    for(i=0;i&amp;lt;r;i++){
        if(queue[i].head==-1) continue;
        else break;
    }

    int j=queue[i].head;
    while(node[j].next!=-1){        //排序结果
        cout &amp;lt;&amp;lt; node[j].key &amp;lt;&amp;lt; endl;
        j = node[j].next;
    }
    cout &amp;lt;&amp;lt; node[j].key &amp;lt;&amp;lt; endl;

    delete[] queue;
}

void Distribute(vector&amp;lt;Node&amp;gt;&amp;amp; node, int first, int i, int r, StaticQueue* queue){
    int current=first;
    for(int j=0;j&amp;lt;r;j++){
        queue[j].head=-1;
    }
    while(current!=-1){
        int k = node[current].key;
        for(int a=0;a&amp;lt;i;a++){
            k=k/r;
        }
        k=k%r;
        if(queue[k].head==-1){
            queue[k].head=current;
        }else{
            node[queue[k].tail].next=current;
        }
        queue[k].tail=current;
        current=node[current].next;
    }
}

void Collect(vector&amp;lt;Node&amp;gt;&amp;amp; node, int&amp;amp; first, int r, StaticQueue* queue){
    int last,k=0;
    while(queue[k].head==-1) k++;
    first=queue[k].head;
    last=queue[k].tail;
    while(k&amp;lt;r-1){
        k++;
        while(k&amp;lt;r-1 &amp;amp;&amp;amp; queue[k].head==-1){
            k++;
        }
        if(queue[k].head!=-1){
            node[last].next=queue[k].head;
            last=queue[k].tail;
        }
    }
    node[last].next=-1;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;链式基数排序性能分析&quot;&gt;&lt;a href=&quot;#链式基数排序性能分析&quot; class=&quot;headerlink&quot; title=&quot;链式基数排序性能分析&quot;&gt;&lt;/a&gt;链式基数排序性能分析&lt;/h4&gt;&lt;p&gt;由前面的分析和代码可知，链式基数排序在分配和收集的过程中，不需要移动记录本身，只是在做记录next指针的修改。因此，它的时间代价和空间代价与静态基数排序一致，分别为O（d*(n+r)）和O（n+r）。并且，根据队列的先入先出的特点，链式基数排序也是稳定的。&lt;/p&gt;
&lt;h3 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h3&gt;&lt;p&gt;本文总结了分配排序中最常见的桶排序和基数排序，并给出了具体的实现和相关分析。可以看出，分配排序不会对记录值进行两两比较，因此不受O（nlgn）时间复杂度的限制。从另外一个角度看，分配排序也可以看做是以空间换时间的典型方法。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;分配排序，是指不需要进行两两之间的比较，而根据记录自己的关键码的分配来进行排序的一类方法，因此，在进行分配排序时，我们通常需要知道记录序列的一些具体情况，比如关键码的分布。分配排序主要包括桶排序和基数排序。&lt;br&gt;
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="http://blog.dujiong.net/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>搭建Redis集群</title>
    <link href="http://blog.dujiong.net/2017/01/15/Redis-Cluster/"/>
    <id>http://blog.dujiong.net/2017/01/15/Redis-Cluster/</id>
    <published>2017-01-15T07:08:05.000Z</published>
    <updated>2017-03-29T13:53:02.476Z</updated>
    
    <content type="html">&lt;p&gt;Redis集群是Redis提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Redis集群概述&quot;&gt;&lt;a href=&quot;#Redis集群概述&quot; class=&quot;headerlink&quot; title=&quot;Redis集群概述&quot;&gt;&lt;/a&gt;Redis集群概述&lt;/h3&gt;&lt;p&gt;Redis集群使用数据分片而非一致性哈希来实现，一个Redis集群包含16384个哈希槽（slot），数据库中的每个键都属于这16384个哈希槽中的其中一个，集群中的每个节点可以处理0个或最多16384个槽，当数据库中的16384个槽都有节点在处理时，集群处于上线状态；而如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态。&lt;br&gt;集群中的每个节点负责处理一部分哈希槽，比如，现在有三个独立的节点127.0.0.1:7000,127.0.0.1:7001,127.0.0.1:7002，其中各节点处理的哈希槽关系：节点7000负责处理0到5000号哈希槽，节点7001负责处理5001到10000号哈希槽，节点7002负责处理10001到16384号哈希槽。&lt;br&gt;从而，当向集群中添加或删除节点时，集群只需在对应节点的哈希槽做移动即可。不会造成节点阻塞、集群下线。&lt;br&gt;当然，为了使得集群在一部分节点下线的情况下仍然可以正常运作，Redis集群对节点提供了主从复制功能，集群中的每个节点都有1到N个复制节点，形成主-从模型。&lt;/p&gt;
&lt;h3 id=&quot;Redis集群架构图&quot;&gt;&lt;a href=&quot;#Redis集群架构图&quot; class=&quot;headerlink&quot; title=&quot;Redis集群架构图&quot;&gt;&lt;/a&gt;Redis集群架构图&lt;/h3&gt;&lt;p&gt;Redis集群架构如下图所示：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/O4QfdDF.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;Redis集群架构的主要特点有:&lt;br&gt;（1）所有节点批次互联（PING-PONG机制），没有中心控制协调节点，内部使用二进制协议优化传输速度和带宽；&lt;br&gt;（2）节点的失效通过集群中超过半数的节点“投票”监测；&lt;br&gt;（3）客户端与Redis节点直连，不需要中间proxy层，客户端连接集群中任何一个可用节点即可；    &lt;/p&gt;
&lt;h3 id=&quot;Redis集群实践&quot;&gt;&lt;a href=&quot;#Redis集群实践&quot; class=&quot;headerlink&quot; title=&quot;Redis集群实践&quot;&gt;&lt;/a&gt;Redis集群实践&lt;/h3&gt;&lt;p&gt;下面搭建一个简单的Redis集群环境，集群共包含6个节点，其中3个主节点，3个从节点。节点都部署在本机（ubuntu 14.04 x86-64），以端口号区分，分别为127.0.0.1:7000~127.0.0.1:7005。&lt;br&gt;1.首先安装3.0版本之后的Redis（因为Redis集群是在3.0版本提出的功能），这里采用源码(3.2.8版本)安装。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget http://download.redis.io/releases/redis-3.2.8.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;2.然后解压，安装&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tar xf redis-3.2.8.tar.gz    
cd redis-3.2.8       
make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;3.安装完成后查看Redis版本信息，验证安装是否成功。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/6QuQXPQ.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;4.创建存放多个节点实例的目录&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir redis-data/cluster -p
cd redis-data/cluster
mkdir 7000 7001 7002 7003 7004 7005
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;5.复制并修改配置文件redis.conf&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cp /etc/redis/redis.conf redis-data/cluster/7000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;修改配置文件中下面选项&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;port 7000
daemonize yes
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后把修改完成的redis.conf复制到7001-7005目录下，端口修改成和文件夹对应。 &lt;/p&gt;
&lt;p&gt;6.分别启动6个Redis实例&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd redis-data/cluster/7000
redis-server redis.conf
...
cd redis-data/cluster/7001
redis-server redis.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;7.查看进程是否运行&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/nWUeGum.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;8.接下来创建集群，首先安装依赖包&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apt-get install ruby
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后安装gem-redis，下载地址&lt;a href=&quot;https://rubygems.org/gems/redis/versions/3.3.0&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://rubygems.org/gems/redis/versions/3.3.0&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gem install redis-3.3.0.gem
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来复制集群管理程序到/usr/local/bin&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cp redis-3.2.8/src/redis-trib.rb /usr/local/bin/redis-trib
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;就可以使用redis-trib创建集群了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，命令create表示创建一个新的集群，选项–replicas 1表示未集群中的每个主节点创建一个从节点。即，上述命令运行后，redis-trib将创建一个包含三个主节点和三个从节点的集群。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/6jhul4S.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;键入“yes”后一个三主三从的集群架构就创建完毕。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/WTTZPh8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;一些简单的测试&quot;&gt;&lt;a href=&quot;#一些简单的测试&quot; class=&quot;headerlink&quot; title=&quot;一些简单的测试&quot;&gt;&lt;/a&gt;一些简单的测试&lt;/h3&gt;&lt;p&gt;创建完毕后，可以随时通过redis-cli -p 7000(node number) cluster nodes来查看集群中各节点的信息。包括唯一的节点ID，主从关系，每个主节点分配的slots范围。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/jJcOqwB.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;集群查询&quot;&gt;&lt;a href=&quot;#集群查询&quot; class=&quot;headerlink&quot; title=&quot;集群查询&quot;&gt;&lt;/a&gt;集群查询&lt;/h4&gt;&lt;p&gt;做一个简单的测试，    在7000节点上存储K-V数据，然后分别在其从节点和其他主节点上获取该数据。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/X3GWKHJ.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/a0P55nA.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;要找的数据没有在当前节点上时，cluster发送MOVED指令指示到对应的节点上穿，可以通过-c参数指定查询时接收到MOVED指令自动跳转。&lt;/p&gt;
&lt;h4 id=&quot;删除节点&quot;&gt;&lt;a href=&quot;#删除节点&quot; class=&quot;headerlink&quot; title=&quot;删除节点&quot;&gt;&lt;/a&gt;删除节点&lt;/h4&gt;&lt;p&gt;Slave节点的作用是提供冗余和高可用，大部分情况下用于分担读的压力。移除Slave节点无须多余的动作，直接删除即可。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/Hr2xMz6.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;而如果是Master节点，则需要将其负责的slots范围迁移到其他节点上。保证数据不丢失。&lt;br&gt;所以，先使用redis-trib reshard …来将待删除的节点上的数据迁移到其他节点，然后删除该节点。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/xLG6ctH.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h3&gt;&lt;p&gt;Redis Cluster是3.0版本之后提供的新功能，采用了P2P的去中心化架构，而没有采用像Codis之类的Proxy解决方案中的中心协调节点设计。本文只是简单搭建了一个Redis集群环境，后续还将在此基础上，进一步深入研究高可用、可扩展的Redis集群方案。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Redis集群是Redis提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis Sentinel原理及实践</title>
    <link href="http://blog.dujiong.net/2017/01/11/Redis-sentinel/"/>
    <id>http://blog.dujiong.net/2017/01/11/Redis-sentinel/</id>
    <published>2017-01-11T07:21:58.000Z</published>
    <updated>2017-03-29T13:53:03.368Z</updated>
    
    <content type="html">&lt;p&gt;Sentinel（哨兵）是Redis的高可用性解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的多个从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Redis-Sentinel系统结构&quot;&gt;&lt;a href=&quot;#Redis-Sentinel系统结构&quot; class=&quot;headerlink&quot; title=&quot;Redis Sentinel系统结构&quot;&gt;&lt;/a&gt;Redis Sentinel系统结构&lt;/h3&gt;&lt;p&gt;下图展示了一个Sentinel系统监视服务器的例子。当前系统包括主服务器server1，从服务器server2、server3、server4和Sentinel监听系统。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/Q5Keiy2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;假设某时刻server1进入下线状态，那么从服务器对主服务器的复制操作将中止，并且Sentinel系统会察觉到server1已下线，并在server1的下线时长超过用户设定的上限时，Sentinel系统会执行故障转移操作：&lt;br&gt;（1）首先，Sentinel会挑选一个从服务器，将其升级为新的主服务器，这里涉及到选举算法；&lt;br&gt;（2）然后Sentienl向server1所有的从服务器发送新的复制指令，让它们成为新的主服务器的从服务器；&lt;br&gt;（3）Sentinel还会继续监视已下线的server1，并在它重新上线时，将它设置为新的服务器的从服务器。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/l2q4BV4.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Sentinel与主从Redis通信细节&quot;&gt;&lt;a href=&quot;#Sentinel与主从Redis通信细节&quot; class=&quot;headerlink&quot; title=&quot;Sentinel与主从Redis通信细节&quot;&gt;&lt;/a&gt;Sentinel与主从Redis通信细节&lt;/h3&gt;&lt;h4 id=&quot;获取主服务器信息&quot;&gt;&lt;a href=&quot;#获取主服务器信息&quot; class=&quot;headerlink&quot; title=&quot;获取主服务器信息&quot;&gt;&lt;/a&gt;获取主服务器信息&lt;/h4&gt;&lt;p&gt;Sentinel默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送INFO命令，并通过分析INFO命令的回复来获取主服务器的当前信息。&lt;br&gt;通过分析主服务器返回的INFO命令回复，Sentinel可以获取以下两方面的信息：&lt;br&gt;（1）主服务器本身的信息，包括服务器运行ID，服务器角色等；&lt;br&gt;（2）主服务器属下的所有从服务器信息。包括从服务器的IP地址、端口号等，根据这些信息，Sentinel无须用户提供从服务器的地址信息，就可以自动发现从服务器。   &lt;/p&gt;
&lt;h4 id=&quot;获取从服务器信息&quot;&gt;&lt;a href=&quot;#获取从服务器信息&quot; class=&quot;headerlink&quot; title=&quot;获取从服务器信息&quot;&gt;&lt;/a&gt;获取从服务器信息&lt;/h4&gt;&lt;p&gt;当Sentinel发现主服务器有新的从服务器出现时，Sentinel除了会为这个新的从服务器创建相应的实例结构之外，Sentinel还会创建连接到从服务器的命令连接和订阅连接。&lt;br&gt;在创建命令连接之后，Sentinel在默认情况下，会以每十秒一次的频率通过命令向从服务器发送INFO命令。并收到服务器信息的回复，根据这些信息，对从服务器的实例结构进行更新。   &lt;/p&gt;
&lt;h4 id=&quot;与所有主从服务器通信&quot;&gt;&lt;a href=&quot;#与所有主从服务器通信&quot; class=&quot;headerlink&quot; title=&quot;与所有主从服务器通信&quot;&gt;&lt;/a&gt;与所有主从服务器通信&lt;/h4&gt;&lt;p&gt;默认情况下，Sentinel会以每两秒一次的频率，通过命令连接向所有被监视的主服务器和从服务器发送命令。信息格式为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PUBLISH __sentinel__:hello &amp;lt;s_ip&amp;gt;,&amp;lt;s_port&amp;gt;,&amp;lt;s_runid&amp;gt;,&amp;lt;s_epoch&amp;gt;,&amp;lt;m_name&amp;gt;,&amp;lt;m_ip&amp;gt;,&amp;lt;m_port&amp;gt;,&amp;lt;m_epoch&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，以&lt;code&gt;s_&lt;/code&gt;开头的参数是Sentinel本身的信息。而m_开头的参数则是主服务器的信息。&lt;br&gt;另外，当Sentinel与一个主服务器或者从服务器建立起订阅连接之后，Sentinel就会通过订阅连接，向服务器发送命令： &lt;code&gt;SUBSCRIBE __sentinel__:hello&lt;/code&gt;，Sentinel会在其与服务器之间的连接断开前一直订阅该频道。&lt;br&gt;因此，每个与Sentinel连接的服务器，Sentinel既通过命令连接向服务器的 &lt;code&gt;__sentinel__:hello&lt;/code&gt;频道发送信息，又通过订阅连接从服务器的&lt;code&gt;__sentinel__:hello&lt;/code&gt;频道接收信息。&lt;br&gt;比如，有sentinel1、sentinel2、sentinel3三个sentinel在监视同一个服务器，当sentinel&lt;br&gt;1向服务器的&lt;code&gt;__sentinel__:hello&lt;/code&gt;频道发送一条信息时，所有订阅了该频道的sentine（包括自己）都会收到这条信息。这些信息用于更新其他Sentinel对发送信息Sentinel的认知，也会被用于更新其他Sentinel对监视服务器的认知。&lt;br&gt;此&lt;/p&gt;
&lt;h3 id=&quot;选举领头Sentinel&quot;&gt;&lt;a href=&quot;#选举领头Sentinel&quot; class=&quot;headerlink&quot; title=&quot;选举领头Sentinel&quot;&gt;&lt;/a&gt;选举领头Sentinel&lt;/h3&gt;&lt;p&gt;默认情况下，Sentinel会以每秒一次的频率向所有与它创建了命令连接的实例发送PING命令，并通过实例返回的PING命令回复来判断实例是否在线。&lt;br&gt;当Sentinel将一个主服务器判断为主观下线之后，为了确认这个主服务器是否真的下线，它会向同样监视这一主服务器的其他Sentinel进行询问。当从其他Sentinel那里接收到足够数量的已下线判断之后，Sentinel就会将从服务器判定为客观下线，并对主服务器进行故障转移操作。&lt;br&gt;当一个主服务器被判断为客观下线时，监视这个下线服务器的各个Sentinel会进行协商，选举出一个领头Sentinel，并由领头Sentinel对下线主服务器执行故障转移操作（详细的选举算法这里不详述）。&lt;br&gt;在选举出领头Sentinel之后，领头Sentinel将对已下线的主服务器执行文章开头所述的故障转移操作。&lt;/p&gt;
&lt;h3 id=&quot;Sentinel实践&quot;&gt;&lt;a href=&quot;#Sentinel实践&quot; class=&quot;headerlink&quot; title=&quot;Sentinel实践&quot;&gt;&lt;/a&gt;Sentinel实践&lt;/h3&gt;&lt;p&gt;以下实验均是基于ubuntu 14.04 x86-64bits平台，Redis通过apt-get install redis-server简易安装，版本2.8.x，主从服务器都位于本机。&lt;/p&gt;
&lt;h4 id=&quot;实验架构和目录结构&quot;&gt;&lt;a href=&quot;#实验架构和目录结构&quot; class=&quot;headerlink&quot; title=&quot;实验架构和目录结构&quot;&gt;&lt;/a&gt;实验架构和目录结构&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/lXobqNP.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/6dgbBnr.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;搭建Reis主从结构&quot;&gt;&lt;a href=&quot;#搭建Reis主从结构&quot; class=&quot;headerlink&quot; title=&quot;搭建Reis主从结构&quot;&gt;&lt;/a&gt;搭建Reis主从结构&lt;/h4&gt;&lt;p&gt;首先配置master对应的配置redis.conf，重点需要关注的地方有：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pidfile /var/run/redis.pid
port 7003
tcp-keepalive 60
logfile /var/log/redis/redis-server.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其余很多选项保持默认即可。然后&lt;code&gt;redis-server master/redis.conf&lt;/code&gt;启动master。&lt;br&gt;然后搭建slave，slave的配置文件和master基本一致，只需要修改相应的pidfile、端口(8003)、日志文件名，并配上master的地址&lt;code&gt;slaveof 127.0.0.1:7003&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&quot;Sentinel配置&quot;&gt;&lt;a href=&quot;#Sentinel配置&quot; class=&quot;headerlink&quot; title=&quot;Sentinel配置&quot;&gt;&lt;/a&gt;Sentinel配置&lt;/h4&gt;&lt;p&gt;接下来配置三个哨兵。以sentinel1.conf为例&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;port 26371

daemonize yes
logfile &amp;quot;./sentinel1.log&amp;quot;
dir &amp;quot;/home/dujiong/redis-data/sentinel&amp;quot;

sentinel monitor Master 127.0.0.1 7003 1
sentinel down-after-milliseconds Master 1500
sentinel failover-timeout Master 10000

sentinel config-epoch Master 15

sentinel known-slave Master 127.0.0.1 8003
sentinel known-sentinel Master 127.0.0.1 26372 0aca3a57038e2907c8a07be2b3c0d15171e44da5
sentinel known-sentinel Master 127.0.0.1 26373 e7625d74a5a4b142c495baa8ca522517bd08c65b
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其余两个哨兵在此基础上修改相应的参数即可。&lt;br&gt;配置好环境后，使用ps命令查看进程运行情况。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/QbnRZN8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;查看Sentinel监控的主从服务器&quot;&gt;&lt;a href=&quot;#查看Sentinel监控的主从服务器&quot; class=&quot;headerlink&quot; title=&quot;查看Sentinel监控的主从服务器&quot;&gt;&lt;/a&gt;查看Sentinel监控的主从服务器&lt;/h4&gt;&lt;p&gt;在Sentinel中查看当前的主从服务器状态如下：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/EqCwUhI.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/UtcunJV.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/O2zmFTF.png&quot; alt=&quot;&quot;&gt;  、&lt;/p&gt;
&lt;h4 id=&quot;故障转移&quot;&gt;&lt;a href=&quot;#故障转移&quot; class=&quot;headerlink&quot; title=&quot;故障转移&quot;&gt;&lt;/a&gt;故障转移&lt;/h4&gt;&lt;p&gt;接下来，使用&lt;code&gt;redis-cli -p 7003 shutdown&lt;/code&gt;关闭master，按照前面的理论，Sentinel将会进行故障转移操作，选择slave作为主服务器。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/tAqniY8.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;最后，再次启动master服务器（127.0.0.1:7003），Sentinel将它设置为新的服务器的从服务器。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/rNhhyy6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h3&gt;&lt;p&gt;Sentinel是一个运行在特殊模式下的Redis服务器，用以提供Redis的高可用解决方案。在3.0之后的Redis版本中，Redis Cluster重用了Sentinel的代码逻辑，不需要单独启动一个Sentinel集群，Cluster本身就能自动进行Master选举和Failover切换。   &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Sentinel（哨兵）是Redis的高可用性解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的多个从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis事务</title>
    <link href="http://blog.dujiong.net/2017/01/05/Redis-transaction/"/>
    <id>http://blog.dujiong.net/2017/01/05/Redis-transaction/</id>
    <published>2017-01-05T12:25:52.000Z</published>
    <updated>2017-03-29T13:53:01.348Z</updated>
    
    <content type="html">&lt;p&gt;在关系数据库的事务中，用户首先向数据库服务器发送BEGIN，然后执行各个相互一致的写操作和读操作，最后，用户可以选择发送COMMIT来确认之前所做的修改，或者发送ROLLBACK来放弃那些修改。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Redis事务处理流程&quot;&gt;&lt;a href=&quot;#Redis事务处理流程&quot; class=&quot;headerlink&quot; title=&quot;Redis事务处理流程&quot;&gt;&lt;/a&gt;Redis事务处理流程&lt;/h3&gt;&lt;p&gt;Redis通过MULTI、EXEC、WATCH等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而该去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。&lt;br&gt;一个事务从开始到结束会经历如下三个阶段：&lt;br&gt;（1）事务开始&lt;br&gt;MULTI命令标志着事务的开始，其将执行该命令的客户端从非事务状态切换至事务状态。&lt;br&gt;（2）命令入队&lt;br&gt;当一个客户端处于非事务状态时，这个客户端发送的命令会立即被服务器执行，而当其切换到事务状态时，服务器会根据客户端发来的不同命令来执行不同的操作：如果是MULTI、EXEC、DISCARD、WATCH四个命令中的一个，那么服务器立即执行；如果不是，那么服务器不立即执行命令，而是将其放入一个服务队列里面，然后向客户端返回QUEUED回复。&lt;br&gt;这些命令保存在每个客户端所持有的一个事务队列中，该队列以FIFO的方式保存入队的命令。&lt;br&gt;（3）执行事务&lt;br&gt;当处于事务状态的客户端向服务器发送EXEC命令时，EXEC命令将立即被服务器执行。服务器会遍历这个客户端的事务队列，执行队列中保持的所有命令，最后将执行结果全部返回给客户端。&lt;br&gt;但是，MULTI和EXEC只能保证它们中间的那些入队的命令不会被其他客户端掺杂在一起，却没有锁的逻辑，即不能解决干扰。比如，现有一个key为1的数字，客户端1需要在事务中连续做两次自增操作，但在EXEC之前，客户端2对key进行了修改&lt;code&gt;set key 100&lt;/code&gt;。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/6PQWySj.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;可以看出，在事务执行的过程中，读到了脏数据，为了保障不被干扰，需要对数据加锁。        &lt;/p&gt;
&lt;h3 id=&quot;乐观锁WATCH&quot;&gt;&lt;a href=&quot;#乐观锁WATCH&quot; class=&quot;headerlink&quot; title=&quot;乐观锁WATCH&quot;&gt;&lt;/a&gt;乐观锁WATCH&lt;/h3&gt;&lt;p&gt;在访问以写入为目的数据的时候，关系数据库会对被访问的数据加锁，直到事务被提交或者被回滚为止。如果有其他客户端试图对被加锁的数据行进行写入，那么该客户端将被阻塞，直到第一个事务执行完毕为止。这种做法称为悲观锁。&lt;br&gt;而Redis为了减少客户端的等待时间，并不会在执行WATCH命令时对数据加锁。相反地，Redis只会在数据已经被其他客户端抢先修改了的情况下，通知执行了WATCH的客户端，这种做法称为乐观锁。当被监视的键至少有一个已经被修改过了，服务器将拒绝执行事务，并向客户端返回代表事务执行失败的空回复。&lt;br&gt;对于上面的例子，使用WATCH监视key。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/PMcavUU.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;当客户端执行EXEC时，服务器发现WATCH监视的键“name”已经被修改，因此服务器拒绝执行客户端A的事务，并向客户端A返回空回复。&lt;br&gt;但是，需要注意的是，WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以，通常情况下在EXEC执行失败后会重新执行整个函数。   &lt;/p&gt;
&lt;h4 id=&quot;利用WATCH实现原子自增&quot;&gt;&lt;a href=&quot;#利用WATCH实现原子自增&quot; class=&quot;headerlink&quot; title=&quot;利用WATCH实现原子自增&quot;&gt;&lt;/a&gt;利用WATCH实现原子自增&lt;/h4&gt;&lt;p&gt;使用伪代码表示为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;WATCH key
val = GET key
val = val + 1
MULTI
SET key $val
EXEC
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;事务的ACID性质&quot;&gt;&lt;a href=&quot;#事务的ACID性质&quot; class=&quot;headerlink&quot; title=&quot;事务的ACID性质&quot;&gt;&lt;/a&gt;事务的ACID性质&lt;/h3&gt;&lt;p&gt;（1）原子性&lt;br&gt;对于Redis的事务功能来说，事务队列中的命令要么就全部都执行，要么就一个都不执行，因此，从这一点来看，Redis的事务是具有原子性的。&lt;br&gt;下图是事务因为命令入队出错而被服务器拒绝执行，事务中的所有命令都不会被执行。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/TnIfK7G.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;但是，Redis事务不支持关系数据库的事务回滚机制，即事务队列中的某个命令在执行期间（非语法错误，执行错误）出现了错误，事务的后续命令也会继续执行下去，并且之前执行的命令没有影响。因此，在这方面是不符合原子性的。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/igmOQqS.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;（2）一致性&lt;br&gt;事务的一致性是指，如果数据库在执行事务之前是一致的，那么在事务执行之后，无论事务是否执行成功，数据库也应该是一致的。&lt;br&gt;如前所述，当出现入队错误或是执行错误的情况，Redis分别通过拒绝执行和进行错误处理的方式来保证事务的一致性。&lt;br&gt;当Redis服务器在执行事务的过程中停机，那么根据服务器使用的持久化方式，分以下几种情况讨论：&lt;br&gt;a. 如果当前Redis采用的是内存模式，那么重启后Redis数据库是空的，满足一致性条件；&lt;br&gt;b. 如果当前Redis采用RDB模式存储，在执行事务时，Redis不会中断事务去执行保存RDB的工作，只有在事务执行之后，保存RDB的工作才有可能开始。所以当RDB模式下的Redis服务器进程在事务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到RDB文件里。恢复数据库需要使用现有的RDB文件，而这个RDB文件的数据保存的是最近一次的数据库快照，所以它的数据可能不是最新的，但只要RDB文件本身没有因为其他问题而出错，那么还原后的数据库就是一致的；&lt;br&gt;c. 如果当前Redis采用的AOF模式存储，那么可能事务的内容还未写入到AOF文件，那么此时肯定是满足一致性的，如果事务的内容有部分写入到AOF文件中，那么需要用工具把AOF中事务执行部分成功的指令移除，这时，移除之后的AOF文件也是满足一致性的。&lt;br&gt;（3）隔离性&lt;br&gt;事务的隔离性是指，即使数据库有多个事务并发地执行，各个事务之间也不会互相影响，并发状态下执行的事务和串行执行的事务产生的结果完全相同。&lt;br&gt;因为Redis采用单线程的方式来执行事务，并且服务器保证，在执行事务期间不会对事务进行中断，因此，Redis的事务总是以串行的方式运行的，且事务总是具有隔离性。&lt;br&gt;（4）持久性&lt;br&gt;事务的持久性是指，当一个事务执行完毕时，执行这个事务所得的结果已经被保存到永久性存储介质里面了，即使服务器在事务执行完毕之后停机，执行事务所得的结果也不会丢失。&lt;br&gt;Redis事务只有当服务器运行在AOF持久化模式下，并且appendfsync选项为always时，才是具有持久性的。因为程序总会在执行命令之后调用同步函数，将命令数据真正地保存到硬盘里。&lt;br&gt;当然，不论Redis以什么模式运行，在一个事务的最后加上SAVE命令总可以保证持久性，但这种做法效率太低。&lt;br&gt;所以，Redis只满足ACID中的一致性和隔离性。 &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;在关系数据库的事务中，用户首先向数据库服务器发送BEGIN，然后执行各个相互一致的写操作和读操作，最后，用户可以选择发送COMMIT来确认之前所做的修改，或者发送ROLLBACK来放弃那些修改。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>更好地使用STL关联容器</title>
    <link href="http://blog.dujiong.net/2016/12/30/Effective-STL-3/"/>
    <id>http://blog.dujiong.net/2016/12/30/Effective-STL-3/</id>
    <published>2016-12-30T02:13:50.000Z</published>
    <updated>2017-01-19T07:13:29.279Z</updated>
    
    <content type="html">&lt;p&gt;在STL的使用过程中，一直对关联容器掌握的不够熟练。这一篇就来总结下使用关联容器时的一些注意问题。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;理解等价关系&quot;&gt;&lt;a href=&quot;#理解等价关系&quot; class=&quot;headerlink&quot; title=&quot;理解等价关系&quot;&gt;&lt;/a&gt;理解等价关系&lt;/h3&gt;&lt;p&gt;在STL中，对两个对象进行比较，看它们的值是否相等，这样的操作随处可见。在实际操作中，相等的概念是基于operator==的。如果表达式“x==y”返回真，则x和y的值相等。&lt;br&gt;等价关系是以“在以排序的区间中对象值的相对顺序”为基础的。如果从每个标准关联容器的排列顺序来考虑等价关系，那么这将是非常有意义的。对于两个对象x和y，如果按照关联容器c的排列顺序，每个都不在另一个的前面，那么称这两个对象按照c的排列顺序由等价的值。&lt;br&gt;下面以一个实例进行说明：    &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bool ciCharLess(char c1, char c2){
    return tolower(static_cast&amp;lt;unsigned char&amp;gt;(c1))&amp;lt;tolower(static_cast&amp;lt;unsigned char&amp;gt;(c2));
}

bool ciStringCompare(const string&amp;amp; s1, const string&amp;amp; s2){
    return lexicographical_compare(s1.begin(), s1.end(), s2.begin(), s2.end(), ciCharLess);
}

struct CIStringCompare : public binary_function&amp;lt;string, string, bool&amp;gt;{
    bool operator()(const string&amp;amp; lhs, const string&amp;amp; rhs){
        return ciStringCompare(lhs, rhs); 
    }
}

int main()
{
    set&amp;lt;string, CIStringCompare&amp;gt; s;
    s.insert(&amp;quot;STL&amp;quot;);
    s.insert(&amp;quot;stl&amp;quot;);

    for(auto n:s){
        cout &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl;            //STL
    }

    if(s.find(&amp;quot;stl&amp;quot;)!=s.end()){
        cout &amp;lt;&amp;lt; &amp;quot;success&amp;quot; &amp;lt;&amp;lt; endl;        //success
    }else{
        cout &amp;lt;&amp;lt; &amp;quot;fail&amp;quot; &amp;lt;&amp;lt; endl;
    }
    if(std::find(s.begin(), s.end(), &amp;quot;stl&amp;quot;)!=s.end()){
        cout &amp;lt;&amp;lt; &amp;quot;success&amp;quot; &amp;lt;&amp;lt; endl;
    }else{
        cout &amp;lt;&amp;lt; &amp;quot;fail&amp;quot; &amp;lt;&amp;lt; endl;            //fail
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;s是一个不区分大小写的set&lt;string&gt;，即当set的比较函数忽略字符串中字符的大小写时的set&lt;string&gt;。这样一个比较函数将把“STL”和“stl”看做是等价的。因此，在先后插入“STL”和“stl”时，只有“STL”会成功插入。如果使用set的find成员函数来查找“stl”时，该查找会成功；而如果是使用非成员的find算法，则查找将失败。因为“STL”和“stl”是等价的。顺便说一句，该例子从一个方面解释了为什么应该优先选用成员函数而不是与之对应的非成员函数。&lt;/string&gt;&lt;/string&gt;&lt;/p&gt;
&lt;h3 id=&quot;为包含指针的关联容器指定比较类型&quot;&gt;&lt;a href=&quot;#为包含指针的关联容器指定比较类型&quot; class=&quot;headerlink&quot; title=&quot;为包含指针的关联容器指定比较类型&quot;&gt;&lt;/a&gt;为包含指针的关联容器指定比较类型&lt;/h3&gt;&lt;p&gt;假如有一个包含string*指针的set，把一些动物的名字插入到该集合中：     &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set&amp;lt;string*&amp;gt; ssp;
ssp.insert(new string(&amp;quot;Anteater&amp;quot;));
ssp.insert(new string(&amp;quot;Wombat&amp;quot;));
ssp.insert(new string(&amp;quot;Lemur&amp;quot;));
ssp.insert(new string(&amp;quot;Penguin&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为集合中所包含的是指针，所以，可能会想到使用下面的代码来打印出动物的名字：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for(set&amp;lt;string*&amp;gt;::const_iterator i=ssp.begin(); i!=ssp.end(); ++i){
    cout &amp;lt;&amp;lt; **i &amp;lt;&amp;lt; endl;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;没错，动物的名称会被打印出来，但它们以字母顺序出现的概率仅为1/24。ssp会按顺序保存它的内容，但因为它包含的是指针，所以会按指针的值而不是按字符串的值进行排序，4个指针的值共有24个可能的排列方式，所以对要存储的指针会有24种可能的排列。&lt;br&gt;为了解决这个问题，需要知道set&lt;string\*&gt; ssp是如下代码set&lt;string\*,less&lt;string*&gt;&amp;gt; ssp的缩写，当然，更精确的讲是set&lt;string\*,less&lt;string\*&gt;,allocator&lt;string\*&gt;&amp;gt;的缩写，只是这里不考虑分配子的影响。&lt;br&gt;因此，如果想让string*指针在集合中按字符串的值排序，那么不能使用默认的比较函数子类。必须自己编写比较函数子类。        &lt;/string\*&gt;&lt;/string\*,less&lt;string\*&gt;&lt;/string\*,less&lt;string*&gt;&lt;/string\*&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct StringPtrLess : public binary_function&amp;lt;const string*, const string*, bool&amp;gt;{
    bool operator() (const string* s1, const string* s2) const{
        return *s1 &amp;lt; *s2;
    }
};  

set&amp;lt;string*, StringPtrLess&amp;gt; ssp;

/*
void print(const string* ps){
    cout &amp;lt;&amp;lt; *ps &amp;lt;&amp;lt; endl;
}

for_each(ssp.begin(), ssp.end(), print);
*/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在上述的打印循环可以做到预期的事情了。&lt;br&gt;所以，当需要创建包含指针的关联容器时，容器将会按照指针的值进行排序。绝大多数情况下，这不会是所希望的，这种情况下，几乎肯定要创建自己的函数子类作为该关联容器的比较类型。&lt;/p&gt;
&lt;h3 id=&quot;考虑用排序的vector替代关联容器&quot;&gt;&lt;a href=&quot;#考虑用排序的vector替代关联容器&quot; class=&quot;headerlink&quot; title=&quot;考虑用排序的vector替代关联容器&quot;&gt;&lt;/a&gt;考虑用排序的vector替代关联容器&lt;/h3&gt;&lt;p&gt;个人使用STL的经历中，当需要一个可提供快速查找功能的数据结构时，都会立刻想到标准关联容器，即set、multiset、map和multimap。但是，它们并不总是适合的。比如，如果查找速度真的很重要，那么，非标准的散列容器（unordered_map等）几乎是值得考虑的。因为通过适当的散列函数，散列容器几乎能提供常数时间的查找能力，优于set、multiset、map和multimap的确定的对数时间查找能力。&lt;br&gt;但是，即使确定的对数时间查找能力满足需求，标准关联容器可能也不是最好的选择。标准关联容器的效率比vector还低的情况并不少见。标准关联容器通常被实现为平衡的二叉查找树。二叉查找树这种数据结构对插入、删除和查找的混合操作做了优化，也就是，它所适合的那些应用程序的主要特征是插入、删除和查找混在一起。即没办法预测出针对这棵树的下一个操作是什么。&lt;br&gt;而还有很多应用程序使用其数据结构的方式并不这么混乱。它们使用其数据结构的过程可以明显地分为三个阶段：&lt;br&gt;（1）设置阶段。创建新的数据结构，并插入大量元素。&lt;br&gt;（2）查找阶段。查询该数据结构以找到特定的信息。&lt;br&gt;（3）重组阶段。改变该数据结构的内容。&lt;br&gt;对于以这种方式使用其数据结构的应用程序来说，vector可能比关联容器提供了更好的性能。但是不是任意的vector，而必须是排序的vector，因为只有对排序的容器才能够正确地使用查找算法binary_search、lower_bound和equal_range等。&lt;br&gt;那么，为什么通过排序的vector执行的二分搜索，比通过二叉查找树执行的二分搜索具有更好的性能呢？&lt;br&gt;其原因主要是：关联容器几乎肯定在使用平衡二叉树。这就意味着在一个关联容器中存储一个类型所伴随的空间开销至少是三个指针（父指针，左儿子，右儿子）。相反，存储在vector中则不会有任何的额外开销；只是简单地存储一个类型。&lt;br&gt;当然，对于排序的vector，最不利的地方在于它必须保持有序！当一个新的元素被插入时，新元素之后的所有元素都必须向后移动一个元素的位置。当一个元素从vector中删除了，则在它之后的所有元素也都要向前移动。插入和删除操作对于vector来说是昂贵的，但对于关联容器却是廉价的。这就是为什么当“对数据结构的使用方式是：查找操作几乎从不跟插入和删除操作混在一起”时，再考虑使用排序的vector而不是关联容器才是合理的。     &lt;/p&gt;
&lt;h3 id=&quot;在map-operator-和map-insert之间选择&quot;&gt;&lt;a href=&quot;#在map-operator-和map-insert之间选择&quot; class=&quot;headerlink&quot; title=&quot;在map::operator[]和map::insert之间选择&quot;&gt;&lt;/a&gt;在map::operator[]和map::insert之间选择&lt;/h3&gt;&lt;p&gt;map的operator[]函数与众不同。它与vector、deque和string的operator[]函数无关，与用于数组的内置operator[]也没有关系。相反，map::operator[]的设计目的是为了提供“添加和更新”的功能，也就是说，对于map&lt;k,v&gt; m；来说，表达式m[k]=v;检查键k是否已经在map中了，如果没有，它就被加入，并以v作为相应的值。如果k已经在映射表中了，则与之关联的值被更新为v。&lt;br&gt;下面以一个例子来说明：      &lt;/k,v&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class Widget {
    public:
        Widget();
        Widget(double weight);
        Widget&amp;amp; operator=(double weight);
    private:
        double weight_;
    ...        
};

map&amp;lt;int, Widget&amp;gt; m;
m[1]=1.50;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;表达式m[1]是m.operator[](1)的缩写形式，即对map::operator[]的调用。该函数必须返回一个指向Widget的引用，因为m所映射的值类型是Widget。这时，m中什么都没有，所以键1没有对应的值对象。因此，operator[]默认构造了一个Widget，作为与1相关联的值，然后返回一个指向该Widget的引用。最后，这个Widget成了赋值的目标。因此，m[1]=1.50在功能上等价于以下代码：      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;typedef map&amp;lt;int, Widget&amp;gt; IntWidgetMap;

pair&amp;lt;IntWidgetMap::iterator, bool&amp;gt; result = m.insert(IntWidgetMap::value_type(1, Widget()));

result.first-&amp;gt;second = 1.50;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因此，使用operator[]会降低效率。因为我们先默认构造了一个Widget，然后立刻赋给它新的值。而如果我们换成对insert的直接调用：    &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;m.insert(IntWidgetMap::value_type(1, 1.50));
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最终效果和前面相同，但是它通常会节省三个函数调用：一个用于创建默认构造的临时Widget对象，一个用于析构该临时对象，还有一个是调用Widget的赋值描述符。&lt;br&gt;而operator[]的设计目的是为了提供“添加和更新”的功能，现在我们已经知道，当做为“添加”操作时，insert比operator[]效率更高，而当我们做更新操作时，即当一个等价的键已经在映射表中时，形势就反过来了。因为调用insert时，必须构造和析构一个pair类型的对象，需要付出一个pair构造函数和一个pair析构函数的代价。而这又会导致对Widget的构造和析构动作。而operator[]不使用pair对象，所以它不会构造和析构任何pair或Widget。&lt;br&gt;总结：当向映射表中添加元素时，优先选用insert而不是operator[]；而当更新已经在映射表中的元素的值时，要优先选择operator[]。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;在STL的使用过程中，一直对关联容器掌握的不够熟练。这一篇就来总结下使用关联容器时的一些注意问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.dujiong.net/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>协程</title>
    <link href="http://blog.dujiong.net/2016/12/23/Coroutine/"/>
    <id>http://blog.dujiong.net/2016/12/23/Coroutine/</id>
    <published>2016-12-23T10:41:42.000Z</published>
    <updated>2017-01-19T08:53:27.830Z</updated>
    
    <content type="html">&lt;p&gt;协程，顾名思义，是“协作的例程”。跟具有操作系统概念的线程不一样，协程是在用户空间利用程序语言的语法语义就能实现逻辑上类似多任务的编程技巧。协程可以在运行期间的某个点上暂停执行，并在恢复运行时从暂停的点上继续执行。协程已经被证明是一种非常有用的程序组件，不仅被Python、lua、ruby等脚本语言广泛语言，还被新一代面多核的编程语言如golang等作为并发的基本单位。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;   &lt;/p&gt;
&lt;h3 id=&quot;协程的特点&quot;&gt;&lt;a href=&quot;#协程的特点&quot; class=&quot;headerlink&quot; title=&quot;协程的特点&quot;&gt;&lt;/a&gt;协程的特点&lt;/h3&gt;&lt;p&gt;协程的调度完全由用户控制，一个线程可以有多个协程，每个协程都是循环按照指定的任务清单顺序完成不同的任务，当任务被阻塞的时候执行下一个任务，当恢复的时候再回来执行这个任务，任务之间的切换只需要保存每个任务的上下文内容，就像直接操作栈一样，这样就完全没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。&lt;br&gt;因此，与传统的抢占式线程相比，协程主要具有以下两个优点：&lt;br&gt;（1）与线程不同，线程是自己主动让出CPU，并交付它期望的下一个协程运行，而不是在任何时候都有可能被系统调度打断。因此协程的使用更加清晰易懂，并且大多数情况下不需要锁机制。&lt;br&gt;（2）与线程相比，协程的切换由程序控制，发生在用户空间而非内核空间，因此切换的代价非常小。    &lt;/p&gt;
&lt;h3 id=&quot;进程与线程&quot;&gt;&lt;a href=&quot;#进程与线程&quot; class=&quot;headerlink&quot; title=&quot;进程与线程&quot;&gt;&lt;/a&gt;进程与线程&lt;/h3&gt;&lt;p&gt;下面简单回顾下进程与线程的概念。   &lt;/p&gt;
&lt;h4 id=&quot;进程&quot;&gt;&lt;a href=&quot;#进程&quot; class=&quot;headerlink&quot; title=&quot;进程&quot;&gt;&lt;/a&gt;进程&lt;/h4&gt;&lt;p&gt;进程是具有一定独立功能的程序关于某个数据集合上的一次活动，进程是系统进行资源分配和调度的单位。&lt;br&gt;进程之间不共享任何状态，进程的调度由操作系统完成，每个进程都有自己的独立的内存空间，而进程间的通信主要是通过信号传递的方式来实现的，实现的方式有多种，如信号量、管道等，但是任何一种方式的通信都需要通过内核，因此效率比较低。同时，由于进程拥有的是独立的内存空间，所以在进行上下文切换的时候需要先保存调用栈的信息，CPU各寄存器的信息，虚拟内存以及打开的句柄等信息，所以导致进程间切换开销很大。&lt;/p&gt;
&lt;h4 id=&quot;线程&quot;&gt;&lt;a href=&quot;#线程&quot; class=&quot;headerlink&quot; title=&quot;线程&quot;&gt;&lt;/a&gt;线程&lt;/h4&gt;&lt;p&gt;线程是进程的一个实体，是CPU调度的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点再运行中比不可少的资源（如程序计数器、一些寄存器和栈），它与同一个进程的其他线程共享所拥有的全部资源。&lt;br&gt;线程之间共享变量，解决了通信麻烦的问题，但同时，多个线程对变量的访问需要进行同步与互斥操作。线程的调度也主要由操作系统完成，一个进程可以拥有多个线程，每个线程会共享父进程向操作系统申请的资源，包括虚拟内存，文件等，因此创建线程所需的资源要比进程小很多，相应的可创建的线程数量也变得多很多。线程之间的通信除了可以使用进程之间通信的方式之外还可以通过共享内存的方式，此外，在线程调度方面，由于线程间共享进程的资源，所以上下文切换的时候需要保存的东西相对少些，上下文也因此高效一些。     &lt;/p&gt;
&lt;h3 id=&quot;构建C协程&quot;&gt;&lt;a href=&quot;#构建C协程&quot; class=&quot;headerlink&quot; title=&quot;构建C协程&quot;&gt;&lt;/a&gt;构建C协程&lt;/h3&gt;&lt;p&gt;C/C++不直接支持协程语义，但目前已有不少开源的协程库，本文以其中最常用的使用glibc的ucontext组件的实现方式进行说明。&lt;/p&gt;
&lt;h4 id=&quot;ucontext组件&quot;&gt;&lt;a href=&quot;#ucontext组件&quot; class=&quot;headerlink&quot; title=&quot;ucontext组件&quot;&gt;&lt;/a&gt;ucontext组件&lt;/h4&gt;&lt;p&gt;ucontext组件是GNU C库提供的一组用于创建、保存、切换用户态执行”上下文”的API。主要包括以下两个结构体和四个函数：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//mcontest_t类型与机器相关，并且不透明
typedef struct ucontext {
    struct ucontext* uc_link;    //链接下一个执行的上下文
    sigset_t uc_sigmask;    //阻塞信号集合
    stack_t uc_stack;        //该上下文中使用的栈
    mcontext_t uc_mcontest;
    ...
} ucontext_t;

//初始化一个ucontext_t类型的结构，即用户执行上下文。
//函数指针func指明了该context的入口函数，argc指入口参数个数    
void makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);
//&amp;quot;原子&amp;quot;地完成旧状态的保存和切换到新状态的工作
int swapcontext(ucontext_t *oucp, ucontext_t *ucp);
//将当前执行上下文保存到ucp中，若后续调用setcontext或swapcontext恢复状态，
//则程序会沿着getcontext调用点之后继续执行，看起来好像刚从getcontext函数返回一样。
int getcontext(ucontext_t *ucp);
//将当前程序执行线索切换到ucp所指向的上下文状态，
//在执行正确的情况下，该函数直接切入到新的执行状态，不再回返回
int setcontext(const ucontext_t *ucp);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;来看一个简单的实例：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;ucontext.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main()
{
    ucontext_t context;

    getcontext(&amp;amp;context);    
    puts(&amp;quot;hello world&amp;quot;);
    sleep(1);
    setcontext(&amp;amp;context);
    return 0;
}         
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行结果如下：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/SvtkRI7.png&quot; alt=&quot;&quot;&gt;               &lt;/p&gt;
&lt;p&gt;程序通过getcontext保存了一个上下文，然后输出”hello world”，睡一秒后执行到setcontext，恢复上下文到getcontext之后，重新执行代码，所以程序不断输出“hello world”。   &lt;/p&gt;
&lt;h4 id=&quot;使用ucontext实现线程切换&quot;&gt;&lt;a href=&quot;#使用ucontext实现线程切换&quot; class=&quot;headerlink&quot; title=&quot;使用ucontext实现线程切换&quot;&gt;&lt;/a&gt;使用ucontext实现线程切换&lt;/h4&gt;&lt;p&gt;虽然我们称协程是一个用户态的轻量级线程，但实际上多个协程同属于一个线程。任意一个时刻，同一个线程不可能同时运行两个协程。&lt;br&gt;接下来通过一个实例说明协程与主函数的切换，即实现协程的调度。 &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;ucontext.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

void func1(void *arg)
{
    puts(&amp;quot;1&amp;quot;);
    puts(&amp;quot;11&amp;quot;);
    puts(&amp;quot;111&amp;quot;);
    puts(&amp;quot;1111&amp;quot;);
}

void context_test()
{
    char stack[1024*128];
    ucontext_t child, main;
    getcontext(&amp;amp;child);
    child.uc_stack.ss_sp = stack;
    child.uc_stack.ss_size = sizeof(stack);
    child.uc_link = &amp;amp;main;
    makecontext(&amp;amp;child, (void (*)(void))func1, 0);
    swapcontext(&amp;amp;main, &amp;amp;child);
    puts(&amp;quot;main&amp;quot;);
}

int main()
{
    context_test();
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行结果如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/FbFrKKW.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在context_test中，创建了一个用户线程（协程）child，其运行的函数为func1，指定后继上下文为main，当func1返回后激活后继上下文，继续执行main函数。&lt;/p&gt;
&lt;h3 id=&quot;开源C-C-协程库&quot;&gt;&lt;a href=&quot;#开源C-C-协程库&quot; class=&quot;headerlink&quot; title=&quot;开源C/C++协程库&quot;&gt;&lt;/a&gt;开源C/C++协程库&lt;/h3&gt;&lt;p&gt;最后，罗列几个比较出名的开源C/C++协程库，后面争取再深入学习下。&lt;br&gt;libco： &lt;a href=&quot;http://code.tencent.com/libco.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;腾讯的开源协程库&lt;/a&gt;&lt;br&gt;coroutine: &lt;a href=&quot;https://github.com/cloudwu/coroutine/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;云风大牛的作品&lt;/a&gt;&lt;br&gt;Protothreads: &lt;a href=&quot;http://coolshell.cn/articles/10975.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;一个”蝇量级”C语言协程库&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;协程，顾名思义，是“协作的例程”。跟具有操作系统概念的线程不一样，协程是在用户空间利用程序语言的语法语义就能实现逻辑上类似多任务的编程技巧。协程可以在运行期间的某个点上暂停执行，并在恢复运行时从暂停的点上继续执行。协程已经被证明是一种非常有用的程序组件，不仅被Python、lua、ruby等脚本语言广泛语言，还被新一代面多核的编程语言如golang等作为并发的基本单位。&lt;br&gt;
    
    </summary>
    
    
      <category term="进程/线程/并发" scheme="http://blog.dujiong.net/tags/%E8%BF%9B%E7%A8%8B-%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>使用STL算法时需注意的问题</title>
    <link href="http://blog.dujiong.net/2016/12/19/Effective-STL-2/"/>
    <id>http://blog.dujiong.net/2016/12/19/Effective-STL-2/</id>
    <published>2016-12-19T12:21:14.000Z</published>
    <updated>2016-12-29T13:46:36.245Z</updated>
    
    <content type="html">&lt;p&gt;接着上一篇，对STL中非常有用而又容易犯错的一些算法做总结和记录。希望在后面的使用中更加注重STL算法。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;确保目标空间足够大&quot;&gt;&lt;a href=&quot;#确保目标空间足够大&quot; class=&quot;headerlink&quot; title=&quot;确保目标空间足够大&quot;&gt;&lt;/a&gt;确保目标空间足够大&lt;/h3&gt;&lt;p&gt;如上篇所述，当有新的对象加入容器时，STL容器会自动扩充存储空间以容纳这些对象。但是，需要注意的是，STL容器并不总是能够正确地管理其存储空间。比如下面这个例子：     &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int trans(int x){
    return -x;
}

vector&amp;lt;int&amp;gt; ivec{1,2};
vector&amp;lt;int&amp;gt; res;

std::transform(ivec.begin(), ivec.end(), res.begin(), trans);    // error
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这段代码中，transform首先以ivec[0]为参数调用trans，并将结果赋给&lt;em&gt;\res.end()。然后，再以ivec[1]为参数调用trans，并将结果赋给\&lt;/em&gt;(res.end()+1)。这可能会引起灾难性的后果，因为在*res.end()中并没有对象，*(res.end()+1)就更没有对象了。这种transform调用时错误的，因为导致了对无效对象的赋值操作。&lt;br&gt;犯这种错误的程序员总是希望他们所调用的算法的结果会被插入到目标容器中。事实上，必须向STL明确表达意图。所以，上面的例子需要通过调用back_insert生成一个迭代器来指定目标区间的起始位置：      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int trans(int x){
    return -x;    
}

vector&amp;lt;int&amp;gt; ivec{1,2};
vector&amp;lt;int&amp;gt; res;

std::transform(ivec.begin(), ivec.end(), back_inserter(res), trans);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在内部，back_insert返回的迭代器将使得push_back被调用，所以back_insert可使用于所有提供push_back方法的容器。而如果想让一个算法在容器的头部而不是尾部插入对象可以使用front_inserter。&lt;br&gt;因此，无论何时，如果所使用的算法需要指定一个目标区间，那么必须确保目标区间足够大，或者确保它会随着算法的运行而增大。要么在算法执行过程中增大目标区间，请使用插入型迭代器，比如ostream_iterator，或者由back_inserter、front_inserter返回的迭代器。&lt;/p&gt;
&lt;h3 id=&quot;与排序有关的选择&quot;&gt;&lt;a href=&quot;#与排序有关的选择&quot; class=&quot;headerlink&quot; title=&quot;与排序有关的选择&quot;&gt;&lt;/a&gt;与排序有关的选择&lt;/h3&gt;&lt;p&gt;提到排序，首先想到的是std::sort。但需要注意的是，std::sort并非在任何场合下都是完美无缺的。有些场景并不需要完全的排序操作。比如使用partial_sort来选择前n个最好的商品送给n个最重要的客户，或者当只需要将最好的20个商品送给最重要的20个客户，而不关心哪个商品送给哪位客户的时候选用nth_element。   &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; ivec{2,5,10,23,12,56,34,100};

std::partial_sort(ivec.begin(), ivec.begin()+4, ivec.end(), greater&amp;lt;int&amp;gt;());
for(auto n : ivec){
    cout &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行结果：&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/5BIVF20.png&quot; alt=&quot;&quot;&gt;   &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; ivec{2,5,10,23,12,56,34,100};

std::nth_element(ivec.begin(), ivec.begin()+3, ivec.end(), greater&amp;lt;int&amp;gt;());
for(auto n : ivec){
    cout &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行结果:&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/aZpaEcc.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;可以看出，对nth_element的调用与partial_sort几乎一样。在效果上唯一不同之处在于，partial_sort对前20个元素进行了排序，而nth_element没有对他们进行排序。&lt;br&gt;sort、partial_sort、nth_element和stable_sort都属于非稳定的排序算法，有一个名为stable_sort的算法可以提供稳定排序特性。&lt;br&gt;另外，sort、partial_sort、nth_element和stable_sort算法都要求随机访问迭代器，所以这些算法只能被应用于vector、string、deque和数组。对标准关联容器中的元素进行排序并没有实际意义，因为这样的容器总是使用比较函数来维护内部元素的有序性。list是唯一需要排序却无法使用这些排序算法的容器，为此，list特别提供了sort成员函数（稳定排序）。    &lt;/p&gt;
&lt;h3 id=&quot;remove算法后调用erase删除元素&quot;&gt;&lt;a href=&quot;#remove算法后调用erase删除元素&quot; class=&quot;headerlink&quot; title=&quot;remove算法后调用erase删除元素&quot;&gt;&lt;/a&gt;remove算法后调用erase删除元素&lt;/h3&gt;&lt;p&gt;我们需要知道，从容器中删除元素唯一的办法是调用容器的成员函数，几乎总是erase的某种形式。因此，remove并不知道它操作的元素所在的容器，所以remove不可能从容器中删除元素。那么remove究竟做了什么？&lt;br&gt;其实，remove移动了区间中的元素，其结果是，“不用背删除”的元素移到了区间的前部（保持原来的相对顺序）。返回的是一个迭代器指向最后一个“不用背删除”的元素之后的元素。这个返回值相当于该区间“新的逻辑结尾”。但是，需要注意的是，通常来说，当调用了remove以后，从区间中被删除的那些元素可能在也可能不在区间中，这是算法操作的附带结果。&lt;br&gt;所以，如果真想删除元素，那就必须在remove之后使用erase。要删除的元素的位置从“新的逻辑结尾”一直到原区间的结尾。比如:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; ivec;
...
v.erase(remove(v.begin(), v.end(), 99), v.end());    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;此外，remove并不是唯一一个适用于这种情形的算法，其他还有两个属于“remove类”的算法：remove_if和unique。&lt;/p&gt;
&lt;h3 id=&quot;使用accumulate或者for-each进行区间统计&quot;&gt;&lt;a href=&quot;#使用accumulate或者for-each进行区间统计&quot; class=&quot;headerlink&quot; title=&quot;使用accumulate或者for_each进行区间统计&quot;&gt;&lt;/a&gt;使用accumulate或者for_each进行区间统计&lt;/h3&gt;&lt;p&gt;有时候，我们需要按照某种自定义的方式对区间进行统计处理。这个时候，必须自己定义统计方法。STL通过算法accumulate来提供。&lt;br&gt;accumulate有两种形式：第一种形式有两个迭代器和一个初始值，它返回该初始值加上由迭代器标识的区间中的值的总和。如下面的例子：      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; ivec{1,2,3};

std::accumulate(ivec.begin(), ivec.end(), 0);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而另一种形式是accumulate带一个初始值和一个任意的统计函数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; ivec{2,2,3};

std::accumulate(ivec.begin(), ivec.end(), 2, multiplies&amp;lt;int&amp;gt;());  //24    
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另一个可用来统计区间的算法时for_each，如图accumulate一样，for_each也带两个参数：一个是区间，另一个是函数（通常是函数对象）—对区间中的每个元素都要调用这个函数，但是，传给for_each的这个函数只接受一个实参（即当前的区间元素）；for_each执行完毕后会返回它的函数。&lt;/p&gt;
&lt;h3 id=&quot;使用排序的区间作为参数的算法&quot;&gt;&lt;a href=&quot;#使用排序的区间作为参数的算法&quot; class=&quot;headerlink&quot; title=&quot;使用排序的区间作为参数的算法&quot;&gt;&lt;/a&gt;使用排序的区间作为参数的算法&lt;/h3&gt;&lt;p&gt;有些算法既可以与排序的区间一起工作，也可以与未排序的区间一起工作，但是当它们作用在排序的区间上时，算法会更加有效。&lt;br&gt;首先，罗列出要求排序区间的STL算法:&lt;br&gt;binary_search, lower_bound, upper_bound, equal_range, set_union, set_intersection, set_difference, set_symmetric_difference, merge, inplace_merge, includes&lt;br&gt;还有一些算法并不一定要求排序的区间，但通常情况下会与排序区间一起使用。&lt;br&gt;unique, unique_copy&lt;br&gt;这其中，用于查找的算法binary_search, lower_bound, upper_bound, equal_range要求排序的区间，因为它们用二分法查找数据。&lt;br&gt;set_union, set_intersection, set_difference, set_symmetric_difference这四个算法提供了线性时间效率的集合操作。它们要求排序的区间，因为如果不满足，它们就无法在线性时间内完成工作。&lt;br&gt;merge和inplace_merge实际上实现了合并和排序的联合操作：它们读入两个排序的区间，然后合并成一个新的排序区间。如果源区间没有排过序，就不可能在线性时间内完成。&lt;br&gt;同样，includes也要求两个区间是排序的，承诺线性时间的效率。&lt;br&gt;而unique和unique_copy与上述讨论过的算法有所不同，它们即使对于未排序的区间也有很好的行为。以unique为例，unique用于删除区间中所有重复的元素，所以必须保证所有相等的元素都是连续存放的。因此，总是要确保传给unique的区间是排序的。&lt;br&gt;顺便提一下，unique使用了与remove类似的办法来删除区间中的元素，而并非真正意义上的删除。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;接着上一篇，对STL中非常有用而又容易犯错的一些算法做总结和记录。希望在后面的使用中更加注重STL算法。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.dujiong.net/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>使用vector和string时需注意的问题</title>
    <link href="http://blog.dujiong.net/2016/12/11/Effective-STL-1/"/>
    <id>http://blog.dujiong.net/2016/12/11/Effective-STL-1/</id>
    <published>2016-12-11T09:39:38.000Z</published>
    <updated>2017-01-03T02:05:31.993Z</updated>
    
    <content type="html">&lt;p&gt;随着对STL的频繁使用和着迷，越发意识到更深层次理解STL的重要性。使用STL正确得到结果是一回事，而高效地使用STL得到结果又是另一回事。因此，拿起了Scott Meyers大神的《Effective STL》，结合自己平时遇到的问题，作一些总结和记录。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;使用reserve避免不必要的重新分配&quot;&gt;&lt;a href=&quot;#使用reserve避免不必要的重新分配&quot; class=&quot;headerlink&quot; title=&quot;使用reserve避免不必要的重新分配&quot;&gt;&lt;/a&gt;使用reserve避免不必要的重新分配&lt;/h3&gt;&lt;p&gt;我们知道，STL容器最大的进步之一是它们会自动增长以便容纳下所放入的数据，只要没有超出它们的最大限制（可以使用max_size()成员函数查看）就可以。vector和string的增长过程是：&lt;br&gt;（1）分配一块大小为当前容量的某个倍数的新内存，在大多数实现中，vector和string的容量每次以2的倍数增长，即每当容器需要扩张时，它们的容量即加倍。&lt;br&gt;（2）把容器的所有元素从旧的内存复制到新的内存中。&lt;br&gt;（3）析构掉旧内存中的对象。&lt;br&gt;（4）释放旧内存。&lt;br&gt;可以看出，这个过程会非常耗时。因此，可以使用reserve成员函数来讲重新分配的次数减少到最低限度，从而避免了重新分配和迭代器/引用失效带来的开销。&lt;br&gt;因此，当一个元素需要被插入而容器的容量不够时，就会发生重新分配过程。比如：创建一个1到1000之间的vector&lt;int&gt;，如果采用这样的方式：&lt;/int&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; ivec;
for(int i=1;i&amp;lt;=1000;i++)
    ivec.push_back(i);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;对于大多数STL实现，该循环在进行过程中将导致2到10次重新分配。而如果改用reserve。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;int&amp;gt; v;
v.reserve(1000);
for(int i=1;i&amp;lt;=1000;++i)
    v.push_back(i);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么，在循环过程中，将不会再发生重新分配。&lt;br&gt;所以，当能确切知道或大致预计容器中最终会有多少元素时，使用reserve。如果不知道，可以先预留足够大的空间，然后，当所有数据都加入以后，再去除多余的容量。   &lt;/p&gt;
&lt;h3 id=&quot;把vector和string数据传递给旧的C-API&quot;&gt;&lt;a href=&quot;#把vector和string数据传递给旧的C-API&quot; class=&quot;headerlink&quot; title=&quot;把vector和string数据传递给旧的C API&quot;&gt;&lt;/a&gt;把vector和string数据传递给旧的C API&lt;/h3&gt;&lt;p&gt;在使用C/C++时，我们发现很多时候还有旧的C API的身影，它们使用数组和char*指针而不是vector或string对象来进行数据交换。因此，在使用STL时，也必须处理好与旧的C API之间的关系。&lt;br&gt;比如，有一个vector v，而需要得到一个指向v中数据的指针，从而可把v中的数据作为数组来对待，那么只需要使用&amp;amp;v[0]就可以了。而对于string s,对应的形式是s.c_str()。&lt;br&gt;所以，对于给定的C API：        &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void doSomething(const int* pInts, size_t numInts);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以这样使用：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if(!v.empty()){
    doSomething(&amp;amp;v[0], v.size());
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;请注意，不要使用v.begin()来代替&amp;amp;v[0]，因为begin的返回值是一个迭代器，不是指针；当需要一个指向vector中的数据的指针时，永远不应该使用begin()。（可以使用&amp;amp;*v.begin()）&lt;br&gt;但是，上述方法对string却是不可靠的。原因如下：&lt;br&gt;（1）string中的数据不一定存储在连续的内存中；&lt;br&gt;（2）string的内部表示不一定是以空字符结尾的。&lt;br&gt;所以，对于给定的C API：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void doSomething(const char* pString)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以这样使用： &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;doSomething(s.c_str())    
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;避免使用vector&quot;&gt;&lt;a href=&quot;#避免使用vector&quot; class=&quot;headerlink&quot; title=&quot;避免使用vector&quot;&gt;&lt;/a&gt;避免使用vector&lt;bool&gt;&lt;/bool&gt;&lt;/h3&gt;&lt;p&gt;作为一个STL容器，vector&lt;bool&gt;有两点问题。首先，它不是一个STL容器。其次，它并不存储bool。除此之外，一切正常。&lt;br&gt;vector&lt;bool&gt;不是一个STL容器，原因是一个对象要成为STL容器，必须满足的一个条件是，支持operator[]。也就是当使用operator[]取得了容器Container&lt;t&gt;中的一个T对象，那么可以通过取它的地址得到一个指向该对象的指针。&lt;br&gt;因此，如果vector&lt;bool&gt;是一个容器，那么下面这段代码必须可以被编译：&lt;/bool&gt;&lt;/t&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vector&amp;lt;bool&amp;gt; v;
bool *p = &amp;amp;v[0];
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是，它不能通过编译。原因是，vector&lt;bool&gt;是一个假的容器，它并不真的储存bool，相反，为了节省空间，它储存的是bool的紧凑表示。在一个典型的实现中，储存在”vector”中的每个”bool”仅占一个二进制位，一个8位的字节可容纳8个”bool”。在内部，vector&lt;bool&gt;使用了与位域一样的思想，来表示它所存储的那些bool；实际上它只是假装存储了这些bool。&lt;br&gt;位域与bool相似，它只能表示两个可能的值，但是在bool与看似bool的位域之间有一个重要的区别：可以创建一个指向bool的指针，而指向单个位的指针则是不允许的。&lt;br&gt;所以，在上述的实验中，vector&lt;bool&gt;::operator[]需要返回一个指向单个位的引用，而这样的引用却不存在。&lt;br&gt;那么。当需要vector&lt;bool&gt;的时候，应该使用什么呢？&lt;br&gt;标准库提供了两种选择，可以满足绝大多数的需求。第一种是deque&lt;bool&gt;。deque几乎提供了vector所提供的一切，而deque&lt;bool&gt;是一个STL容器，并且确实存储bool。只是需要注意的是，deque中的元素的内存不是连续的，所以不能把deque&lt;bool&gt;中的数据传递给一个期望bool数组的C API。&lt;br&gt;第二种方案时bitset。bitset不是STL容器，但它是标准C++库的一部分。与STL容器不同的是，它的大小（即元素的个数）在编译时就确定了，所以它不支持插入和删除元素。而且，它不支持迭代器。但是，与vector&lt;bool&gt;一样，它使用了一种紧凑表示，只为包含所包含的每个值提高一位空间，它提供了vector&lt;bool&gt;所特有的flip成员函数，以及其他一些特有的、对位的集合有意义的成员函数。在不需要迭代器和动态改变大小的环境下，bitset很适合需要。&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/bool&gt;&lt;/p&gt;
&lt;h3 id=&quot;string实现的多样性&quot;&gt;&lt;a href=&quot;#string实现的多样性&quot; class=&quot;headerlink&quot; title=&quot;string实现的多样性&quot;&gt;&lt;/a&gt;string实现的多样性&lt;/h3&gt;&lt;p&gt;几乎每个string实现都包含如下信息：&lt;br&gt;（1）字符串的大小，即所包含的字符的个数。&lt;br&gt;（2）用于存储该字符串中字符的内存的容量。&lt;br&gt;（3）字符串的值。&lt;br&gt;除此之外，一个string可能还包含：&lt;br&gt;（1）它的分配子的一份拷贝。&lt;br&gt;（2）对值的引用计数。&lt;br&gt;不同的string实现以不同的方式来组织这些信息。下面以4种不同的string实现方式来说明。它们来源于四种STL实现。    &lt;/p&gt;
&lt;h4 id=&quot;实现A&quot;&gt;&lt;a href=&quot;#实现A&quot; class=&quot;headerlink&quot; title=&quot;实现A&quot;&gt;&lt;/a&gt;实现A&lt;/h4&gt;&lt;p&gt;每个string对象包含其分配子的一份拷贝、该字符串的大小、它的容量以及一个指针，该指针指向一块动态分配的内存，其中包含了引用计数和字符串的值。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/5ySzRwe.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;在该实现中，使用默认分配子的string对象是一个指针的4倍。若使用了自定义的分配子，则string对象会更大一些，多出的部分取决于分配子对象的大小。    &lt;/p&gt;
&lt;h4 id=&quot;实现B&quot;&gt;&lt;a href=&quot;#实现B&quot; class=&quot;headerlink&quot; title=&quot;实现B&quot;&gt;&lt;/a&gt;实现B&lt;/h4&gt;&lt;p&gt;在实现B中，string对象与指针大小相同，因为它只包含一个指向结构的指针。B的string所指向的对象中包含了该字符串的大小、容量和引用计数，以及一个指向动态分配的内存的指针，该内存中存放了字符串的值。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/NhnOLNi.png&quot; alt=&quot;&quot;&gt;      &lt;/p&gt;
&lt;h4 id=&quot;实现C&quot;&gt;&lt;a href=&quot;#实现C&quot; class=&quot;headerlink&quot; title=&quot;实现C&quot;&gt;&lt;/a&gt;实现C&lt;/h4&gt;&lt;p&gt;在实现C中，string对象的大小总是与指针的相同，该指针指向一块动态分配的内存，其中包含了与该字符串相关的一切数据：它的大小、容量、引用计数和值。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/pdowvMe.png&quot; alt=&quot;&quot;&gt;    &lt;/p&gt;
&lt;h4 id=&quot;实现D&quot;&gt;&lt;a href=&quot;#实现D&quot; class=&quot;headerlink&quot; title=&quot;实现D&quot;&gt;&lt;/a&gt;实现D&lt;/h4&gt;&lt;p&gt;实现D的string对象是指针大小的7倍（仍然假定使用的是默认的分配子）。这一实现不使用引用计数，但是每个string对象内部包含一块内存，最大可容纳15个字符的字符串。因此，小的字符串可以完整地存放在string对象中，这通常被称为“小字符串优化”特性。而当一个string的容量超过15时，该内存的起始部分被当做一个指向一块动态分配的内存的指针，而该string的值就在这块内存中。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/dD1svMe.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;比较&quot;&gt;&lt;a href=&quot;#比较&quot; class=&quot;headerlink&quot; title=&quot;比较&quot;&gt;&lt;/a&gt;比较&lt;/h4&gt;&lt;p&gt;（1）在以引用计数为基础的设计方案中，string对象之外的一切可以被多个string对象（如果它们有同样的值）。所以，实现A比实现B或C提供了较小的共享能力。尤其是，实现B和C可以共享string的大小和容量，从而减少每个对象存储这些数据的平均开销。&lt;br&gt;（2）创建一个新的字符串值可能需要零次、一次或两次动态分配。      &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;随着对STL的频繁使用和着迷，越发意识到更深层次理解STL的重要性。使用STL正确得到结果是一回事，而高效地使用STL得到结果又是另一回事。因此，拿起了Scott Meyers大神的《Effective STL》，结合自己平时遇到的问题，作一些总结和记录。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.dujiong.net/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>《C++编程思想》之字符串学习笔记</title>
    <link href="http://blog.dujiong.net/2016/12/08/String/"/>
    <id>http://blog.dujiong.net/2016/12/08/String/</id>
    <published>2016-12-08T13:10:07.000Z</published>
    <updated>2016-12-07T08:33:13.404Z</updated>
    
    <content type="html">&lt;p&gt;字符串是C++标准库的一个重要组成部分，本文参照《C++编程思想》中字符串的相关内容，对该部分进行简单总结。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;C字符型数组和C-字符串&quot;&gt;&lt;a href=&quot;#C字符型数组和C-字符串&quot; class=&quot;headerlink&quot; title=&quot;C字符型数组和C++字符串&quot;&gt;&lt;/a&gt;C字符型数组和C++字符串&lt;/h3&gt;&lt;p&gt;在C语言中，字符串基本上就是字符串数组，并且总是以二进制零（空结束符）作为最末元素。C++ string与它们在C语言中的前身截然不同。首先，也是最重要的不同点，C++隐藏了它所包含的字符序列的物理表示。程序设计人员不必关心数组的维数或空结束符方面的问题。C++ string对象知道自己在内存中的开始位置、包含的内容、包含的字符长度以及在必需重新调整内部缓冲区的大小之前自己可以增长到的最大字符长度。C++字符串极大地减少了C语言编程中最常见且最具破坏性的错误：（1）超越数组边界；（2）通过未被初始化或被赋以错误值的指针来访问数组元素；（3）在释放了某一数组原先所分配的存储单元后仍保留了“悬挂”指针。&lt;br&gt;C++标准没有定义字符串类内存布局的确切实现。采用这种体系结构是为了获得足够的灵活性，从而允许不同的编译器厂商能够提供不同的实现，并且向用户保证提供可预测的行为。&lt;/p&gt;
&lt;h3 id=&quot;创建并初始化C-字符串&quot;&gt;&lt;a href=&quot;#创建并初始化C-字符串&quot; class=&quot;headerlink&quot; title=&quot;创建并初始化C++字符串&quot;&gt;&lt;/a&gt;创建并初始化C++字符串&lt;/h3&gt;&lt;p&gt;创建C++ string一般有以下几种方式：&lt;br&gt;（1）&lt;code&gt;string temp;&lt;/code&gt;  创建空string对象，且并不立即用字符数据对其初始化；&lt;br&gt;（2）&lt;code&gt;string temp(&amp;quot;hello world&amp;quot;);&lt;/code&gt;  将一个文字的引用数组作为参数传递给构造函数，以此来对一个string对象进行初始化；&lt;br&gt;（3）&lt;code&gt;string temp = &amp;quot;hello world&amp;quot;;&lt;/code&gt;  用等号来初始化一个string对象;&lt;br&gt;（4）&lt;code&gt;string temp(str);&lt;/code&gt;        用一个string对象看来初始化另一个string对象。&lt;br&gt;另外，还有一些相关的成员函数和特性：&lt;br&gt;（1）用operator+来将不同的初始化数据源结合在一起；&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;string s1(&amp;quot;hello&amp;quot;);
std::cout &amp;lt;&amp;lt; s1 + &amp;quot; world&amp;quot; &amp;lt;&amp;lt; std::endl;        //hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;（2）用string对象的成员函数substr()来创建一个子串。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;string s1(&amp;quot;hello world, hello&amp;quot;);
std::cout &amp;lt;&amp;lt; s1.substr(0, 11) &amp;lt;&amp;lt; std::endl;        //hello world   \
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;string类对象的成员函数substr()将开始位置作为其第一个参数，而将待选字符的个数作为其第2个参数。&lt;br&gt;（3）将string看做容器对象，利用string类的迭代器string::begin()和string::end()。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;string s1(&amp;quot;hello world&amp;quot;);
string s2(s1.begin(), s2.end());
std::cout &amp;lt;&amp;lt; s2 &amp;lt;&amp;lt; std::endl;        //hello world
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;不可以使用单个的字符、ASCII码或其他整数值来初始化C++字符串。&lt;/p&gt;
&lt;h3 id=&quot;字符串的操作&quot;&gt;&lt;a href=&quot;#字符串的操作&quot; class=&quot;headerlink&quot; title=&quot;字符串的操作&quot;&gt;&lt;/a&gt;字符串的操作&lt;/h3&gt;&lt;p&gt;标准C语言的char型数组工具中存在着其固有的误区，那就是它们都显示地依赖一种假设：字符数组包括一个空结束符。若由于疏忽或是其他差错，这个空结束符被忽略或重写，这个小小的差错就会使C语言的char数组处理函数几乎不可避免地操作其已分配空间之外的内存，有时会带来灾难性后果。&lt;br&gt;C++提供的string类对象，在使用的便利性和安全性上都有很大的提高。     &lt;/p&gt;
&lt;h4 id=&quot;追加、插入和连接字符串&quot;&gt;&lt;a href=&quot;#追加、插入和连接字符串&quot; class=&quot;headerlink&quot; title=&quot;追加、插入和连接字符串&quot;&gt;&lt;/a&gt;追加、插入和连接字符串&lt;/h4&gt;&lt;p&gt;C++字符串一个便于使用的特色是：无需程序员干预，可根据需要自行扩充规模。这样程序员不再需要跟踪字符串的存储边界。此外，当字符串增长时，字符串成员函数append()–(追加)和insert()–(插入)很明显地重新分配了存储空间。&lt;br&gt;此外，还有一些成员函数：capacity()返回当前分配的存储空间的规模；reserve(int size)提供一种优化机制，按照程序员的意图，预留一定数量的存储空间，以便将来使用。而如果要生成的新字符串的规模比当前的字符串大或者需要截断原字符串，resize()函数就会在字符串的末尾追加空格或是指定的字符。&lt;/p&gt;
&lt;h4 id=&quot;替换字符串的字符&quot;&gt;&lt;a href=&quot;#替换字符串的字符&quot; class=&quot;headerlink&quot; title=&quot;替换字符串的字符&quot;&gt;&lt;/a&gt;替换字符串的字符&lt;/h4&gt;&lt;p&gt;insert()使程序员放心地向字符串中插入字符，而不必担心会使存储空间越界，或者会改写插入点之后紧跟的字符。存储空间增大了，原有的字符会该改变其存储位置，以便安置新元素。&lt;br&gt;replace()用于替换（删除）字符串中的字符，如果对replace()的调用使“替换”超出了原有序列的边界，这与追加操作是等价的，这样replace()也会增加存储空间。&lt;br&gt;此外，我们知道，STL中的算法功能很强大，由于string类几乎可以与STL容器等价，因此，string的很多解决方案可以由标准库的算法完成。比如，使用&lt;code&gt;std::replace(s.begin(), s.end(), &amp;#39;X&amp;#39;, &amp;#39;Y&amp;#39;)&lt;/code&gt;将字符串中出现的某个字符（’X’）全部用另一个字符(‘Y’)替换掉。这里的replace()是STL通用算法，不是string的成员函数。&lt;/p&gt;
&lt;h4 id=&quot;使用非成员重载运算符连接&quot;&gt;&lt;a href=&quot;#使用非成员重载运算符连接&quot; class=&quot;headerlink&quot; title=&quot;使用非成员重载运算符连接&quot;&gt;&lt;/a&gt;使用非成员重载运算符连接&lt;/h4&gt;&lt;p&gt;C++ string类另一个令人欣喜的特性是可以借助operator+和operator+=来轻易地实现字符串的合并与追加操作。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;string str(&amp;quot;hello&amp;quot;);
str += &amp;quot; world&amp;quot;;
cout &amp;lt;&amp;lt; str &amp;lt;&amp;lt; endl;        //hello world
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;字符串的查找&quot;&gt;&lt;a href=&quot;#字符串的查找&quot; class=&quot;headerlink&quot; title=&quot;字符串的查找&quot;&gt;&lt;/a&gt;字符串的查找&lt;/h3&gt;&lt;p&gt;string成员函数中的find族是用来在给定字符串中定位某个或某组字符的。包括：&lt;br&gt;（1）find()：在字符串中查找一个指定的单个字符或字符组；&lt;br&gt;（2）find_first_of()：查找第一个与指定字符组任何字符匹配的字符位置；&lt;br&gt;（3）find_first_not_of()：查找第一个与指定字符组任何字符都不匹配的字符位置；&lt;br&gt;（4）find_last_of();&lt;br&gt;（5）find_last_not_of();&lt;br&gt;（6）rfind()：对一个字符串从尾到头查找指定的字符或字符组，返回首次匹配的开始位置。&lt;br&gt;string类中没有改变字符串大小写的函数，可以借助于标准C语言的toupper()和tolower()实现，这两个函数以此只能改变一个字符的大小写。&lt;/p&gt;
&lt;h3 id=&quot;字符串的删除&quot;&gt;&lt;a href=&quot;#字符串的删除&quot; class=&quot;headerlink&quot; title=&quot;字符串的删除&quot;&gt;&lt;/a&gt;字符串的删除&lt;/h3&gt;&lt;p&gt;使用erase()成员函数删除字符串中的字符，该函数有两个参数：一个参数表示开始删除字符的位置（默认值是0）；另一个表示要删除多少个字符（默认值是string::npos）。如果指定删除的字符个数比字符串中剩余的字符还多，那么剩余的字符将全部删除，所以，调用不含参数的erase()函数将删除字符串中的所有字符。&lt;/p&gt;
&lt;h3 id=&quot;字符串的比较&quot;&gt;&lt;a href=&quot;#字符串的比较&quot; class=&quot;headerlink&quot; title=&quot;字符串的比较&quot;&gt;&lt;/a&gt;字符串的比较&lt;/h3&gt;&lt;p&gt;字符串的比较，简而言之就是：当两个字符串比较遇到第一对不同的字符时，字符串s1中第一个不同的字符比字符串s2中同样位置的字符在ASCII表中的位置更靠后，那么s1“大于”s2。&lt;br&gt;C++提供了多种字符串比较方法，它们各具特色。其中最简单的就是使用非成员的重载运算符函数：operator==、operator!=、operator&amp;gt;、operator&amp;lt;、operator&amp;gt;=、operator&amp;lt;=。&lt;br&gt;此外，string类还提供compare()成员函数，用以提供更复杂紧密的比较手段。用多种重载版本来对比较的字串做控制。&lt;br&gt;C语言中获取字符串中的某个字符都是采用数组索引法（[]）,C++ string提供一种s[n]表示法的替代方法：at()成员函数。如果不出异常，这两种索引机制在C++中产生的结果是一样的。而有异常的情况时，比如程序员引用了一个超过边界的数组元素，at()将会友好地抛出一个异常，而普通的[]下标语法将让程序员自行决策。at()成员函数抛出的是一个out_of_range类对象，它(最终)派生于std::exception。程序可在一个异常处理器中捕获该对象，并采取适当的补救措施，比如重新计算越界下标或扩充数组。而string::operator&lt;a href=&quot;&quot;&gt;&lt;/a&gt;不会有那样的保护性，它的危险性等同于C语言中国对char型数组的处理。    &lt;/p&gt;
&lt;h3 id=&quot;C语言函数处理时的转换&quot;&gt;&lt;a href=&quot;#C语言函数处理时的转换&quot; class=&quot;headerlink&quot; title=&quot;C语言函数处理时的转换&quot;&gt;&lt;/a&gt;C语言函数处理时的转换&lt;/h3&gt;&lt;p&gt;C++ string提供了c_str()函数返回一个const char*，它指向一个C语言风格的具有“空结束符”的字符串，此字符串与string对象的内容等价。当想将一个字符串传送给一个标准C语言函数时，比如atoi()、printf()，const char*可派上用场。不过，用c_str()的返回值作为非const参数应用于任一函数都是错误的。    &lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;字符串是C++标准库的一个重要组成部分，本文参照《C++编程思想》中字符串的相关内容，对该部分进行简单总结。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.dujiong.net/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Redis两种持久化方式比较</title>
    <link href="http://blog.dujiong.net/2016/12/03/Redis-RDB-AOF/"/>
    <id>http://blog.dujiong.net/2016/12/03/Redis-RDB-AOF/</id>
    <published>2016-12-03T09:14:03.000Z</published>
    <updated>2017-03-27T07:02:28.516Z</updated>
    
    <content type="html">&lt;p&gt;本文对前面所述的Redis两种持久化方式RDB和AOF做简单总结和比较。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;h3 id=&quot;RDB和AOF简单总结&quot;&gt;&lt;a href=&quot;#RDB和AOF简单总结&quot; class=&quot;headerlink&quot; title=&quot;RDB和AOF简单总结&quot;&gt;&lt;/a&gt;RDB和AOF简单总结&lt;/h3&gt;&lt;p&gt;RDB持久化可以在指定的时间间隔内生成数据集的时间点快照，保存的是该时间点Redis在内存中的数据库状态。&lt;br&gt;AOF持久化记录的是服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。AOF文件中的命令全部以Redis协议的格式来保存，新命令会被追加到文件的末尾。Redis还可以在后台对AOF文件进行重写，使得AOF文件的体积不会超出保存数据集状态所需的实际大小。Redis可以同时使用AOF持久化和RDB持久化，在这种情况下，当Redis重启下，它会优先使用AOF来还原数据集，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整。           &lt;/p&gt;
&lt;h3 id=&quot;RDB&quot;&gt;&lt;a href=&quot;#RDB&quot; class=&quot;headerlink&quot; title=&quot;RDB&quot;&gt;&lt;/a&gt;RDB&lt;/h3&gt;&lt;h4 id=&quot;RDB的优点&quot;&gt;&lt;a href=&quot;#RDB的优点&quot; class=&quot;headerlink&quot; title=&quot;RDB的优点&quot;&gt;&lt;/a&gt;RDB的优点&lt;/h4&gt;&lt;p&gt;RDB是一个非常紧凑的文件，它保存了Redis在某个时间点上的数据集，这种文件非常适合进行备份。比如，可以在一天内，每小时备份RDB文件，并在每月的每一天，也备份一个RDB文件。这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。RDB非常适用于灾难恢复，它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心。RDB可以最大化Redis的性能：父进程在保存RDB文件时唯一要做的就是fork出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘I/O操作。RDB在恢复大数据集时的速度比AOF的恢复速度要快。          &lt;/p&gt;
&lt;h4 id=&quot;RDB的缺点&quot;&gt;&lt;a href=&quot;#RDB的缺点&quot; class=&quot;headerlink&quot; title=&quot;RDB的缺点&quot;&gt;&lt;/a&gt;RDB的缺点&lt;/h4&gt;&lt;p&gt;如果需要尽量避免在服务器故障时丢失数据，那么RDB不适合。因为，虽然Redis允许设置不同的保存点来控制保存RDB文件的频率，但是，因为RDB文件需要保存整个数据集的状态，所以它并不是一个轻松的操作。因此，可能会至少需要5分钟才保存一次RDB文件。这种情况下，一旦出现故障停机，那么便会丢失几分钟的数据。此外，每次保存RDB的时候，Redis都要fork出一个子进程，并由子进程来进行实际的持久化工作。当数据集比较庞大的时候，fork过程会非常耗时，造成服务器端在短时间内停止处理客户端。虽然AOF重写也需要进行fork，但无论fork重写的执行间隔有多长，数据的耐久性都不会有任何损失。&lt;/p&gt;
&lt;h3 id=&quot;AOF&quot;&gt;&lt;a href=&quot;#AOF&quot; class=&quot;headerlink&quot; title=&quot;AOF&quot;&gt;&lt;/a&gt;AOF&lt;/h3&gt;&lt;h4 id=&quot;AOF的优点&quot;&gt;&lt;a href=&quot;#AOF的优点&quot; class=&quot;headerlink&quot; title=&quot;AOF的优点&quot;&gt;&lt;/a&gt;AOF的优点&lt;/h4&gt;&lt;p&gt;使用AOF持久化会让Redis变得非常耐久，AOF支持设置不同的fsync策略，比如每秒钟fsync、每次执行写命令时fsync。AOF的默认策略为每秒钟fsync一次，在这种配置下，Redis仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（fsync会在后台进程执行，所以主进程可以继续处理命令请求）。另外，AOF文件是一个只进行追加操作的日志文件，因此对AOF文件的写入不需要进行seek，即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机），redis-check-aof工具也可以轻易地修复这种问题。Redis可以在AOF文件体积变得过大时，自动地在后台对AOF进行重写，重写后的新AOF文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为Redis在创建AOF文件的过程中，会继续将命令追加到现有的AOF文件里面，即使重写过程中发生停机，现有的AOF文件也不会丢失。而一旦新AOF文件创建完毕，Redis就会从旧AOF文件切换到新AOF文件，并开始对新AOF文件进行追加操作。AOF文件有序地保存了对数据库执行的所有写操作，这些写入操作以Redis协议的格式保存，因此AOF文件的内容非常容易被人读懂，对文件进行分析也很轻松。比如，如果不小心执行了FLUSHALL命令，但只要AOF文件未被重写，那么只要停止服务器，移除AOF文件末尾的FLUSHALL命令，并重启Redis，就可以将数据集恢复到执行FLUSHALL之前的状态。           &lt;/p&gt;
&lt;h4 id=&quot;AOF的缺点&quot;&gt;&lt;a href=&quot;#AOF的缺点&quot; class=&quot;headerlink&quot; title=&quot;AOF的缺点&quot;&gt;&lt;/a&gt;AOF的缺点&lt;/h4&gt;&lt;p&gt;对同样的数据集，AOF文件通常要大于等价的RDB文件。&lt;br&gt;AOF可能比RDB慢，这取决于具体的fsync策略。&lt;br&gt;另外，Redis AOF是通过递增地更新一个已经存在的状态，而RDB快照则是一次又一次地从头开始创造一切，概念上更健壮。&lt;/p&gt;
&lt;h3 id=&quot;二者的使用&quot;&gt;&lt;a href=&quot;#二者的使用&quot; class=&quot;headerlink&quot; title=&quot;二者的使用&quot;&gt;&lt;/a&gt;二者的使用&lt;/h3&gt;&lt;p&gt;一般来说，如果想达到很高的数据安全性，那么应该同时使用两种持久化功能。如果可以承受数分钟以内的数据丢失，那么可以只使用RDB持久化。但是不建议只是用AOF持久化：因为定时生成RDB快照非常便于进行数据库备份，并且RDB恢复数据集的速度也要比AOF恢复的速度要快。&lt;/p&gt;
&lt;h3 id=&quot;其他一些问题&quot;&gt;&lt;a href=&quot;#其他一些问题&quot; class=&quot;headerlink&quot; title=&quot;其他一些问题&quot;&gt;&lt;/a&gt;其他一些问题&lt;/h3&gt;&lt;h4 id=&quot;RDB设置&quot;&gt;&lt;a href=&quot;#RDB设置&quot; class=&quot;headerlink&quot; title=&quot;RDB设置&quot;&gt;&lt;/a&gt;RDB设置&lt;/h4&gt;&lt;p&gt;在默认情况下，Redis将数据库快照保存dump.rdb的二进制文件中。可以对Redis进行设置，让它在“N秒内数据集至少有M个改动”这样的条件满足时，自动保存一次数据集。此外，还可以使用SAVE或BGSAVE手动对Redis进行数据集保存操作。&lt;/p&gt;
&lt;h4 id=&quot;AOF文件出错&quot;&gt;&lt;a href=&quot;#AOF文件出错&quot; class=&quot;headerlink&quot; title=&quot;AOF文件出错&quot;&gt;&lt;/a&gt;AOF文件出错&lt;/h4&gt;&lt;p&gt;服务器可能在程序正在对AOF文件进行写入时停机，如果停机造成了AOF文件出错，那么Redis在重启时会拒绝载入这个AOF文件，从而确保数据的一致性不会被破坏。&lt;br&gt;当发生这种情况时，可以为现有的AOF文件创建一个备份，然后使用Redis附带的redis-check-aof程序，对原来的AOF文件进行修复，并查看两个文件的不同之处，最后重启Redis服务器，等待服务器载入修复后的AOF文件，并进行数据恢复。&lt;/p&gt;
&lt;h4 id=&quot;RDB和AOF之间的相互作用&quot;&gt;&lt;a href=&quot;#RDB和AOF之间的相互作用&quot; class=&quot;headerlink&quot; title=&quot;RDB和AOF之间的相互作用&quot;&gt;&lt;/a&gt;RDB和AOF之间的相互作用&lt;/h4&gt;&lt;p&gt;在版本大于等于2.4的Redis中，BGSAVE执行的过程中，不可以执行BGREWRITEAOF。同样，在执行BGREWRITEAOF的过程中，也不可执行BGSAVE。这样主要是防止两个Redis后台进程同时对磁盘进行大量的I/O操作。&lt;br&gt;如果BGSAVE正在执行，并且用户显示地调用BGREWRITEAOF命令， 那么服务器将向用户回复一个OK状态，并告知用户，BGREWRITEAOF已经被预定执行：一旦BGSAVE执行完毕，BGREWRITEAOF就会正式开始。&lt;br&gt;当Redis启动时，如果RDB持久化和AOF持久化都打开了，那么程序会优先使用AOF文件来恢复数据集，因为AOF文件所保存的数据通常是最完整的。&lt;/p&gt;
&lt;h4 id=&quot;RDB备份数据&quot;&gt;&lt;a href=&quot;#RDB备份数据&quot; class=&quot;headerlink&quot; title=&quot;RDB备份数据&quot;&gt;&lt;/a&gt;RDB备份数据&lt;/h4&gt;&lt;p&gt;Redis对于数据备份是非常友好的，可以在服务器运行的时候对 RDB 文件进行复制：RDB文件一旦被创建，就不会进行任何修改。当服务器要创建一个新的RDB文件时，它先将文件的内容保存在一个临时文件里，当临时文件写入完毕时，程序才原子地使用临时文件替换原来的RDB文件。即，无论何时，复制RDB文件都是绝对安全的。&lt;/p&gt;
&lt;p&gt;参考： &lt;a href=&quot;https://segmentfault.com/a/1190000005052628&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Redis持久化&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;本文对前面所述的Redis两种持久化方式RDB和AOF做简单总结和比较。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis之AOF持久化</title>
    <link href="http://blog.dujiong.net/2016/11/30/Redis-AOF/"/>
    <id>http://blog.dujiong.net/2016/11/30/Redis-AOF/</id>
    <published>2016-11-30T12:17:07.000Z</published>
    <updated>2017-03-27T07:06:48.645Z</updated>
    
    <content type="html">&lt;p&gt;除了RDB持久化功能之外，Redis还提供了AOF（Append Only File）持久化功能。与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;AOF持久化的实现&quot;&gt;&lt;a href=&quot;#AOF持久化的实现&quot; class=&quot;headerlink&quot; title=&quot;AOF持久化的实现&quot;&gt;&lt;/a&gt;AOF持久化的实现&lt;/h3&gt;&lt;p&gt;AOF持久化功能的实现可以分为命令追加、文件写入、文件同步三个步骤。&lt;/p&gt;
&lt;h4 id=&quot;命令追加&quot;&gt;&lt;a href=&quot;#命令追加&quot; class=&quot;headerlink&quot; title=&quot;命令追加&quot;&gt;&lt;/a&gt;命令追加&lt;/h4&gt;&lt;p&gt;当AOF持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct redisServer {
    sds aof_buf;        //AOF缓冲区
    ...
};    
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;AOF文件的写入与同步&quot;&gt;&lt;a href=&quot;#AOF文件的写入与同步&quot; class=&quot;headerlink&quot; title=&quot;AOF文件的写入与同步&quot;&gt;&lt;/a&gt;AOF文件的写入与同步&lt;/h4&gt;&lt;p&gt;Redis的服务器进程是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像serverCron函数这样需要定时运行的函数。&lt;br&gt;因为服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区，所以在服务器每次结束一个事件循环之前，它都会调用flushAppendOnlyFile函数，考虑是否需要将aof_buf缓冲区中的内容写入和保存到AOF文件中。&lt;br&gt;因为程序需要在回复客户端之前对AOF执行写操作，而客户端能执行写操作的唯一机会就是在事件loop中，因此，程序将所有AOF写累计到缓存中，并在重新事件之前，将缓存写入到文件中。下面是flushAppendOnlyFile函数的关键代码。 &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void flushAppendOnlyFile(int force) {
    ssize_t nwritten;
    int sync_int_progress = 0;
    if(sdslen(server.aof_buf) == 0) return;        //缓冲区没有内容

    //当fsync策略为每秒一次时，如果后台线程仍然有fsync在执行，那么可能会延迟执行冲洗（flush）操作，因为Linux上的write会被后台的fsync阻塞。
    //当这种情况下，说明需要尽快冲洗aof缓存，程序会尝试在serverCron函数中对缓存进行冲洗。
    //但是，当force为1时，不管后台是否在fsync，程序都直接进行写入
    if(server.aof_fsync == AOF_FSYNC_EVERYSEC)    //每秒同步
        sync_in_progress = bioPendingJobsOfType(REDIS_BIO_AOF_FSYNC) != 0;
    if(server.aof_fsync == AOF_FSYNC_EVERYSEC &amp;amp;&amp;amp; !force){
        if(sync_in_progress){    //后台正在执行FSYNC
            if(server.aof_flush_postponed_start == 0){
                server.aof_flush_postponed_start = server.unixtime;     //前面没有推迟过write操作，记录推迟写操作的时间
                return；
            }
            else if(server.unixtime-server.aof_flush_postponed_start &amp;lt; 2){    //推迟时间小于2秒
                return；
            }
            server.aof_delayed_fsync++;
            redisLog(REDIS_NOTICE, &amp;quot;...&amp;quot;);
        }
    }
    //程序对AOF文件进行写入
    server.aof_flush_postponed_start = 0;
    //如果写入设备是物理的话，这个操作应该是原子的
    //若出现电源中断这样的不可抗现象，AOF文件也是可能出现问题的，这时就需要使用redis-check-aof程序来修复
    nwritten = write(server.aof_fd, server.aof_buf, sdslen(server.aof_buf));
    if(nwritten != (signed)sdslen(server.aof_buf)){
        //不成功处理
        ...
    }else{
        if(server.aof_last_write_status == REDIS_ERR){
            redisLog(REDIS_WARNING, &amp;quot;...&amp;quot;);
            server.aof_last_write_status = REDIS_OK;
        }
    }
    server.aof_current_size += nwritten;    //更新写入后的AOF文件大小
    if(sdslen(server.aof_buf)+sdsavail(server.aof_buf) &amp;lt; 4000){
        sdsclear(server.aof_buf);    //清除缓存内容，等待重用
    }else{
        //释放缓存
        sdsfree(server.aof_buf);
        server.aof_buf = sdsempty();
    }

    if(server.aof_fsync == AOF_FSYNC_ALWAYS) {        //策略为总是执行fsync
        aof_fsync(server.aof_fd);
        server.aof_last_fsync = server.unixtime;
    }else if(server.aof_fsync == AOF_FSYNC_EVERYSEC &amp;amp;&amp;amp;
            server.unixtime &amp;gt; server.aof_last_fsync){        //策略为每秒fsync, 并且距离上次fsync已经超过1秒
        if (!sync_in_progress) aof_background_fsync(server.aof_fd);
        server.aof_last_fsync = server.unixtime;
    }
}       
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;服务器配置appendfsync选项的值直接决定AOF持久化功能的效率和安全性。&lt;br&gt;当appendfsync的值为always时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且同步AOF文件，所有always的效率是appendfsync三个选项值中最慢的一个，但从安全性来看，always也是最安全的，因为即使出现故障停机，AOF持久化也只会丢失一个事件循环中所产生的命令数据。&lt;br&gt;当appendfsync的值为everysec时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且每隔一秒就要在子线程中对AOF文件进行一次同步。从效率上来讲，everysec模式足够快，并且即使出现故障停机，也只丢失一秒钟的命令数据。&lt;br&gt;当appendfsync的值为no时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，至于何时对AOF文件进行同步，则由操作系统决定。所以该模式的单次同步时长通常是三种模式中时间最长的，并且在出现故障时，将丢失上次同步AOF文件之后的所有写命令数据。     &lt;/p&gt;
&lt;h3 id=&quot;AOF文件的载入与数据还原&quot;&gt;&lt;a href=&quot;#AOF文件的载入与数据还原&quot; class=&quot;headerlink&quot; title=&quot;AOF文件的载入与数据还原&quot;&gt;&lt;/a&gt;AOF文件的载入与数据还原&lt;/h3&gt;&lt;p&gt;因为AOF文件里面包含了重建数据库状态所需的所有写命令，所有服务器只要读入并重新执行一遍AOF文件里面保存的命令，就可以还原服务器关闭之前的数据库状态。Redis读取AOF文件并还原数据库状态的详细步骤如下：&lt;br&gt;（1）创建一个不带网络连接的伪客户端：因为Redis的命令只能在客户端上下文中执行，而载入AOF文件时所使用的命令直接来源于AOF文件而不是网络连接，所以服务器使用了一个没有网络连接的伪客户端来执行AOF文件保存的写命令，伪客户端执行命令的效果和带网络连接的客户端执行命令的效果完全一样。&lt;br&gt;（2）从AOF文件中分析并读出一条写命令。&lt;br&gt;（3）使用伪客户端执行被读出的写命令。&lt;br&gt;（4）重复执行步骤2和步骤3，直到AOF文件中的所有写命令都被处理完毕为止。      &lt;/p&gt;
&lt;h4 id=&quot;AOF重写&quot;&gt;&lt;a href=&quot;#AOF重写&quot; class=&quot;headerlink&quot; title=&quot;AOF重写&quot;&gt;&lt;/a&gt;AOF重写&lt;/h4&gt;&lt;p&gt;因为AOF持久化是通过保存被执行的写命令来记录数据库状态的，所以随着服务器运行时间的流逝，AOF文件中的内容会越来越多，文件的体积也会越来越大，如果不加以控制的话，体积过大的AOF文件很可能对Redis服务器、甚至整个宿主计算机造成影响，并且AOF文件的体积越大，使用AOF文件来进行数据还原所需的时间就越多。&lt;br&gt;为了解决AOF文件体积膨胀的问题，Redis提供了AOF文件重写功能。通过该功能，Redis服务器可以创建一个新的AOF文件来替代现有的AOF文件，新旧两个AOF文件所保存的数据库状态相同，但新AOF文件不会包含任何浪费空间的冗余命令，所以新AOF文件的体积通常会比旧AOF文件的体积要小得多。&lt;/p&gt;
&lt;h4 id=&quot;实现&quot;&gt;&lt;a href=&quot;#实现&quot; class=&quot;headerlink&quot; title=&quot;实现&quot;&gt;&lt;/a&gt;实现&lt;/h4&gt;&lt;p&gt;AOF文件重写是通过读取服务器当前的数据库状态来实现的，并不需要对现有的AOF文件进行任何读取、分析或者写入操作。即首先从数据库状态中读取键现在的值，然后用一条命令去记录键值对，代替之前记录这个键值对的多条命令。&lt;br&gt;下面是相关的源代码实现。       &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int rewriteAppendOnlyFile(char* filename)
{
    dictIterator *di = NULL;
    dictEntry *de;
    rio aof;
    FILE *fp;
    char tmpfile[256];
    int j;
    long long now = mstime();

    snprintf(tmpfile, 256, &amp;quot;temp-rewriteaof-%d.aof&amp;quot;, (int)getpid());
    fp = fopen(tmpfile, &amp;quot;w&amp;quot;);

    rioInitWithFile(&amp;amp;aof, fp);
    //设置每写入REDIS_AOF_AUTOSYNC_BYTES字节，就执行一次FSYNC
    //防止缓存中积累太多命令内容，造成I/O阻塞时间过长
    if(server.aof_rewrite_incremental_fsync){
        rioSetAutoSync(&amp;amp;aof, REDIS_AOF_AUTOSYNC_BYTES);
    }
    //遍历所有数据库
    for(j=0; j&amp;lt;server.dbnum; j++){
        char selectcmd[] = &amp;quot;*2\r\n$6\r\nSELECT\r\n&amp;quot;;
        redisDb *db = server.db+j;
        dict *d = db-&amp;gt;dict;
        if (dictSize(d) == 0) continue;
        di = dictGetSafeIterator(d);        //键空间迭代器
        if(!di) {
            fclose(fp);    
            return REDIS_ERR;
        }
        //写入SELECT命令
        if(rioWrite(&amp;amp;aof, selectcmd, sizeof(selectcmd)-1) ==0 ) goto werr;
        if (rioWriteBulkLongLong(&amp;amp;aof,j) == 0) goto werr;

        //遍历数据库所有键
        while((de = dictNext(di)) != NULL) {
            robj key, *o;
            long long expiretime;    
            sds keystr = dictGetKey(de);

            o = dictGetVal(de);        //取出键
            initStaticStringObject(key, keystr);
            expiretime = getExpire(db, &amp;amp;key);
            if(expiretime != -1 &amp;amp;&amp;amp; expiretime &amp;lt; now) continue;
            //根据值的类型，选择适当的命令来保存
            if(o-&amp;gt;type == REDIS_STRING){
                char cmd[]=&amp;quot;*3\r\n$3\r\nSET\r\n&amp;quot;;
                if (rioWrite(&amp;amp;aof,cmd,sizeof(cmd)-1) == 0) goto werr;
                if (rioWriteBulkObject(&amp;amp;aof,&amp;amp;key) == 0) goto werr;
                if (rioWriteBulkObject(&amp;amp;aof,o) == 0) goto werr;
            } else if(o-&amp;gt;type == REDIS_LIST){
                if (rewriteListObject(&amp;amp;aof,&amp;amp;key,o) == 0) goto werr;
            } else if(o-&amp;gt;type == REDIS_SET){
                if (rewriteSetObject(&amp;amp;aof,&amp;amp;key,o) == 0) goto werr;
            }
            ...
            //写入键的过期时间
            if(expiretime != -1) {
                char cmd[]=&amp;quot;*3\r\n$9\r\nPEXPIREAT\r\n&amp;quot;;
                if (rioWrite(&amp;amp;aof,cmd,sizeof(cmd)-1) == 0) goto werr;
                if (rioWriteBulkObject(&amp;amp;aof,&amp;amp;key) == 0) goto werr;
                if (rioWriteBulkLongLong(&amp;amp;aof,expiretime) == 0) goto werr;
            }
        }
        dictReleaseIterator(di);    //释放迭代器
    }
    if(fflush(fp)==EOF) goto werr;    
    if(aof_fsync(fileno(fp))==-1) goto werr;
    if(fclose(fp)==EOF) goto werr;
    //重命名
    if (rename(tmpfile,filename)==-1){
        ...
    }

    return REDIS_OK;
werr:
    fclose(fp);
    unlink(tmpfile);
    if(di) dictReleaseIterator(di);
    return REDIS_ERR;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;因为rewriteAppendOnlyFile函数生成的新AOF文件只包含还原当前数据库状态所必须的命令，所以新AOF文件不会浪费任何硬盘空间。&lt;/p&gt;
&lt;h4 id=&quot;后台重写&quot;&gt;&lt;a href=&quot;#后台重写&quot; class=&quot;headerlink&quot; title=&quot;后台重写&quot;&gt;&lt;/a&gt;后台重写&lt;/h4&gt;&lt;p&gt;rewriteAppendOnlyFile函数虽然可以很好地完成一个新的AOF文件的任务，但是，该函数会进行大量的写入操作，所以调用这个函数的线程将被长时间阻塞，因为Redis服务器使用单个线程来处理命令请求，所以如果服务器直接调用rewriteAppendOnlyFile函数的话，那么在重写AOF文件期间，服务器将无法处理客户端发来的命令请求。&lt;br&gt;所以Redis将AOF重写程序放到子进程里执行，这样做可以同时满足：（1）子进程在处理AOF重写的同时，服务器进程（父进程）可以继续处理命令请求。 （2）子进程带有服务器进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下，保证数据的安全性。当然，使用子进程也有一个问题，即子进程在进行AOF重写期间，服务器进程还需要继续处理命令请求，而新的命令可能会对现有的数据库状态进行修改，从而使得服务器当前的数据状态和重写后的AOF文件所保存的数据库状态不一致。&lt;br&gt;为此，Redis服务器设置了一个AOF重写缓冲区，该缓冲区在服务器创建子进程之后开始使用，当Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区。当子进程完成AOF重写工作之后，它会向父进程发送一个信号，父进程在接到信号后，会调用一个信号处理函数：（1）将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保存的数据库状态将和服务器当前的数据库状态一致；（2）对新的AOF文件进行改名，原子地覆盖现有的AOF文件，完成新旧两个AOF文件的替换。&lt;br&gt;因此，在整个AOF后台重写的过程中，只有信号处理函数执行时会对服务器进程（父进程）造成阻塞，在其他时候，AOF后台重写都不会阻塞父进程，这将AOF重写对服务器性能造成的影响降到了最低。&lt;br&gt;下面是相关的源代码实现。  &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int rewriteAppendOnlyFileBackground(void) {
    pid_t childpid;
    long long start;

    if(server.aof_child_pid != -1) return REDIS_ERR;

    start = ustime();
    if(childpid=fork()==0) {    //子进程
        char tmpfile[256];
        closeListeningSockets(0);    //关闭网络连接fd
        redisSetProcTitle(&amp;quot;redis-aof-rewrite&amp;quot;);        //为进程设置名字
        //创建临时文件，并进行AOF重写
        snprintf(tmpfile, 256, &amp;quot;temp-rewriteaof-bg-%d.aof&amp;quot;, (int) getpid());
        if(rewriteAppendOnlyFile(tmpfile) == REDIS_OK){
            exitFromChild(0);    //发送重写成功信号
        } else{
            exitFromChild(1);
        }
    } else{                    //父进程
        server.stat_fork_time = ustime()-start;
        ...
        server.aof_selected_db = -1;            //feedAppendOnlyFile()
        replicationScriptCacheFlush();
        return REDIS_OK;
    }
    return REDIS_OK;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;待续&quot;&gt;&lt;a href=&quot;#待续&quot; class=&quot;headerlink&quot; title=&quot;待续&quot;&gt;&lt;/a&gt;待续&lt;/h3&gt;&lt;p&gt;Redis提供了RDB和AOF两种持久化方式，对于二者各自的优缺点和使用的场景，还有待分析。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;除了RDB持久化功能之外，Redis还提供了AOF（Append Only File）持久化功能。与RDB持久化通过保存数据库中的键值对来记录数据库状态不同，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis之RDB持久化</title>
    <link href="http://blog.dujiong.net/2016/11/27/Redis-RDB/"/>
    <id>http://blog.dujiong.net/2016/11/27/Redis-RDB/</id>
    <published>2016-11-27T06:22:16.000Z</published>
    <updated>2017-03-27T07:06:50.360Z</updated>
    
    <content type="html">&lt;p&gt;Redis是内存数据库，它将自己的数据库状态储存在内存里面，所以，如果不想办法将储存在内存中的数据库状态保存到磁盘里面，那么一旦服务器进程退出，服务器中的数据库状态也会消失不见。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;Redis提供了RDB持久化功能，该功能可以将Redis在内存中的数据库状态保存到磁盘里面，避免数据意外丢失。RDB持久化可以手动执行，也可以根据服务器配置选项定期执行。RDB文件是一个压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态。      &lt;/p&gt;
&lt;h3 id=&quot;RDB文件的创建与载入&quot;&gt;&lt;a href=&quot;#RDB文件的创建与载入&quot; class=&quot;headerlink&quot; title=&quot;RDB文件的创建与载入&quot;&gt;&lt;/a&gt;RDB文件的创建与载入&lt;/h3&gt;&lt;p&gt;有两个命令Redis命令可以用于生成RDB文件，一个是SAVE（rdb.c/rdbSave），另一个是BGSAVE(rdb.c/rdbSaveBackground)。&lt;br&gt;SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求。而BGSAVE命令会派生一个子进程，然后由子进程负责创建RDB文件，服务器进程（父进程）继续处理命令请求。&lt;br&gt;创建RDB文件的实际工作由rdb.c/rdbSave函数完成，SAVE命令和BGSAVE命令以不同的方式调用这个函数，关键代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int rdbSave(char* filename) {
    dictIterator *di = NULL;
    dictEntry *de;
    char tmpfile[256];
    char magic[10];
    int j;
    long long now = mstime();
    FILE *fp;
    rio rdb;
    uint64_t cksum;

    snprintf(tmpfile, 256, &amp;quot;temp-%d.rdb&amp;quot;, (int)getpid());    //创建临时文件，pid为标识
    fp = fopen(tmpfile, &amp;quot;w&amp;quot;);
    if(!fp) {
        redisLog(REDIS_WARNING, &amp;quot;Failed opening .rdb for saving : %s&amp;quot;, strerror(errno));
        return REDIS_ERR;
    }

    rioInitWithFile(&amp;amp;rdb, fp);        //初始化I/O

    if(server.rdb_checksum)            //设置校验和函数
        rdb.update_cksum = rioGenericUpdateChecksum;
    snprintf(magic, sizeof(magic), &amp;quot;REDIS%04d&amp;quot;, REDIS_RDB_VERSION);
    if(rdbWriteRaw(&amp;amp;rdb, magic, 0)==-1) goto err;    //写入RDB版本号

    for(j=0; j&amp;lt;server.dbnum; j++){
        redisDb *db = server.db+j;    //指向数据库
        dict *d = db-&amp;gt;dict;            //数据库键空间
        if(dictSize(d) == 0) continue;    //跳过空数据库
        di = dictGetSafeIterator(d);
        if(!di){
            fclose(fp);            
            return REDIS_ERR;
        }
        //写入SELECTDB
        if (rdbSaveType(&amp;amp;rdb,REDIS_RDB_OPCODE_SELECTDB) == -1) goto werr;
        //对数据库ID编码后写入
        if (rdbSaveLen(&amp;amp;rdb,j) == -1) goto werr;
        //遍历数据库，写入每个键值对数据
        while((de = dictNext(di)) != NULL){
            sds keystr = dictGetKey(de);
            robj key, *o = dictGetValue(de);
            long long expire;

            initStaticStringObject(key, keystr);
            expire = getExpire(db, &amp;amp;key);
            if(rdbSaveKeyValuePair(&amp;amp;rdb, &amp;amp;key, o, expire, now)==-1) goto err;
        }
        dictReleaseIterator(di);
    }
    di = NULL;
    if (rdbSaveType(&amp;amp;rdb,REDIS_RDB_OPCODE_EOF) == -1) goto werr;        //写入EOF
    //校验和
    cksum = rdb.cksum;
    memrev64ifbe(&amp;amp;cksum);
    rioWrite(&amp;amp;rdb,&amp;amp;cksum,8);
    //冲洗缓存
    if (fflush(fp) == EOF) goto werr;
    if (fsync(fileno(fp)) == -1) goto werr;
    if (fclose(fp) == EOF) goto werr;
    //命名
    if (rename(tmpfile,filename) == -1) {
        redisLog(REDIS_WARNING,&amp;quot;Error moving temp DB file on the final destination: %s&amp;quot;, strerror(errno));
        unlink(tmpfile);
        return REDIS_ERR;
    }
    //清零数据库脏状态
    server.dirty = 0;
    //记录最后一次完成 SAVE 的时间
    server.lastsave = time(NULL);
    //记录最后一次执行 SAVE 的状态
    server.lastbgsave_status = REDIS_OK;

    return REDIS_OK;

werr:
    fclose(fp);
    unlink(tmpfile);
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;RDB文件的结构&quot;&gt;&lt;a href=&quot;#RDB文件的结构&quot; class=&quot;headerlink&quot; title=&quot;RDB文件的结构&quot;&gt;&lt;/a&gt;RDB文件的结构&lt;/h3&gt;&lt;p&gt;从上文的代码分析中可以看出RDB文件的结构。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/fnagdgd.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;RDB文件的最开头部分是REDIS，通过这五个字符，程序可以在载入文件时，快速检查所载入的文件是否是RDB文件。&lt;br&gt;db_version长度为4字节，它的值是一个字符串表示的整数，该整数记录了RDB文件的版本号，比如“0006”代表RDB文件的版本为第六版。  &lt;/p&gt;
&lt;h4 id=&quot;databases部分&quot;&gt;&lt;a href=&quot;#databases部分&quot; class=&quot;headerlink&quot; title=&quot;databases部分&quot;&gt;&lt;/a&gt;databases部分&lt;/h4&gt;&lt;p&gt;一个RDB文件的databases部分可以保存任意多个非空数据库。每个非空数据库在RDB文件中都可以保存为SELECTDB、db_number、key_value_pairs三个部分，如下图所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/vjF6XaN.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/8NLANJY.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;SELECTDB常量的长度为1字节，当读入程序遇到这个值的时候，它知道接下来要读入的将是一个数据库号码。db_number保存着一个数据库号码，根据号码的大小不同，该部分的长度可以是1、2或者5字节。key_value_pairs部分保存了数据库中的所有键值对数据，如果键值对带有过期时间，那么过期时间也会和键值对保存在一起。&lt;/p&gt;
&lt;h5 id=&quot;key-value-pairs部分&quot;&gt;&lt;a href=&quot;#key-value-pairs部分&quot; class=&quot;headerlink&quot; title=&quot;key_value_pairs部分&quot;&gt;&lt;/a&gt;key_value_pairs部分&lt;/h5&gt;&lt;p&gt;RDB文件中的每个key_value_pairs部分都保存了一个或以上数量的键值对，如果键值对带有过期时间的话，那么键值对的过期时间也会被保存在内。&lt;br&gt;不带过期时间的键值对在RDB文件中由TYPE、key、value三部分组成。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/NYbtrsP.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;TYPE记录了value的类型，长度为1字节。可能的取值包括REDIS_RDB_TYPE_STRING、REDIS_RDB_TYPE_LIST、REDIS_RDB_TYPE_SET、REDIS_RDB_TYPE_HASH等，每个TYPE常量代表了一种对象类型或者底层编码，当服务器读入RDB文件中的键值对数据时，程序会根据TYPE的值来决定如何读入和解释value的数据。&lt;br&gt;带有过期时间的键值对在RDB文件中的结构如图10-17所示。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/vI9BEm5.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;带有过期时间的键值对中的TYPE、key、value三部分的意义和前面不带过期时间键值对三部分的意义完全相同，新增的EXPIRETIME_\MS常量的长度为1字节，它的作用是告知程序，接下来要读入的将是一个以毫秒为单位的过期时间；ms是一个8字节长的带符号整数，记录着以ms为单位的时间戳，即过期时间。  &lt;/p&gt;
&lt;h3 id=&quot;自动间隔性保存&quot;&gt;&lt;a href=&quot;#自动间隔性保存&quot; class=&quot;headerlink&quot; title=&quot;自动间隔性保存&quot;&gt;&lt;/a&gt;自动间隔性保存&lt;/h3&gt;&lt;p&gt;因为BGSAVE命令可以在不阻塞服务器进程的情况下执行，所以Redis允许用户通过设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令。用户可以通过save选项设置多个保存条件，但只要其中任意一个条件满足，服务器就会执行BGSAVE命令。&lt;br&gt;当Redis服务器启动时，用户可以通过指定配置文件或者传入启动参数的方式设置save选项，如果用户没有主动设置save选项，那么服务器会为save选项设置默认条件：     &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;save 900 1
save 300 10
save 60 10000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接着，服务器程序会根据save选项所设置的保存条件，设置服务器状态redisServer结构的saveparams属性。      &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct redisServer {
    struct saveparam *saveparams;
    ...
}
struct saveparams {
    time_t seconds;        //秒数
    int changes;        //修改数
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;RDB文件的载入&quot;&gt;&lt;a href=&quot;#RDB文件的载入&quot; class=&quot;headerlink&quot; title=&quot;RDB文件的载入&quot;&gt;&lt;/a&gt;RDB文件的载入&lt;/h3&gt;&lt;p&gt;RDB文件的载入工作(rdb.c/rdbLoad)是在服务器启动时自动执行的，所以Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时监测到RDB文件存在，它就会自动载入RDB文件。&lt;br&gt;另外，因为AOF文件（见下文分析）的更新频率通常比RDB文件的更新频率高，所以：&lt;br&gt;（1）如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态；&lt;br&gt;（2）只有在AOF持久化功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;Redis是内存数据库，它将自己的数据库状态储存在内存里面，所以，如果不想办法将储存在内存中的数据库状态保存到磁盘里面，那么一旦服务器进程退出，服务器中的数据库状态也会消失不见。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis之复制</title>
    <link href="http://blog.dujiong.net/2016/11/25/Redis-replicate/"/>
    <id>http://blog.dujiong.net/2016/11/25/Redis-replicate/</id>
    <published>2016-11-25T07:04:22.000Z</published>
    <updated>2017-03-28T07:14:36.056Z</updated>
    
    <content type="html">&lt;p&gt;本文来学习下Redis中的复制特性。我们知道，在使用多Redis服务器的时候，保持主从服务器间的数据同步（一致性）是一个很重要的问题。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h3 id=&quot;Redis-2-8以前的复制&quot;&gt;&lt;a href=&quot;#Redis-2-8以前的复制&quot; class=&quot;headerlink&quot; title=&quot;Redis 2.8以前的复制&quot;&gt;&lt;/a&gt;Redis 2.8以前的复制&lt;/h3&gt;&lt;p&gt;Redid的复制功能分为同步和命令传播两个操作：&lt;br&gt;（1）同步用于将从服务器的数据库状态更新至主服务器当前所处的数据库状态；&lt;br&gt;（2）命令传播操作则用于在数据库状态被修改，导致主从服务器的数据库状态不一致时，让主从服务器的数据库状态重新回到一致状态。   &lt;/p&gt;
&lt;h4 id=&quot;同步&quot;&gt;&lt;a href=&quot;#同步&quot; class=&quot;headerlink&quot; title=&quot;同步&quot;&gt;&lt;/a&gt;同步&lt;/h4&gt;&lt;p&gt;从服务器对主服务器的同步操作需要通过向主服务器发送SYNC命令来完成，其执行步骤是：&lt;br&gt;（1）从服务器向主服务器发送SYNC命令。&lt;br&gt;（2）收到SYNC命令的主服务器执行BGSAVE命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有写命令。&lt;br&gt;（3）当主服务器的BGSAVE命令执行完毕时，主服务器会将BGSAVE命令生成的RDB文件发送给从服务器，从服务器接收并载入该RDB文件，将自己的数据库状态更新至主服务器执行BGSAVE命令时的数据库状态。&lt;br&gt;（4）主服务器将记录在缓冲区中的所有写命令发送给从服务器，从服务器执行这些写命令，将自己的数据库状态更新至主服务器数据库当前所处的状态。      &lt;/p&gt;
&lt;h4 id=&quot;命令传播&quot;&gt;&lt;a href=&quot;#命令传播&quot; class=&quot;headerlink&quot; title=&quot;命令传播&quot;&gt;&lt;/a&gt;命令传播&lt;/h4&gt;&lt;p&gt;同步操作执行完毕后，主从服务器两者的数据库达到一致状态，但这种一致并非一成不变的，每当主服务器执行客户端发送的写命令时，主服务器的数据库就有可能会被修改，并导致主服务器状态不再一致。&lt;br&gt;因此，为了让主从服务器再次回到一致状态，主服务器需要对服务器执行命令传播操作：主服务器会将自己执行的写命令，也即是造成主从服务器不一致的那条写命令，发送给从服务器执行，当从服务器执行了相同的写命令之后，主从服务器将再次回到一致状态。     &lt;/p&gt;
&lt;h4 id=&quot;Redis-2-8以前的复制存在的问题&quot;&gt;&lt;a href=&quot;#Redis-2-8以前的复制存在的问题&quot; class=&quot;headerlink&quot; title=&quot;Redis 2.8以前的复制存在的问题&quot;&gt;&lt;/a&gt;Redis 2.8以前的复制存在的问题&lt;/h4&gt;&lt;p&gt;Redis 2.8之前的版本对于断线后重新复制支持不好，虽然能够完成任务，但是效率却非常低。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/BixE10t.png&quot; alt=&quot;&quot;&gt;&lt;img src=&quot;http://i.imgur.com/PYwhGV3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;从上面的例子可以看出，虽然再次向主服务器发送SYNC命令可以让主从服务器重新回到一致状态，但是，主服务器生成的RDB文件中却包含了键k1至键k10089的RDB文件，而其中k1至k10086的数据对于服务器来说都是不必要的。     &lt;/p&gt;
&lt;h3 id=&quot;新版Redis复制功能的实现&quot;&gt;&lt;a href=&quot;#新版Redis复制功能的实现&quot; class=&quot;headerlink&quot; title=&quot;新版Redis复制功能的实现&quot;&gt;&lt;/a&gt;新版Redis复制功能的实现&lt;/h3&gt;&lt;p&gt;Redis从2.8版本开始，使用PSYNC命令代替SYNC命令来执行复制时的同步操作。而PSYNC命令具有完整重同步和部分重同步。不难想到，完全重同步用于处理初次复制的情况；而部分重同步用于处理断线后重复制的情况。&lt;/p&gt;
&lt;h4 id=&quot;部分重同步的实现&quot;&gt;&lt;a href=&quot;#部分重同步的实现&quot; class=&quot;headerlink&quot; title=&quot;部分重同步的实现&quot;&gt;&lt;/a&gt;部分重同步的实现&lt;/h4&gt;&lt;p&gt;部分重同步功能由三个部分构成：（1）主服务器和从服务器的复制偏移量；（2）主服务器的复制积压缓冲区；（3）服务器的运行ID。       &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;执行复制的主从服务器双方会分别维护一个复制偏移量：&lt;br&gt;（1）主服务器每次向从服务器传播N个字节的数据时，就将自己的复制偏移量的值加上N；&lt;br&gt;（2）从服务器每次收到主服务器传来的N个字节的数据时，将自己的复制偏移量的值加上N。&lt;br&gt;从而，通过对比主从服务器的复制偏移量，程序可以很容易地知道主从服务器是否处于抑制状态。   &lt;/li&gt;
&lt;li&gt;复制积压缓冲区是主从服务器在进行同步时的一个重要结构。它是由主服务器维护的一个固定长度的FIFO队列，默认大小为1MB。&lt;br&gt;当主服务器进行命令传播时，它不仅会将写命令发送给所有从服务器，还会将写命令入队到复制积压缓冲区中。因此，主服务器的复制积压缓冲区里面会保存一部分最近传播的写命令，并且会为每个字节记录相应的复制偏移量。&lt;br&gt;所以，当从服务器重新连上主服务器时，从服务器会通过PSYNC命令将自己的复制偏移量offset发送给主服务器，主服务器会根据这个复制偏移量来决定对从服务器执行何种同步操作。&lt;/li&gt;
&lt;li&gt;除了上述两部分之外，实现部分重同步操作还需要用到服务器运行ID，每个Redis服务器都拥有自己的运行ID。从而当从服务器对主服务器进行初次复制时，主服务器会将自己的运行ID传送给从服务器，从服务器则会将这个运行ID保存起来。当从服务器断线并重新连上一个主服务器时，从服务器将向当前连接的主服务器发送之前保存的运行ID，主服务器进行对比来判断是执行部分重同步还是完整重同步。    &lt;h3 id=&quot;心跳监测&quot;&gt;&lt;a href=&quot;#心跳监测&quot; class=&quot;headerlink&quot; title=&quot;心跳监测&quot;&gt;&lt;/a&gt;心跳监测&lt;/h3&gt;在命令传播阶段，从服务器默认以每秒一次的频率，向主服务器发送命令： &lt;code&gt;REPLCONF ACK &amp;lt;replication_offset&amp;gt;&lt;/code&gt;，其中replication_offset是从服务器当前的复制偏移量。&lt;br&gt;发送该命令对于主从服务器有三个作用：&lt;br&gt;（1）检测主从服务器的网络连接状态。&lt;br&gt;（2）辅助实现mini-slaves选项。&lt;br&gt;（3）检测命令丢失。    &lt;h4 id=&quot;检测主从服务器的网络连接状态&quot;&gt;&lt;a href=&quot;#检测主从服务器的网络连接状态&quot; class=&quot;headerlink&quot; title=&quot;检测主从服务器的网络连接状态&quot;&gt;&lt;/a&gt;检测主从服务器的网络连接状态&lt;/h4&gt;主从服务器通过发送和接收REPLCONF ACK命令来检查两者之间的网络连接是否正常：如果主服务器超过一秒钟没有收到从服务器发来的REPLCONF ACK命令，那么主服务器就知道主从服务器之间的连接出现问题了。    &lt;h4 id=&quot;辅助实现mini-slaves配置选项&quot;&gt;&lt;a href=&quot;#辅助实现mini-slaves配置选项&quot; class=&quot;headerlink&quot; title=&quot;辅助实现mini-slaves配置选项&quot;&gt;&lt;/a&gt;辅助实现mini-slaves配置选项&lt;/h4&gt;Redis的mini-slaves-to-write和mini-slaves-max-lag两个选项可以防止主服务器在不安全的情况下执行写命令。比如，主服务器有如下配置：&lt;br&gt;&lt;code&gt;min-slaves-to-write 3&lt;/code&gt;&lt;br&gt;&lt;code&gt;min-slaves-max-lag 10&lt;/code&gt;&lt;br&gt;那么在从服务器的数量少于3个，或者三个从服务器的延迟值都大于或等于10秒时，主服务器将拒绝执行写命令。    &lt;h4 id=&quot;检测命令丢失&quot;&gt;&lt;a href=&quot;#检测命令丢失&quot; class=&quot;headerlink&quot; title=&quot;检测命令丢失&quot;&gt;&lt;/a&gt;检测命令丢失&lt;/h4&gt;如果因为网络故障，主服务器传播给从服务器的写命令在半路丢失，那么当从服务器向主服务器发送REPLCONF ACK命令时，主服务器将发觉从服务器当前的复制偏移量少于自己的复制偏移量，然后主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区里找到从服务器缺少的数据，并将这些数据重新发送给从服务器。  &lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;本文来学习下Redis中的复制特性。我们知道，在使用多Redis服务器的时候，保持主从服务器间的数据同步（一致性）是一个很重要的问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.dujiong.net/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>最短路问题</title>
    <link href="http://blog.dujiong.net/2016/11/20/shortest-path/"/>
    <id>http://blog.dujiong.net/2016/11/20/shortest-path/</id>
    <published>2016-11-20T12:03:09.000Z</published>
    <updated>2016-11-25T01:45:32.826Z</updated>
    
    <content type="html">&lt;p&gt;最短路问题是图论中最基础的问题。最短路是给定两个顶点，在以这两个点为起点和终点的路径中，边的权值和最小的路径。如果把权值当做距离，考虑最短距离就很容易理解了。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;单源最短路问题&quot;&gt;&lt;a href=&quot;#单源最短路问题&quot; class=&quot;headerlink&quot; title=&quot;单源最短路问题&quot;&gt;&lt;/a&gt;单源最短路问题&lt;/h3&gt;&lt;p&gt;单源最短路问题是固定一个起点，求它到其他所有点的最短路的问题。如果终点也固定，这样的问题叫做两点之间的最短路问题。        &lt;/p&gt;
&lt;h4 id=&quot;Bellman-Ford算法&quot;&gt;&lt;a href=&quot;#Bellman-Ford算法&quot; class=&quot;headerlink&quot; title=&quot;Bellman-Ford算法&quot;&gt;&lt;/a&gt;Bellman-Ford算法&lt;/h4&gt;&lt;p&gt;记从起点s出发到顶点i的最短距离为d[i]。则下列等式成立：&lt;br&gt; &lt;img src=&quot;http://i.imgur.com/sZLfLxF.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;如果给定的图是一个有向无环图，可以按拓扑排序给顶点编号，并利用这条递推关系式计算出d。但是，如果图中有圈，就无法依赖这样的顺序进行排序。这种情况下，记当前到顶点i的最短路长度为d[i]，并设初值d[s]=0，d[i]=INF，再不断使用这条关系式更新d的值，就可以算出新的d。只要图中不存在负圈，这样的更新操作是有限的。结束之后的d就是所求的最短距离。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct edge {
    int from, to;
    int cost;
}

edge es[MAX_E];        //边

int d[MAX_V];        //最短距离
int V, E;            //顶点数和边数

void shortest_path(int s) {
    for(int i=0;i&amp;lt;V;i++){
        d[i]=INF;    //初始化为INF
    }
    d[s]=0;
    while(true){
        bool update =false;
        for(int i=0;i&amp;lt;E;i++){
            edge e = es[i];        //边
            if(d[e.from]!=INF &amp;amp;&amp;amp; d[e.to]&amp;gt;d[e.from]+e.cost){
                d[e.to] = d[e.from]+cost;    
                update = true;
            }
        }
        if(!update) break;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;该算法叫做Bellman-Ford算法。如果在图中不存在从s可达的负圈，那么最短路不会经过同一顶点两次（即最多通过|V-1|条边），while循环最多执行|V|-1次。因此，复杂度是O(|V|x|E|)。反之，如果存在从s到达的负圈，那么在第|V|次循环中也会更新d的值，因此可以利用这个性质来检查负圈。&lt;/p&gt;
&lt;h4 id=&quot;Dijkstra算法&quot;&gt;&lt;a href=&quot;#Dijkstra算法&quot; class=&quot;headerlink&quot; title=&quot;Dijkstra算法&quot;&gt;&lt;/a&gt;Dijkstra算法&lt;/h4&gt;&lt;p&gt;考虑一下Bellman-Ford算法没有负边的情况。如果d[i]还不是最短距离的话，那么即使进行d[j]=d[i]+(从i到j的边的权值)的更新，d[j]也不会变成最短距离。而且，即使d[i]没有变化，每一次循环也要检查一遍从i出发的所有边，这是很浪费时间的。因此可以对算法做如下修改：&lt;br&gt;（1）找到最短距离已经确定的顶点，从它出发更新相邻的最短距离；&lt;br&gt;（2）此后不需要再关心（1）中“最短距离已经确定的顶点”。&lt;br&gt;得到上述提到的“最短距离已经确定的顶点“是问题的关键。在最开始时，只有起点的最短距离是确定的。而在尚未使用过的顶点中，距离d[i]最小的顶点就是最短距离已经确定的顶点。因为不存在负边，所以d[i]不会在后面的更新中变小。该算法称为Dijkstra算法。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int cost[MAX_V][MAX_V];        //cost[u][v]表示边e=(u,v)的权值
int d[MAX_V];                //顶点s出发的最短距离
bool used[MAX_V];            //已经使用过的图
int V;

void dijkstra(int s){
    fill(d, d+V, INF);        //初始距离设为无穷大
    d[s]=0;

    while(true){
        int v=-1;
        for(int u=0;u&amp;lt;V;u++){
            if(!used[u] &amp;amp;&amp;amp; (v==-1 || d[u]&amp;lt;d[v])) v=u;    //从尚未使过的顶点中选择一个距离最小的顶点
        }
        if(v==-1) break;
        used[v]=true;
        for(int u=0;u&amp;lt;V;u++){
            d[u] = min(d[u], d[v]+cost[v][u]);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用邻接矩阵实现的Dijkstra算法的复杂度是O(|V|^2)。使用邻接表的话，更新最短距离只需要访问每条边一次即可，因此这部分的复杂度是O(|E|)。但是每次要枚举所有的顶点来查找下一个使用的顶点，因此最终复杂度还是O(|V|^2)。在|E|比较小时，大部分时间花在了查找下一个使用的顶点上，因此需要使用合适的数据结构对其进行优化。&lt;br&gt;使用堆就可以了，把每个顶点当前的最短距离用堆维护，在更新最短距离时，把对应的元素往根的方向移动以满足堆的性质。而每次从堆中取出的最小值就是下一次要使用的顶点。这样堆中元素共有O(|V|)个，更新和取出数值的操作有O(|E|)次，因此整个算法的复杂度是O(|E|log|V|)。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct edge {
    int to, cost;    
}

typedef pair&amp;lt;int, int&amp;gt; P;        //first是最短距离，second是顶点的编号

int V;
vector&amp;lt;edge&amp;gt; G[MAX_V];

int d[MAX_V];

void dijkstra(int s) {
    priority_queue&amp;lt;P, vector&amp;lt;P&amp;gt;, greater&amp;lt;P&amp;gt; &amp;gt; que;
    fill(d, d+V, INF);
    d[s]=0;
    que.push(P(0, s));

    while(!que.empty()){
        P p=que.top();
        que.pop();
        int v=p.second;        //顶点
        if(d[v]&amp;lt;p.first) continue;
        for(int i=0;i&amp;lt;G[v].size();i++){        //找距离最短的邻点
            edge e=G[v][i];
            if(d[e.to]&amp;gt;d[v]+e.cost){            //更新
                d[e.to]=d[v]+e.cost;
                que.push(P(d[e.to], e.to));        //把最近的节点和距离放入堆中
            }
        }    

    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;任意两点间的最短路问题&quot;&gt;&lt;a href=&quot;#任意两点间的最短路问题&quot; class=&quot;headerlink&quot; title=&quot;任意两点间的最短路问题&quot;&gt;&lt;/a&gt;任意两点间的最短路问题&lt;/h3&gt;&lt;p&gt;求解所有两点间的最短路问题叫做任意两点间的最短路问题，可以尝试使用动态规划。只使用顶点0~k和i,j的情况下，记i到j的最短路长度为d[k+1][i][j]。k=-1时，认为只使用i和j，所以d[0][i][j]=cost[i][j]。接下来把只使用顶点0~k的问题归约到只使用0~k-1的问题上。&lt;br&gt;只使用0~k时，我们分i到j的最短路正好经过顶点k一次和完全不经过顶点k两种情况来讨论。不经过顶点k的情况，d[k][i][j]=d[k-1][i][j]。通过顶点k的情况下，d[k][i][j]=d[k-1][i][k]+d[k-1][k][j]。合起来，就得到了d[k][i][j]=min(d[k-1][i][j], d[k-1][i][k]+d[k-1][k][j])。这个DP也可以使用同一个数组，不断进行d[i][j]=d[i][k]+d[k][j]的更新来实现。&lt;br&gt;该算法叫做Floyd-Warshall算法，可以在O(|V|^3)时间里求得所有两点间的最短路长度。该算法可以处理边是负数的情况。而判断图中是否有负圈，只需检查是否存在d[i][i]是负数的顶点i即可。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int d[MAX_V][MAX_V];    //边不存在为INF，d[i][i]=0
int V;

void warshall_floyd() {
    for(int k=0;k&amp;lt;V;k++){
        for(int i=0;i&amp;lt;V;i++){
            for(int j=0;j&amp;lt;V;j++){
                dp[i][j]=min(d[i][j], d[i][k]+d[k][j]);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样通过三重循环非常简单地就可以求出所有两点间的最短路长度。实现起来简单，复杂度也在可以承受的范围之内。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;最短路问题是图论中最基础的问题。最短路是给定两个顶点，在以这两个点为起点和终点的路径中，边的权值和最小的路径。如果把权值当做距离，考虑最短距离就很容易理解了。&lt;br&gt;
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="http://blog.dujiong.net/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
